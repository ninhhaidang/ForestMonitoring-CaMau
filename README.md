# á»¨ng dá»¥ng Viá»…n thÃ¡m vÃ  Há»c sÃ¢u trong GiÃ¡m sÃ¡t Biáº¿n Ä‘á»™ng Rá»«ng tá»‰nh CÃ  Mau

**Äá»“ Ã¡n tá»‘t nghiá»‡p - CÃ´ng nghá»‡ HÃ ng khÃ´ng VÅ© trá»¥**

Sinh viÃªn: **Ninh Háº£i ÄÄƒng** (MSSV: 21021411)
NÄƒm há»c: 2025 - 2026, Há»c ká»³ I

---

## ğŸ“‹ Tá»•ng quan

Dá»± Ã¡n nÃ y phÃ¡t triá»ƒn má»™t há»‡ thá»‘ng tá»± Ä‘á»™ng giÃ¡m sÃ¡t biáº¿n Ä‘á»™ng rá»«ng táº¡i tá»‰nh CÃ  Mau sá»­ dá»¥ng káº¿t há»£p dá»¯ liá»‡u viá»…n thÃ¡m Ä‘a nguá»“n (Sentinel-1 SAR vÃ  Sentinel-2 Optical) vá»›i hai phÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n: Machine Learning truyá»n thá»‘ng (Random Forest) vÃ  Deep Learning (CNN). Há»‡ thá»‘ng cÃ³ kháº£ nÄƒng phÃ¡t hiá»‡n vÃ  phÃ¢n loáº¡i cÃ¡c khu vá»±c máº¥t rá»«ng dá»±a trÃªn phÃ¢n tÃ­ch chuá»—i thá»i gian áº£nh vá»‡ tinh, vá»›i Ä‘á»™ chÃ­nh xÃ¡c > 98%.

---

## ğŸ“Š Dá»¯ liá»‡u

### Ground Truth Points
- **Tá»•ng sá»‘ Ä‘iá»ƒm:** 1,300 Ä‘iá»ƒm training
- **PhÃ¢n bá»‘:**
  - Label 0 (KhÃ´ng máº¥t rá»«ng): 650 Ä‘iá»ƒm (50.0%)
  - Label 1 (Máº¥t rá»«ng): 650 Ä‘iá»ƒm (50.0%)
- **Format:** CSV file vá»›i cÃ¡c trÆ°á»ng: `id`, `label`, `x`, `y` (tá»a Ä‘á»™ UTM Zone 48N)
- **File:** `data/raw/ground_truth/Training_Points_CSV.csv`

### Sentinel-2 (Optical)
- **7 bands** gá»“m spectral bands vÃ  spectral indices:
  - **Spectral bands:** B4 (Red), B8 (NIR), B11 (SWIR1), B12 (SWIR2)
  - **Spectral indices:** NDVI, NBR, NDMI
- **Äá»™ phÃ¢n giáº£i khÃ´ng gian:** 10m
- **Ká»³ áº£nh:**
  - TrÆ°á»›c: 30/01/2024 (`S2_2024_01_30.tif`)
  - Sau: 28/02/2025 (`S2_2025_02_28.tif`)
- **ÄÃ£ xá»­ lÃ½:** Cáº¯t theo ranh giá»›i rá»«ng tá»‰nh CÃ  Mau, masked NoData

### Sentinel-1 (SAR)
- **2 bands:** VV vÃ  VH polarization
- **Äá»™ phÃ¢n giáº£i khÃ´ng gian:** 10m (matched vá»›i Sentinel-2)
- **Ká»³ áº£nh:**
  - TrÆ°á»›c: 04/02/2024 (`S1_2024_02_04_matched_S2_2024_01_30.tif`)
  - Sau: 22/02/2025 (`S1_2025_02_22_matched_S2_2025_02_28.tif`)
- **ÄÃ£ xá»­ lÃ½:** Co-registered vá»›i Sentinel-2, cáº¯t theo ranh giá»›i rá»«ng

### Boundary Shapefile
- **File:** `data/raw/boundary/forest_boundary.shp`
- **Má»¥c Ä‘Ã­ch:** Giá»›i háº¡n khu vá»±c phÃ¢n tÃ­ch chá»‰ trong vÃ¹ng rá»«ng

---

## ğŸ“¦ Output Files

Sau khi cháº¡y xong, káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trong folder `results/`:

**Random Forest Outputs:**
```
results/
â”œâ”€â”€ rasters/
â”‚   â”œâ”€â”€ rf_classification.tif               # Binary classification map (0/1)
â”‚   â””â”€â”€ rf_probability.tif                  # Probability map (0.0-1.0)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ rf_model.pkl                        # Trained Random Forest (277 KB)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ training_data.csv                   # Training features (1,300 samples)
â”‚   â”œâ”€â”€ rf_feature_importance.csv           # Feature importance rankings
â”‚   â””â”€â”€ rf_evaluation_metrics.json          # Performance metrics
â””â”€â”€ plots/
    â”œâ”€â”€ rf_confusion_matrices.png           # Confusion matrices
    â”œâ”€â”€ rf_roc_curve.png                    # ROC curve
    â”œâ”€â”€ rf_feature_importance.png           # Top 20 features
    â”œâ”€â”€ rf_classification_maps.png          # Binary & probability maps
    â””â”€â”€ rf_cv_scores.png                    # 5-fold CV scores
```

**CNN Outputs:**
```
results/
â”œâ”€â”€ rasters/
â”‚   â”œâ”€â”€ cnn_classification.tif              # Binary classification map
â”‚   â””â”€â”€ cnn_probability.tif                 # Probability map
â”œâ”€â”€ models/
â”‚   â””â”€â”€ cnn_model.pth                       # Trained CNN (448 KB)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cnn_training_patches.npz            # Saved patches data
â”‚   â”œâ”€â”€ cnn_evaluation_metrics.json         # Performance metrics
â”‚   â””â”€â”€ cnn_training_history.json           # Training curves (loss, acc)
â””â”€â”€ plots/
    â”œâ”€â”€ cnn_confusion_matrices.png          # Confusion matrices
    â”œâ”€â”€ cnn_roc_curve.png                   # ROC curve
    â”œâ”€â”€ cnn_training_curves.png             # Loss & accuracy curves
    â””â”€â”€ cnn_classification_maps.png         # Binary & probability maps
```

---

---

## ğŸ”„ Pipeline Xá»­ LÃ½

### Pipeline Random Forest (Pixel-based Classification)

Pipeline Random Forest xá»­ lÃ½ dá»¯ liá»‡u á»Ÿ má»©c **pixel-level**, sá»­ dá»¥ng cÃ¡c feature Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« chuá»—i thá»i gian áº£nh vá»‡ tinh Ä‘á»ƒ phÃ¢n loáº¡i tá»«ng pixel Ä‘á»™c láº­p.

#### **BÆ°á»›c 1: Load Dá»¯ liá»‡u (Data Loading)**
- **Input:**
  - Sentinel-2 Before/After: 7 bands má»—i ká»³ (B4, B8, B11, B12, NDVI, NBR, NDMI)
  - Sentinel-1 Before/After: 2 bands má»—i ká»³ (VV, VH)
  - Ground truth points: CSV vá»›i 1,300 Ä‘iá»ƒm (x, y, label)
  - Forest boundary: Shapefile ranh giá»›i rá»«ng

- **Xá»­ lÃ½:**
  - Load táº¥t cáº£ dá»¯ liá»‡u raster vá»›i `rasterio`
  - Äá»c ground truth tá»« CSV vá»›i `pandas`
  - Kiá»ƒm tra kÃ­ch thÆ°á»›c, CRS, Ä‘á»™ phÃ¢n giáº£i

- **Output:** Dictionary chá»©a arrays vÃ  metadata

#### **BÆ°á»›c 2: Feature Extraction**
- **Input:** S2 before/after (7Ã—HÃ—W), S1 before/after (2Ã—HÃ—W)

- **Xá»­ lÃ½:**
  ```
  1. Sentinel-2 Features (21 features):
     - S2_before[0:7]  â†’ 7 features (B4, B8, B11, B12, NDVI, NBR, NDMI)
     - S2_after[0:7]   â†’ 7 features
     - S2_delta = S2_after - S2_before â†’ 7 features (temporal change)

  2. Sentinel-1 Features (6 features):
     - S1_before[0:2]  â†’ 2 features (VV, VH)
     - S1_after[0:2]   â†’ 2 features
     - S1_delta = S1_after - S1_before â†’ 2 features (temporal change)

  3. Valid Mask Creation:
     - Loáº¡i bá» pixels cÃ³ NoData/NaN á»Ÿ báº¥t ká»³ band/thá»i Ä‘iá»ƒm nÃ o
     - Äáº£m báº£o táº¥t cáº£ 27 features há»£p lá»‡ cho má»—i pixel
  ```

- **Output:** Feature stack (27Ã—HÃ—W), Valid mask (HÃ—W)

#### **BÆ°á»›c 3: Extract Training Data**
- **Input:** Feature stack, Ground truth points, Transform

- **Xá»­ lÃ½:**
  ```
  1. Coordinate Conversion:
     - Convert ground truth (x,y) tá»« UTM â†’ pixel coordinates
     - Sá»­ dá»¥ng rasterio transform

  2. Feature Extraction:
     - Vá»›i má»—i ground truth point:
       - TÃ¬m pixel tÆ°Æ¡ng á»©ng (row, col)
       - TrÃ­ch xuáº¥t 27 feature values táº¡i pixel Ä‘Ã³
       - GÃ¡n label tá»« ground truth
       - Skip náº¿u pixel náº±m ngoÃ i bounds hoáº·c cÃ³ NoData

  3. Data Quality Check:
     - Kiá»ƒm tra missing values, infinite values
     - Kiá»ƒm tra features cÃ³ zero variance
     - Kiá»ƒm tra class balance

  4. Train/Val/Test Split:
     - Train: 70% (stratified)
     - Validation: 15% (stratified)
     - Test: 15% (stratified)
     - Random state = 42 Ä‘á»ƒ reproducible
  ```

- **Output:**
  - Training DataFrame (n_samples Ã— 28): 27 features + 1 label
  - Split arrays: X_train, X_val, X_test, y_train, y_val, y_test

#### **BÆ°á»›c 4: Train Random Forest Model**
- **Input:** X_train (n_train Ã— 27), y_train (n_train,)

- **Hyperparameters:**
  ```python
  n_estimators = 100          # Sá»‘ decision trees
  max_depth = 20              # Äá»™ sÃ¢u tá»‘i Ä‘a cá»§a tree
  min_samples_split = 10      # Sá»‘ samples tá»‘i thiá»ƒu Ä‘á»ƒ split
  min_samples_leaf = 4        # Sá»‘ samples tá»‘i thiá»ƒu á»Ÿ leaf node
  max_features = 'sqrt'       # Sá»‘ features cho má»—i split
  class_weight = 'balanced'   # CÃ¢n báº±ng class weights
  oob_score = True            # Out-of-Bag score Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
  n_jobs = -1                 # Parallel processing
  random_state = 42
  ```

- **Training Process:**
  ```
  1. Model Creation:
     - Khá»Ÿi táº¡o RandomForestClassifier vá»›i hyperparameters
     - Sá»­ dá»¥ng sklearn.ensemble

  2. Model Fitting:
     - Fit model vá»›i X_train, y_train
     - Má»—i tree Ä‘Æ°á»£c train trÃªn random subset cá»§a data
     - Bootstrap sampling vá»›i replacement
     - Random feature selection táº¡i má»—i split

  3. Validation:
     - ÄÃ¡nh giÃ¡ trÃªn validation set
     - TÃ­nh OOB score (Out-of-Bag)
     - Log training/validation accuracy

  4. Feature Importance:
     - TÃ­nh Gini importance cho má»—i feature
     - Rank features theo importance
     - LÆ°u top 20 features quan trá»ng nháº¥t
  ```

- **Output:**
  - Trained model (pickle file ~277 KB)
  - Feature importance rankings

#### **BÆ°á»›c 5: Predict Full Raster**
- **Input:** Feature stack (27Ã—HÃ—W), Valid mask, Trained model

- **Xá»­ lÃ½:**
  ```
  1. Reshape Features:
     - Reshape tá»« (27, H, W) â†’ (HÃ—W, 27)
     - Táº¡o 2D feature matrix cho prediction

  2. Batch Prediction:
     - Lá»c chá»‰ valid pixels theo mask
     - Chia thÃ nh batches (10,000 pixels/batch) Ä‘á»ƒ tiáº¿t kiá»‡m memory
     - Vá»›i má»—i batch:
       - predictions = model.predict(batch_features)
       - probabilities = model.predict_proba(batch_features)[:, 1]

  3. Reconstruct Rasters:
     - Táº¡o classification map: shape (H, W), dtype int8
       - 0 = No deforestation
       - 1 = Deforestation
       - -1 = NoData
     - Táº¡o probability map: shape (H, W), dtype float32
       - Range [0.0, 1.0] = xÃ¡c suáº¥t máº¥t rá»«ng
       - -9999.0 = NoData
  ```

- **Output:**
  - Classification raster (GeoTIFF)
  - Probability raster (GeoTIFF)

#### **BÆ°á»›c 6: Evaluation & Visualization**
- **Input:** y_test, predictions, probabilities

- **Metrics:**
  ```
  1. Classification Metrics:
     - Accuracy, Precision, Recall, F1-Score
     - Confusion Matrix (train/val/test)
     - ROC Curve & AUC Score

  2. Cross-Validation:
     - 5-fold stratified CV
     - CV scores distribution plot

  3. Feature Analysis:
     - Feature importance plot (top 20)
     - Feature importance CSV export
  ```

- **Output:**
  - Confusion matrices plot
  - ROC curve plot
  - Feature importance plot
  - Classification maps visualization
  - Metrics JSON file

---

### Pipeline CNN (Patch-based Classification)

Pipeline CNN xá»­ lÃ½ dá»¯ liá»‡u á»Ÿ má»©c **patch-level**, sá»­ dá»¥ng kiáº¿n trÃºc máº¡ng neural Ä‘á»ƒ há»c spatial patterns tá»« cÃ¡c patches 3Ã—3 pixels.

#### **BÆ°á»›c 1: Load Dá»¯ liá»‡u (Data Loading)**
- Giá»‘ng vá»›i Random Forest Pipeline
- **Output:** Dictionary chá»©a arrays vÃ  metadata

#### **BÆ°á»›c 2: Feature Extraction**
- Giá»‘ng vá»›i Random Forest Pipeline
- **Output:** Feature stack (27Ã—HÃ—W), Valid mask (HÃ—W)

#### **BÆ°á»›c 3: Spatial Patch Extraction**
- **Input:** Feature stack (27Ã—HÃ—W), Ground truth points, Valid mask

- **Patch Configuration:**
  ```python
  patch_size = 3              # 3Ã—3 spatial window
  half_size = 1               # Padding around center pixel
  ```

- **Extraction Process:**
  ```
  1. Coordinate Conversion:
     - Convert ground truth (x,y) â†’ pixel coordinates (row, col)

  2. Patch Extraction:
     Vá»›i má»—i ground truth point táº¡i (row, col):
     - Kiá»ƒm tra edge constraints:
       if row < 1 or row >= H-1 or col < 1 or col >= W-1: skip

     - Extract 3Ã—3 window:
       patch = feature_stack[:, row-1:row+2, col-1:col+2]
       # Shape: (27, 3, 3)

     - Transpose Ä‘á»ƒ phÃ¹ há»£p CNN input:
       patch = transpose(patch, (1, 2, 0))
       # Shape: (3, 3, 27)

     - Validate patch:
       - Kiá»ƒm tra valid_mask[row-1:row+2, col-1:col+2].all()
       - Kiá»ƒm tra NaN/Inf values
       - Skip náº¿u patch khÃ´ng há»£p lá»‡

  3. Quality Control:
     - Loáº¡i bá» patches á»Ÿ edge (khÃ´ng Ä‘á»§ padding)
     - Loáº¡i bá» patches cÃ³ NoData
     - Äáº£m báº£o class balance
  ```

- **Output:**
  - Patches array: (n_samples, 3, 3, 27)
  - Labels array: (n_samples,)
  - Valid indices list

#### **BÆ°á»›c 4: Patch Normalization**
- **Input:** Raw patches (n_samples, 3, 3, 27)

- **Standardization Method:**
  ```python
  # Z-score normalization per feature channel
  mean = patches.mean(axis=(0, 1, 2), keepdims=True)  # Shape: (1, 1, 1, 27)
  std = patches.std(axis=(0, 1, 2), keepdims=True)    # Shape: (1, 1, 1, 27)

  normalized_patches = (patches - mean) / (std + 1e-8)
  ```

- **Output:**
  - Normalized patches
  - Normalization statistics (mean, std) Ä‘á»ƒ dÃ¹ng cho inference

#### **BÆ°á»›c 5: Spatial Data Split**
- **Input:** Patches, Labels, Ground truth coordinates

- **Spatial Split Strategy:**
  ```
  1. Calculate Spatial Median:
     - median_x = median(ground_truth['x'])
     - median_y = median(ground_truth['y'])

  2. Spatial Quadrant Assignment:
     - NW quadrant (x < median_x, y >= median_y) â†’ Train
     - NE quadrant (x >= median_x, y >= median_y) â†’ Train
     - SW quadrant (x < median_x, y < median_y) â†’ Test
     - SE quadrant (x >= median_x, y < median_y) â†’ Validation

  3. Prevent Data Leakage:
     - Train/Val/Test khÃ´ng cÃ³ overlap vá» khÃ´ng gian
     - Äáº£m báº£o model khÃ´ng há»c tá»« vÃ¹ng lÃ¢n cáº­n test areas
  ```

- **Output:**
  - X_train, y_train (spatial NW + NE)
  - X_val, y_val (spatial SE)
  - X_test, y_test (spatial SW)

#### **BÆ°á»›c 6: Build CNN Architecture**

- **Model Architecture:**
  ```
  Input: (batch, 3, 3, 27)
  â†“
  Permute â†’ (batch, 27, 3, 3)  # PyTorch format: (N, C, H, W)
  â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Conv Block 1                        â”‚
  â”‚  - Conv2d: 27 â†’ 64 channels (3Ã—3)  â”‚
  â”‚  - BatchNorm2d(64)                  â”‚
  â”‚  - ReLU activation                  â”‚
  â”‚  - Dropout2d(p=0.3)                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Conv Block 2                        â”‚
  â”‚  - Conv2d: 64 â†’ 32 channels (3Ã—3)  â”‚
  â”‚  - BatchNorm2d(32)                  â”‚
  â”‚  - ReLU activation                  â”‚
  â”‚  - Dropout2d(p=0.3)                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
  Global Average Pooling â†’ (batch, 32, 1, 1)
  â†“
  Flatten â†’ (batch, 32)
  â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ FC Block                            â”‚
  â”‚  - Linear: 32 â†’ 64                  â”‚
  â”‚  - BatchNorm1d(64)                  â”‚
  â”‚  - ReLU activation                  â”‚
  â”‚  - Dropout(p=0.5)                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
  Linear: 64 â†’ 2 (logits)
  â†“
  Output: (batch, 2)
  ```

- **Model Parameters:**
  ```
  - Total parameters: ~50,000 (trainable)
  - Model size: ~448 KB
  ```

- **Regularization Techniques:**
  ```
  - Batch Normalization: á»”n Ä‘á»‹nh training, giáº£m internal covariate shift
  - Dropout (0.3 conv, 0.5 fc): Prevent overfitting
  - Weight Decay (L2): 1e-4
  ```

#### **BÆ°á»›c 7: Train CNN Model**
- **Training Configuration:**
  ```python
  optimizer = AdamW(lr=0.001, weight_decay=1e-4)
  loss_fn = CrossEntropyLoss(weight=[1.0, 1.0])  # Balanced classes
  scheduler = ReduceLROnPlateau(factor=0.5, patience=5)

  batch_size = 32
  epochs = 50
  early_stopping_patience = 10
  ```

- **Training Loop:**
  ```
  For each epoch (1 to 50):
    1. Training Phase:
       - model.train()
       - For each batch in train_loader:
         - Forward pass: logits = model(patches)
         - Compute loss: loss = CrossEntropyLoss(logits, labels)
         - Backward pass: loss.backward()
         - Update weights: optimizer.step()
         - Track: train_loss, train_accuracy

    2. Validation Phase:
       - model.eval()
       - With torch.no_grad():
         - Forward pass trÃªn validation set
         - Compute: val_loss, val_accuracy

    3. Learning Rate Scheduling:
       - scheduler.step(val_loss)
       - Giáº£m LR náº¿u val_loss khÃ´ng cáº£i thiá»‡n sau 5 epochs

    4. Model Checkpointing:
       - If val_loss < best_val_loss:
         - Save model state_dict
         - Update best_val_loss, best_val_acc
         - Reset early_stopping_counter = 0
       - Else:
         - early_stopping_counter += 1

    5. Early Stopping:
       - If early_stopping_counter >= 10:
         - Stop training
         - Load best model checkpoint
  ```

- **Output:**
  - Best model checkpoint (cnn_model.pth)
  - Training history: train_loss, val_loss, train_acc, val_acc per epoch
  - Learning rate schedule

#### **BÆ°á»›c 8: Evaluate CNN Model**
- **Input:** Trained model, Test set (X_test, y_test)

- **Evaluation Process:**
  ```
  1. Test Inference:
     - model.eval()
     - With torch.no_grad():
       - logits = model(X_test)
       - probs = softmax(logits, dim=1)
       - preds = argmax(probs, dim=1)

  2. Metrics Calculation:
     - Accuracy = correct / total
     - Precision = TP / (TP + FP)
     - Recall = TP / (TP + FN)
     - F1-Score = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
     - ROC-AUC = area under ROC curve

  3. Confusion Matrix:
     - Train set confusion matrix
     - Validation set confusion matrix
     - Test set confusion matrix
  ```

- **Output:**
  - Test metrics JSON
  - Confusion matrices plot
  - ROC curve plot
  - Training curves (loss/accuracy over epochs)

#### **BÆ°á»›c 9: Full Raster Prediction (Sliding Window)**
- **Input:** Feature stack (27Ã—HÃ—W), Valid mask, Trained model

- **Sliding Window Extraction:**
  ```
  1. Patch Grid Generation:
     - stride = 1 (sliding window vá»›i bÆ°á»›c 1 pixel)
     - For row in range(1, H-1):
         For col in range(1, W-1):
           - Check valid_mask[row, col]
           - Extract patch táº¡i (row, col)
           - Append to patches_list
           - Save coordinates (row, col)

  2. Batch Prediction:
     - Chia patches thÃ nh batches (1000 patches/batch)
     - For each batch:
       - Normalize batch using training mean/std
       - Forward pass: logits = model(batch)
       - probs = softmax(logits, dim=1)[:, 1]  # Prob of class 1
       - preds = argmax(logits, dim=1)

  3. Reconstruct Rasters:
     - Initialize classification_map (H, W) vá»›i NoData = -1
     - Initialize probability_map (H, W) vá»›i NoData = -9999
     - For each (row, col, pred, prob):
       - classification_map[row, col] = pred
       - probability_map[row, col] = prob
  ```

- **Output:**
  - CNN classification raster (GeoTIFF)
  - CNN probability raster (GeoTIFF)

#### **BÆ°á»›c 10: Probability Calibration**
- **Input:** Model predictions, True labels

- **Calibration Method:**
  ```python
  from sklearn.calibration import CalibratedClassifierCV

  # Isotonic Regression Calibration
  calibrator = CalibratedClassifierCV(
      base_estimator=None,  # Sá»­ dá»¥ng CNN predictions
      method='isotonic',     # Isotonic regression
      cv='prefit'           # Model Ä‘Ã£ Ä‘Æ°á»£c train
  )

  calibrated_probs = calibrator.predict_proba(val_probs)
  ```

- **Calibration Metrics:**
  ```
  - Expected Calibration Error (ECE)
  - Reliability Diagram
  - Brier Score: measure of probability accuracy
  ```

- **Output:**
  - Calibrated probability raster
  - Calibration curve plot

#### **BÆ°á»›c 11: Post-processing & Visualization**
- Giá»‘ng Random Forest Pipeline
- **Additional CNN-specific visualizations:**
  - Training curves (loss & accuracy)
  - Learning rate schedule
  - Calibration curves

---

### So sÃ¡nh 2 Pipeline

| Aspect | Random Forest | CNN |
|--------|--------------|-----|
| **Input Unit** | Single pixel (27 features) | 3Ã—3 patch (3Ã—3Ã—27) |
| **Spatial Context** | KhÃ´ng sá»­ dá»¥ng spatial info | Há»c spatial patterns tá»« patches |
| **Feature Extraction** | Manual feature extraction | Automatic feature learning |
| **Training Time** | ~2-5 phÃºt (100 trees) | ~10-20 phÃºt (50 epochs) |
| **Model Size** | ~277 KB (pickle) | ~448 KB (PyTorch) |
| **Inference Speed** | Nhanh (~10k pixels/s) | Cháº­m hÆ¡n (~1k patches/s) |
| **Interpretability** | Cao (feature importance) | Tháº¥p (black-box) |
| **Data Requirements** | Ãt data, robust vá»›i noise | Cáº§n nhiá»u data hÆ¡n |
| **Overfitting Risk** | Tháº¥p vá»›i ensemble | Cao hÆ¡n (cáº§n regularization) |
| **Edge Handling** | Predict táº¥t cáº£ valid pixels | Bá» qua edge pixels (padding) |
| **Accuracy** | >98% | >98% |

---

## ğŸ“§ LiÃªn há»‡

- **Sinh viÃªn:** Ninh Háº£i ÄÄƒng
- **Email:** ninhhaidangg@gmail.com
- **GitHub:** [ninhhaidang](https://github.com/ninhhaidang)
- **ÄÆ¡n vá»‹:** TrÆ°á»ng Äáº¡i há»c CÃ´ng nghá»‡ - ÄHQGHN