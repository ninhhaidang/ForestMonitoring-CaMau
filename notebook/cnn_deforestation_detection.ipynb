{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Deforestation Detection Pipeline\n",
    "**Patch-based 2D CNN with Spatial Context**\n",
    "\n",
    "Du an: Giam sat Bien dong Rung tinh Ca Mau | SV: Ninh Hai Dang (21021411)\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Setup & Libraries -> 2. Load Data -> 3. Feature Extraction -> 4. Spatial Split -> 5. Extract Patches -> 6. Train CNN -> 7. Evaluate -> 8. Predict -> 9. Visualize -> 10. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Setup & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce GTX 1060 6GB\n",
      "\n",
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Geospatial\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, roc_curve, classification_report\n",
    ")\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = 'cpu'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_STATE)\n",
    "\n",
    "print(\"\\n✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  patch_size               : 3\n",
      "  n_features               : 27\n",
      "  n_classes                : 4\n",
      "  normalize_method         : standardize\n",
      "  dropout_rate             : 0.7\n",
      "  epochs                   : 200\n",
      "  batch_size               : 64\n",
      "  learning_rate            : 0.001\n",
      "  weight_decay             : 0.001\n",
      "  early_stopping_patience  : 15\n",
      "  use_lr_scheduler         : True\n",
      "  lr_scheduler_type        : ReduceLROnPlateau\n",
      "  lr_scheduler_patience    : 10\n",
      "  lr_scheduler_factor      : 0.5\n",
      "  lr_min                   : 1e-06\n",
      "  use_spatial_clustering   : False\n",
      "  cluster_distance         : 50.0\n",
      "  train_size               : 0.7\n",
      "  val_size                 : 0.15\n",
      "  test_size                : 0.15\n",
      "  device                   : cuda\n",
      "  pred_batch_size          : 8000\n",
      "  pred_stride              : 1\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'patch_size': 3,\n",
    "    'n_features': 27,\n",
    "    'n_classes': 4,  # ✅ Updated to 4 classes\n",
    "    'normalize_method': 'standardize',  # 'standardize' or 'minmax'\n",
    "    'dropout_rate': 0.7,  # Increased to prevent overfitting\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 200,  # Increased for more training\n",
    "    'batch_size': 64,  # Increased from 32\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-3,  # Increased to prevent overfitting\n",
    "    'early_stopping_patience': 15,  # Reduced from 50 for faster stopping\n",
    "    \n",
    "\n",
    "    # Learning Rate Scheduler (for long training)\n",
    "    'use_lr_scheduler': True,\n",
    "    'lr_scheduler_type': 'ReduceLROnPlateau',\n",
    "    'lr_scheduler_patience': 10,  # Reduced from 15\n",
    "    'lr_scheduler_factor': 0.5,\n",
    "    'lr_min': 1e-6,\n",
    "        # Spatial split\n",
    "    'use_spatial_clustering': False,  # True: spatial clustering, False: random split\n",
    "    'cluster_distance': 50.0,  # Only used if use_spatial_clustering=True\n",
    "    'train_size': 0.70,\n",
    "    'val_size': 0.15,\n",
    "    'test_size': 0.15,\n",
    "    \n",
    "    # Device\n",
    "    'device': device,\n",
    "    \n",
    "    # Prediction\n",
    "    'pred_batch_size': 8000,\n",
    "    'pred_stride': 1,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key:25s}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 16:58:32 - core.data_loader - INFO - \n",
      "======================================================================\n",
      "2025-11-19 16:58:32 - core.data_loader - INFO - LOADING SENTINEL-2 DATA (OPTICAL)\n",
      "2025-11-19 16:58:32 - core.data_loader - INFO - ======================================================================\n",
      "2025-11-19 16:58:32 - core.data_loader - INFO - LOADING SENTINEL-2 DATA (OPTICAL)\n",
      "2025-11-19 16:58:32 - core.data_loader - INFO - ======================================================================\n",
      "2025-11-19 16:58:32 - core.data_loader - INFO - Loading Sentinel-2 Before (2024-01-30): d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\sentinel-2\\S2_2024_01_30.tif\n",
      "2025-11-19 16:58:32 - core.data_loader - INFO - Loading Sentinel-2 Before (2024-01-30): d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\sentinel-2\\S2_2024_01_30.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 16:58:56 - core.data_loader - INFO -   [OK] Shape: (7, 10917, 12547)\n",
      "2025-11-19 16:58:56 - core.data_loader - INFO -   [OK] Bands: 7\n",
      "2025-11-19 16:58:56 - core.data_loader - INFO -   [OK] CRS: EPSG:32648\n",
      "2025-11-19 16:58:56 - core.data_loader - INFO -   [OK] Bands: 7\n",
      "2025-11-19 16:58:56 - core.data_loader - INFO -   [OK] CRS: EPSG:32648\n",
      "2025-11-19 16:58:56 - core.data_loader - INFO -   [OK] NoData: 0.0\n",
      "2025-11-19 16:59:07 - core.data_loader - INFO - Loading Sentinel-2 After (2025-02-28): d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\sentinel-2\\S2_2025_02_28.tif\n",
      "2025-11-19 16:58:56 - core.data_loader - INFO -   [OK] NoData: 0.0\n",
      "2025-11-19 16:59:07 - core.data_loader - INFO - Loading Sentinel-2 After (2025-02-28): d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\sentinel-2\\S2_2025_02_28.tif\n",
      "2025-11-19 16:59:32 - core.data_loader - INFO -   [OK] Shape: (7, 10917, 12547)\n",
      "2025-11-19 16:59:32 - core.data_loader - INFO -   [OK] Bands: 7\n",
      "2025-11-19 16:59:32 - core.data_loader - INFO -   [OK] CRS: EPSG:32648\n",
      "2025-11-19 16:59:32 - core.data_loader - INFO -   [OK] NoData: 0.0\n",
      "2025-11-19 16:59:32 - core.data_loader - INFO -   [OK] Shape: (7, 10917, 12547)\n",
      "2025-11-19 16:59:32 - core.data_loader - INFO -   [OK] Bands: 7\n",
      "2025-11-19 16:59:32 - core.data_loader - INFO -   [OK] CRS: EPSG:32648\n",
      "2025-11-19 16:59:32 - core.data_loader - INFO -   [OK] NoData: 0.0\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO - \n",
      "[OK] Sentinel-2 data loaded successfully\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO -   - Expected bands: ['B4', 'B8', 'B11', 'B12', 'NDVI', 'NBR', 'NDMI']\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO - \n",
      "======================================================================\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO - LOADING SENTINEL-1 DATA (SAR)\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO - ======================================================================\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO - Loading Sentinel-1 Before (2024-02-04): d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\sentinel-1\\S1_2024_02_04_matched_S2_2024_01_30.tif\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO - \n",
      "[OK] Sentinel-2 data loaded successfully\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO -   - Expected bands: ['B4', 'B8', 'B11', 'B12', 'NDVI', 'NBR', 'NDMI']\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO - \n",
      "======================================================================\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO - LOADING SENTINEL-1 DATA (SAR)\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO - ======================================================================\n",
      "2025-11-19 16:59:44 - core.data_loader - INFO - Loading Sentinel-1 Before (2024-02-04): d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\sentinel-1\\S1_2024_02_04_matched_S2_2024_01_30.tif\n",
      "2025-11-19 16:59:50 - core.data_loader - INFO -   [OK] Shape: (2, 10917, 12547)\n",
      "2025-11-19 16:59:50 - core.data_loader - INFO -   [OK] Bands: 2\n",
      "2025-11-19 16:59:50 - core.data_loader - INFO -   [OK] CRS: EPSG:32648\n",
      "2025-11-19 16:59:50 - core.data_loader - INFO -   [OK] Shape: (2, 10917, 12547)\n",
      "2025-11-19 16:59:50 - core.data_loader - INFO -   [OK] Bands: 2\n",
      "2025-11-19 16:59:50 - core.data_loader - INFO -   [OK] CRS: EPSG:32648\n",
      "2025-11-19 16:59:50 - core.data_loader - INFO -   [OK] NoData: 0.0\n",
      "2025-11-19 16:59:53 - core.data_loader - INFO - Loading Sentinel-1 After (2025-02-22): d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\sentinel-1\\S1_2025_02_22_matched_S2_2025_02_28.tif\n",
      "2025-11-19 16:59:50 - core.data_loader - INFO -   [OK] NoData: 0.0\n",
      "2025-11-19 16:59:53 - core.data_loader - INFO - Loading Sentinel-1 After (2025-02-22): d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\sentinel-1\\S1_2025_02_22_matched_S2_2025_02_28.tif\n",
      "2025-11-19 17:00:00 - core.data_loader - INFO -   [OK] Shape: (2, 10917, 12547)\n",
      "2025-11-19 17:00:00 - core.data_loader - INFO -   [OK] Bands: 2\n",
      "2025-11-19 17:00:00 - core.data_loader - INFO -   [OK] CRS: EPSG:32648\n",
      "2025-11-19 17:00:00 - core.data_loader - INFO -   [OK] Shape: (2, 10917, 12547)\n",
      "2025-11-19 17:00:00 - core.data_loader - INFO -   [OK] Bands: 2\n",
      "2025-11-19 17:00:00 - core.data_loader - INFO -   [OK] CRS: EPSG:32648\n",
      "2025-11-19 17:00:00 - core.data_loader - INFO -   [OK] NoData: 0.0\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - \n",
      "[OK] Sentinel-1 data loaded successfully\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO -   - Expected bands: ['VV', 'VH']\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - \n",
      "======================================================================\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - LOADING GROUND TRUTH POINTS\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - ======================================================================\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - Loading: d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\samples\\4labels.csv\n",
      "2025-11-19 17:00:00 - core.data_loader - INFO -   [OK] NoData: 0.0\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - \n",
      "[OK] Sentinel-1 data loaded successfully\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO -   - Expected bands: ['VV', 'VH']\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - \n",
      "======================================================================\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - LOADING GROUND TRUTH POINTS\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - ======================================================================\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - Loading: d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\samples\\4labels.csv\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - \n",
      "[OK] Ground truth loaded successfully\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO -   - Total points: 2630\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO -   - Class 0 (No deforestation): 656 (24.9%)\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO -   - Class 1 (Deforestation): 650 (24.7%)\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO -   - Columns: ['id', 'label', 'x', 'y']\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - \n",
      "======================================================================\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - LOADING BOUNDARY SHAPEFILE\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - \n",
      "[OK] Ground truth loaded successfully\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO -   - Total points: 2630\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO -   - Class 0 (No deforestation): 656 (24.9%)\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO -   - Class 1 (Deforestation): 650 (24.7%)\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO -   - Columns: ['id', 'label', 'x', 'y']\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - \n",
      "======================================================================\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - LOADING BOUNDARY SHAPEFILE\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - ======================================================================\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - Loading: d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\boundary\\forest_boundary.shp\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - ======================================================================\n",
      "2025-11-19 17:00:02 - core.data_loader - INFO - Loading: d:\\ninhhaidang\\25-26_HKI_DATN_21021411_DangNH\\data\\raw\\boundary\\forest_boundary.shp\n",
      "2025-11-19 17:00:03 - core.data_loader - INFO - \n",
      "[OK] Boundary loaded successfully\n",
      "2025-11-19 17:00:03 - core.data_loader - INFO -   - CRS: EPSG:32648\n",
      "2025-11-19 17:00:03 - core.data_loader - INFO -   - Features: 666\n",
      "2025-11-19 17:00:03 - core.data_loader - INFO - \n",
      "[OK] Boundary loaded successfully\n",
      "2025-11-19 17:00:03 - core.data_loader - INFO -   - CRS: EPSG:32648\n",
      "2025-11-19 17:00:03 - core.data_loader - INFO -   - Features: 666\n",
      "2025-11-19 17:00:03 - core.data_loader - INFO -   - Bounds: [ 465449.59497322  946649.76444972  590929.44661728 1055826.02611518]\n",
      "2025-11-19 17:00:03 - core.data_loader - INFO -   - Bounds: [ 465449.59497322  946649.76444972  590929.44661728 1055826.02611518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Data loaded successfully!\n",
      "\n",
      "Ground truth points: 2630\n",
      "Class distribution:\n",
      "label\n",
      "2    664\n",
      "3    660\n",
      "0    656\n",
      "1    650\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from core.data_loader import DataLoader\n",
    "\n",
    "print(\"Loading data...\")\n",
    "loader = DataLoader()\n",
    "\n",
    "# Load all data\n",
    "s2_before, s2_after = loader.load_sentinel2()\n",
    "s1_before, s1_after = loader.load_sentinel1()\n",
    "ground_truth = loader.load_ground_truth()\n",
    "boundary = loader.load_boundary()\n",
    "\n",
    "# Store metadata\n",
    "metadata = loader.metadata\n",
    "\n",
    "print(\"\\n✓ Data loaded successfully!\")\n",
    "print(f\"\\nGround truth points: {len(ground_truth)}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(ground_truth['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 17:00:03 - core.feature_extraction - INFO - \n",
      "======================================================================\n",
      "2025-11-19 17:00:03 - core.feature_extraction - INFO - STEP 3: FEATURE EXTRACTION\n",
      "2025-11-19 17:00:03 - core.feature_extraction - INFO - ======================================================================\n",
      "2025-11-19 17:00:03 - core.feature_extraction - INFO - \n",
      "Input dimensions: 10917 x 12547\n",
      "2025-11-19 17:00:03 - core.feature_extraction - INFO - \n",
      "Creating valid pixel mask (relaxed for cloud coverage)...\n",
      "2025-11-19 17:00:03 - core.feature_extraction - INFO - STEP 3: FEATURE EXTRACTION\n",
      "2025-11-19 17:00:03 - core.feature_extraction - INFO - ======================================================================\n",
      "2025-11-19 17:00:03 - core.feature_extraction - INFO - \n",
      "Input dimensions: 10917 x 12547\n",
      "2025-11-19 17:00:03 - core.feature_extraction - INFO - \n",
      "Creating valid pixel mask (relaxed for cloud coverage)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 17:00:10 - core.feature_extraction - INFO -   ✓ Total valid pixels: 16,246,925 / 136,975,599 (11.86%)\n",
      "2025-11-19 17:00:10 - core.feature_extraction - INFO -   ✓ Pixels with S2 data: 15,571,487 (11.37%)\n",
      "2025-11-19 17:00:10 - core.feature_extraction - INFO -   ✓ S1-only pixels (cloudy): 675,438 (0.49%)\n",
      "2025-11-19 17:00:10 - core.feature_extraction - INFO - \n",
      "Extracting Sentinel-2 features...\n",
      "2025-11-19 17:00:10 - core.feature_extraction - INFO -   ✓ Pixels with S2 data: 15,571,487 (11.37%)\n",
      "2025-11-19 17:00:10 - core.feature_extraction - INFO -   ✓ S1-only pixels (cloudy): 675,438 (0.49%)\n",
      "2025-11-19 17:00:10 - core.feature_extraction - INFO - \n",
      "Extracting Sentinel-2 features...\n",
      "2025-11-19 17:00:13 - core.feature_extraction - INFO -   - Imputing S2 values for cloudy pixels using median...\n",
      "2025-11-19 17:00:13 - core.feature_extraction - INFO -   - Imputing S2 values for cloudy pixels using median...\n",
      "2025-11-19 17:00:19 - core.feature_extraction - INFO -   - Adding S2 Before bands (7 features)\n",
      "2025-11-19 17:00:19 - core.feature_extraction - INFO -   - Adding S2 After bands (7 features)\n",
      "2025-11-19 17:00:19 - core.feature_extraction - INFO -   - Calculating S2 Delta (7 features)\n",
      "2025-11-19 17:00:19 - core.feature_extraction - INFO -   - Adding S2 Before bands (7 features)\n",
      "2025-11-19 17:00:19 - core.feature_extraction - INFO -   - Adding S2 After bands (7 features)\n",
      "2025-11-19 17:00:19 - core.feature_extraction - INFO -   - Calculating S2 Delta (7 features)\n",
      "2025-11-19 17:00:21 - core.feature_extraction - INFO -   ✓ Total S2 features: 21\n",
      "2025-11-19 17:00:21 - core.feature_extraction - INFO - \n",
      "Extracting Sentinel-1 features...\n",
      "2025-11-19 17:00:21 - core.feature_extraction - INFO -   - Adding S1 Before bands (2 features)\n",
      "2025-11-19 17:00:21 - core.feature_extraction - INFO -   - Adding S1 After bands (2 features)\n",
      "2025-11-19 17:00:21 - core.feature_extraction - INFO -   - Calculating S1 Delta (2 features)\n",
      "2025-11-19 17:00:21 - core.feature_extraction - INFO -   ✓ Total S2 features: 21\n",
      "2025-11-19 17:00:21 - core.feature_extraction - INFO - \n",
      "Extracting Sentinel-1 features...\n",
      "2025-11-19 17:00:21 - core.feature_extraction - INFO -   - Adding S1 Before bands (2 features)\n",
      "2025-11-19 17:00:21 - core.feature_extraction - INFO -   - Adding S1 After bands (2 features)\n",
      "2025-11-19 17:00:21 - core.feature_extraction - INFO -   - Calculating S1 Delta (2 features)\n",
      "2025-11-19 17:00:22 - core.feature_extraction - INFO -   ✓ Total S1 features: 6\n",
      "2025-11-19 17:00:22 - core.feature_extraction - INFO - \n",
      "Stacking all features...\n",
      "2025-11-19 17:00:22 - core.feature_extraction - INFO -   ✓ Total S1 features: 6\n",
      "2025-11-19 17:00:22 - core.feature_extraction - INFO - \n",
      "Stacking all features...\n",
      "2025-11-19 17:00:37 - core.feature_extraction - INFO -   ✓ Feature stack shape: (27, 10917, 12547)\n",
      "2025-11-19 17:00:37 - core.feature_extraction - INFO -   ✓ Total features: 27\n",
      "2025-11-19 17:00:37 - core.feature_extraction - INFO - \n",
      "Feature statistics:\n",
      "2025-11-19 17:00:37 - core.feature_extraction - INFO -   ✓ Feature stack shape: (27, 10917, 12547)\n",
      "2025-11-19 17:00:37 - core.feature_extraction - INFO -   ✓ Total features: 27\n",
      "2025-11-19 17:00:37 - core.feature_extraction - INFO - \n",
      "Feature statistics:\n",
      "2025-11-19 17:00:37 - core.feature_extraction - INFO -   S2_before_B4                  : min=   0.000, max=   0.814, mean=   0.047, std=   0.030\n",
      "2025-11-19 17:00:37 - core.feature_extraction - INFO -   S2_before_B4                  : min=   0.000, max=   0.814, mean=   0.047, std=   0.030\n",
      "2025-11-19 17:00:37 - core.feature_extraction - INFO -   S2_before_B8                  : min=   0.000, max=   0.759, mean=   0.219, std=   0.096\n",
      "2025-11-19 17:00:37 - core.feature_extraction - INFO -   S2_before_B8                  : min=   0.000, max=   0.759, mean=   0.219, std=   0.096\n",
      "2025-11-19 17:00:38 - core.feature_extraction - INFO -   S2_before_B11                 : min=   0.002, max=   0.583, mean=   0.100, std=   0.047\n",
      "2025-11-19 17:00:38 - core.feature_extraction - INFO -   S2_before_B11                 : min=   0.002, max=   0.583, mean=   0.100, std=   0.047\n",
      "2025-11-19 17:00:38 - core.feature_extraction - INFO -   S2_before_B12                 : min=   0.000, max=   0.983, mean=   0.053, std=   0.034\n",
      "2025-11-19 17:00:38 - core.feature_extraction - INFO -   S2_before_B12                 : min=   0.000, max=   0.983, mean=   0.053, std=   0.034\n",
      "2025-11-19 17:00:38 - core.feature_extraction - INFO -   S2_before_NDVI                : min=  -0.996, max=   0.999, mean=   0.571, std=   0.329\n",
      "2025-11-19 17:00:38 - core.feature_extraction - INFO -   S2_before_NDVI                : min=  -0.996, max=   0.999, mean=   0.571, std=   0.329\n",
      "2025-11-19 17:00:39 - core.feature_extraction - INFO -   S2_before_NBR                 : min=  -0.992, max=   0.998, mean=   0.573, std=   0.223\n",
      "2025-11-19 17:00:39 - core.feature_extraction - INFO -   S2_before_NBR                 : min=  -0.992, max=   0.998, mean=   0.573, std=   0.223\n",
      "2025-11-19 17:00:39 - core.feature_extraction - INFO -   S2_before_NDMI                : min=  -0.995, max=   0.923, mean=   0.342, std=   0.214\n",
      "2025-11-19 17:00:39 - core.feature_extraction - INFO -   S2_before_NDMI                : min=  -0.995, max=   0.923, mean=   0.342, std=   0.214\n",
      "2025-11-19 17:00:40 - core.feature_extraction - INFO -   S2_after_B4                   : min=   0.000, max=   0.664, mean=   0.055, std=   0.039\n",
      "2025-11-19 17:00:40 - core.feature_extraction - INFO -   S2_after_B4                   : min=   0.000, max=   0.664, mean=   0.055, std=   0.039\n",
      "2025-11-19 17:00:40 - core.feature_extraction - INFO -   S2_after_B8                   : min=   0.000, max=   0.624, mean=   0.211, std=   0.093\n",
      "2025-11-19 17:00:40 - core.feature_extraction - INFO -   S2_after_B8                   : min=   0.000, max=   0.624, mean=   0.211, std=   0.093\n",
      "2025-11-19 17:00:40 - core.feature_extraction - INFO -   S2_after_B11                  : min=   0.000, max=   0.643, mean=   0.112, std=   0.053\n",
      "2025-11-19 17:00:40 - core.feature_extraction - INFO -   S2_after_B11                  : min=   0.000, max=   0.643, mean=   0.112, std=   0.053\n",
      "2025-11-19 17:00:41 - core.feature_extraction - INFO -   S2_after_B12                  : min=   0.001, max=   1.502, mean=   0.060, std=   0.041\n",
      "2025-11-19 17:00:41 - core.feature_extraction - INFO -   S2_after_B12                  : min=   0.001, max=   1.502, mean=   0.060, std=   0.041\n",
      "2025-11-19 17:00:41 - core.feature_extraction - INFO -   S2_after_NDVI                 : min=  -0.997, max=   1.000, mean=   0.519, std=   0.344\n",
      "2025-11-19 17:00:41 - core.feature_extraction - INFO -   S2_after_NDVI                 : min=  -0.997, max=   1.000, mean=   0.519, std=   0.344\n",
      "2025-11-19 17:00:42 - core.feature_extraction - INFO -   S2_after_NBR                  : min=  -0.997, max=   0.997, mean=   0.518, std=   0.247\n",
      "2025-11-19 17:00:42 - core.feature_extraction - INFO -   S2_after_NBR                  : min=  -0.997, max=   0.997, mean=   0.518, std=   0.247\n",
      "2025-11-19 17:00:42 - core.feature_extraction - INFO -   S2_after_NDMI                 : min=  -0.998, max=   0.983, mean=   0.276, std=   0.224\n",
      "2025-11-19 17:00:42 - core.feature_extraction - INFO -   S2_after_NDMI                 : min=  -0.998, max=   0.983, mean=   0.276, std=   0.224\n",
      "2025-11-19 17:00:42 - core.feature_extraction - INFO -   S2_delta_B4                   : min=  -0.465, max=   0.624, mean=   0.008, std=   0.031\n",
      "2025-11-19 17:00:42 - core.feature_extraction - INFO -   S2_delta_B4                   : min=  -0.465, max=   0.624, mean=   0.008, std=   0.031\n",
      "2025-11-19 17:00:43 - core.feature_extraction - INFO -   S2_delta_B8                   : min=  -0.542, max=   0.562, mean=  -0.008, std=   0.058\n",
      "2025-11-19 17:00:43 - core.feature_extraction - INFO -   S2_delta_B8                   : min=  -0.542, max=   0.562, mean=  -0.008, std=   0.058\n",
      "2025-11-19 17:00:43 - core.feature_extraction - INFO -   S2_delta_B11                  : min=  -0.381, max=   0.458, mean=   0.012, std=   0.038\n",
      "2025-11-19 17:00:43 - core.feature_extraction - INFO -   S2_delta_B11                  : min=  -0.381, max=   0.458, mean=   0.012, std=   0.038\n",
      "2025-11-19 17:00:44 - core.feature_extraction - INFO -   S2_delta_B12                  : min=  -0.795, max=   1.479, mean=   0.008, std=   0.033\n",
      "2025-11-19 17:00:44 - core.feature_extraction - INFO -   S2_delta_B12                  : min=  -0.795, max=   1.479, mean=   0.008, std=   0.033\n",
      "2025-11-19 17:00:44 - core.feature_extraction - INFO -   S2_delta_NDVI                 : min=  -1.760, max=   1.314, mean=  -0.052, std=   0.204\n",
      "2025-11-19 17:00:44 - core.feature_extraction - INFO -   S2_delta_NDVI                 : min=  -1.760, max=   1.314, mean=  -0.052, std=   0.204\n",
      "2025-11-19 17:00:44 - core.feature_extraction - INFO -   S2_delta_NBR                  : min=  -1.742, max=   1.285, mean=  -0.054, std=   0.195\n",
      "2025-11-19 17:00:44 - core.feature_extraction - INFO -   S2_delta_NBR                  : min=  -1.742, max=   1.285, mean=  -0.054, std=   0.195\n",
      "2025-11-19 17:00:45 - core.feature_extraction - INFO -   S2_delta_NDMI                 : min=  -1.556, max=   1.185, mean=  -0.066, std=   0.175\n",
      "2025-11-19 17:00:45 - core.feature_extraction - INFO -   S2_delta_NDMI                 : min=  -1.556, max=   1.185, mean=  -0.066, std=   0.175\n",
      "2025-11-19 17:00:45 - core.feature_extraction - INFO -   S1_before_VV                  : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:45 - core.feature_extraction - INFO -   S1_before_VV                  : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:46 - core.feature_extraction - INFO -   S1_before_VH                  : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:46 - core.feature_extraction - INFO -   S1_before_VH                  : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:46 - core.feature_extraction - INFO -   S1_after_VV                   : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:46 - core.feature_extraction - INFO -   S1_after_VV                   : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:46 - core.feature_extraction - INFO -   S1_after_VH                   : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:46 - core.feature_extraction - INFO -   S1_after_VH                   : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO -   S1_delta_VV                   : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO -   S1_delta_VV                   : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO -   S1_delta_VH                   : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO - \n",
      "======================================================================\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO - ✓ FEATURE EXTRACTION COMPLETED\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO - ======================================================================\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO -   - Total features: 27\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO -   - Feature shape: (27, 10917, 12547)\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO -   - Valid pixels: 16,246,925 (11.86%)\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO -   S1_delta_VH                   : min=     nan, max=     nan, mean=     nan, std=     nan\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO - \n",
      "======================================================================\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO - ✓ FEATURE EXTRACTION COMPLETED\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO - ======================================================================\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO -   - Total features: 27\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO -   - Feature shape: (27, 10917, 12547)\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO -   - Valid pixels: 16,246,925 (11.86%)\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO - ======================================================================\n",
      "2025-11-19 17:00:47 - core.feature_extraction - INFO - ======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Feature extraction completed!\n",
      "Feature stack shape: (27, 10917, 12547)\n",
      "Valid pixels: 16,246,925 / 136,975,599 (11.86%)\n",
      "Valid pixels: 16,246,925 / 136,975,599 (11.86%)\n"
     ]
    }
   ],
   "source": [
    "from core.feature_extraction import FeatureExtraction\n",
    "\n",
    "print(\"Extracting features...\")\n",
    "extractor = FeatureExtraction()\n",
    "\n",
    "feature_stack, valid_mask = extractor.extract_features(\n",
    "    s2_before, s2_after,\n",
    "    s1_before, s1_after\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Feature extraction completed!\")\n",
    "print(f\"Feature stack shape: {feature_stack.shape}\")\n",
    "print(f\"Valid pixels: {valid_mask.sum():,} / {valid_mask.size:,} ({valid_mask.sum()/valid_mask.size*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Split Fixed Test Set (20%)\n",
    "\n",
    "**Quy trình đúng:**\n",
    "1. Tách 20% test cố định - KHÔNG BAO GIỜ động đến\n",
    "2. 5-fold CV trên 80% còn lại - để chọn hyperparameters\n",
    "3. Train mô hình cuối trên toàn bộ 80%\n",
    "4. Đánh giá cuối trên 20% test cố định\n",
    "\n",
    "- `use_spatial_clustering=False`: Sử dụng random split thông thường\n",
    "\n",
    "**Phương pháp chia:**- `use_spatial_clustering=True`: Sử dụng spatial clustering (cluster_distance=50m) để tránh data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2915266109.py, line 36)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mground_truth_test_fixed = ground_truth.iloc[test_indices_fixed].reset_index(drop=True)    print(f\"   Class {label}: {count} ({count/len(ground_truth_test_fixed)*100:.1f}%)\")\u001b[39m\n                                                                                              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from models.cnn.spatial_split import SpatialKFoldWithFixedTest\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: SPLIT OFF FIXED TEST SET (20%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize splitter with fixed test set\n",
    "if CONFIG['use_spatial_clustering']:\n",
    "    print(\"Using SPATIAL CLUSTERING (cluster_distance={:.1f}m)\".format(CONFIG['cluster_distance']))\n",
    "    cv_splitter = SpatialKFoldWithFixedTest(\n",
    "        test_size=0.2,  # 20% fixed test set\n",
    "        n_splits=5,      # 5-fold CV on remaining 80%\n",
    "        cluster_distance=CONFIG['cluster_distance'],\n",
    "        random_state=RANDOM_STATE,\n",
    "        shuffle=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Using RANDOM SPLIT (no spatial clustering)\")\n",
    "    cv_splitter = SpatialKFoldWithFixedTest(\n",
    "        test_size=0.2,  # 20% fixed test set\n",
    "        n_splits=5,      # 5-fold CV on remaining 80%\n",
    "        cluster_distance=None,  # None = random split\n",
    "        random_state=RANDOM_STATE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "# Split off fixed test set\n",
    "trainval_indices, test_indices_fixed, split_metadata = cv_splitter.split_fixed_test(\n",
    "    ground_truth,\n",
    "    stratify_by_class=True\n",
    ")\n",
    "\n",
    "# Get the train+val subset for CV\n",
    "ground_truth_trainval = ground_truth.iloc[trainval_indices].reset_index(drop=True)\n",
    "ground_truth_test_fixed = ground_truth.iloc[test_indices_fixed].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n✅ FIXED TEST SET CREATED:\")\n",
    "print(f\"   Total samples: {len(ground_truth)}\")\n",
    "print(f\"   Train+Val (for CV): {len(trainval_indices)} ({len(trainval_indices)/len(ground_truth)*100:.1f}%)\")\n",
    "print(f\"   Test (FIXED, held-out): {len(test_indices_fixed)} ({len(test_indices_fixed)/len(ground_truth)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ Fixed test set class distribution:\")\n",
    "for label in sorted(ground_truth_test_fixed['label'].unique()):\n",
    "    count = (ground_truth_test_fixed['label'] == label).sum()\n",
    "    print(f\"   Class {label}: {count} ({count/len(ground_truth_test_fixed)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Extract Patches for Fixed Test Set\n",
    "\n",
    "Trích xuất patches tại fixed test set locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn.patch_extractor import PatchExtractor\n",
    "\n",
    "print(\"Extracting patches for FIXED TEST SET...\")\n",
    "patch_extractor_test = PatchExtractor(patch_size=CONFIG['patch_size'])\n",
    "\n",
    "X_test_fixed, y_test_fixed, _ = patch_extractor_test.extract_patches_at_points(\n",
    "    feature_stack,\n",
    "    ground_truth_test_fixed,\n",
    "    metadata['s2_before']['transform'],\n",
    "    valid_mask\n",
    ")\n",
    "\n",
    "# Normalize and SAVE normalization stats (will use for train+val later)\n",
    "X_test_fixed, normalization_stats = patch_extractor_test.normalize_patches(method=CONFIG['normalize_method'])\n",
    "\n",
    "print(f\"\\n✅ Fixed test patches extracted!\")\n",
    "print(f\"  Patches shape: {X_test_fixed.shape}\")\n",
    "print(f\"  Labels shape: {y_test_fixed.shape}\")\n",
    "print(f\"  Normalization stats saved!\")\n",
    "\n",
    "print(f\"\\n✅ Fixed test set class distribution:\")\n",
    "for label in sorted(np.unique(y_test_fixed)):\n",
    "    count = (y_test_fixed == label).sum()\n",
    "    print(f\"  Class {label}: {count} ({count/len(y_test_fixed)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ 5-Fold Cross-Validation on Train+Val Data\n",
    "\n",
    "**Mục đích:** Tìm hyperparameters/architecture tốt nhất\n",
    "\n",
    "- Train 5 models khác nhau (mỗi fold 1 model)\n",
    "- Đánh giá trên validation của mỗi fold\n",
    "- Lấy **trung bình metrics** để đánh giá\n",
    "- **KHÔNG LẤY** bất kỳ model nào trong 5 models này làm model cuối"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn.architecture import create_model\n",
    "from models.cnn.trainer import CNNTrainer\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 2: 5-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_start_time = time.time()\n",
    "fold_results = []\n",
    "\n",
    "# Iterate through 5 folds\n",
    "for fold_idx, train_indices_cv, val_indices_cv in cv_splitter.cross_validate(\n",
    "    ground_truth_trainval,\n",
    "    stratify_by_class=True\n",
    "):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/5\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get ground truth for this fold\n",
    "    gt_train_cv = ground_truth_trainval.iloc[train_indices_cv].reset_index(drop=True)\n",
    "    gt_val_cv = ground_truth_trainval.iloc[val_indices_cv].reset_index(drop=True)\n",
    "    \n",
    "    # Extract patches for this fold\n",
    "    print(f\"\\nExtracting patches for Fold {fold_idx + 1}...\")\n",
    "    patch_extractor_train = PatchExtractor(patch_size=CONFIG['patch_size'])\n",
    "    X_train_cv, y_train_cv, _ = patch_extractor_train.extract_patches_at_points(\n",
    "        feature_stack, gt_train_cv,\n",
    "        metadata['s2_before']['transform'], valid_mask\n",
    "    )\n",
    "    X_train_cv, _ = patch_extractor_train.normalize_patches(method=CONFIG['normalize_method'])\n",
    "    \n",
    "    patch_extractor_val = PatchExtractor(patch_size=CONFIG['patch_size'])\n",
    "    X_val_cv, y_val_cv, _ = patch_extractor_val.extract_patches_at_points(\n",
    "        feature_stack, gt_val_cv,\n",
    "        metadata['s2_before']['transform'], valid_mask\n",
    "    )\n",
    "    X_val_cv, _ = patch_extractor_val.normalize_patches(method=CONFIG['normalize_method'])\n",
    "    \n",
    "    print(f\"  Train: {len(X_train_cv)} patches, Val: {len(X_val_cv)} patches\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    unique, counts = np.unique(y_train_cv, return_counts=True)\n",
    "    class_weights = [len(y_train_cv) / (len(unique) * c) for c in counts]\n",
    "    \n",
    "    # Create and train model\n",
    "    print(f\"\\nTraining Fold {fold_idx + 1}...\")\n",
    "    model_cv = create_model(\n",
    "        model_type='standard',\n",
    "        patch_size=CONFIG['patch_size'],\n",
    "        n_features=CONFIG['n_features'],\n",
    "        n_classes=CONFIG['n_classes'],\n",
    "        dropout_rate=CONFIG['dropout_rate']\n",
    "    )\n",
    "    \n",
    "    trainer_cv = CNNTrainer(\n",
    "        model=model_cv,\n",
    "        device=CONFIG['device'],\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay'],\n",
    "        class_weights=class_weights,\n",
    "        use_lr_scheduler=CONFIG['use_lr_scheduler'],\n",
    "        lr_scheduler_patience=CONFIG['lr_scheduler_patience'],\n",
    "        lr_scheduler_factor=CONFIG['lr_scheduler_factor'],\n",
    "        lr_min=CONFIG['lr_min']\n",
    "    )\n",
    "    \n",
    "    history_cv = trainer_cv.fit(\n",
    "        X_train_cv, y_train_cv,\n",
    "        X_val_cv, y_val_cv,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        early_stopping_patience=CONFIG['early_stopping_patience']\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_metrics_cv = trainer_cv.evaluate(X_val_cv, y_val_cv, batch_size=CONFIG['batch_size'])\n",
    "    \n",
    "    # Compute confusion matrix for this fold\n",
    "    cm_fold = confusion_matrix(val_metrics_cv['labels'], val_metrics_cv['predictions'])\n",
    "    \n",
    "    # Save fold results (including confusion matrix)\n",
    "    fold_results.append({\n",
    "        'fold': fold_idx + 1,\n",
    "        'train_acc': history_cv['train_acc'][-1] / 100.0,\n",
    "        'train_loss': history_cv['train_loss'][-1],\n",
    "        'val_acc': val_metrics_cv['accuracy'],\n",
    "        'val_precision': val_metrics_cv['precision'],\n",
    "        'val_recall': val_metrics_cv['recall'],\n",
    "        'val_f1': val_metrics_cv['f1_score'],\n",
    "        'val_roc_auc': val_metrics_cv['roc_auc'],\n",
    "        'confusion_matrix': cm_fold.tolist()  # ← Save confusion matrix\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1} Results:\")\n",
    "    print(f\"  Val Accuracy: {val_metrics_cv['accuracy']:.4f}\")\n",
    "    print(f\"  Val F1-Score: {val_metrics_cv['f1_score']:.4f}\")\n",
    "\n",
    "cv_total_time = time.time() - cv_start_time\n",
    "\n",
    "# Print CV summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5-FOLD CV SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "val_accs = [r['val_acc'] for r in fold_results]\n",
    "val_f1s = [r['val_f1'] for r in fold_results]\n",
    "print(f\"\\nValidation Accuracy: {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}\")\n",
    "print(f\"Validation F1-Score: {np.mean(val_f1s):.4f} ± {np.std(val_f1s):.4f}\")\n",
    "print(f\"\\nTotal CV time: {cv_total_time/60:.2f} minutes\")\n",
    "print(\"\\n✅ Cross-validation completed!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all 5 folds\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5-FOLD CV CONFUSION MATRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Class names for 4 classes\n",
    "class_names = ['Forest\\nStable', 'Defor.', 'Non-\\nforest', 'Refor.']\n",
    "\n",
    "# Create figure with 2 rows x 3 columns (5 folds + 1 aggregated)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Initialize aggregated confusion matrix\n",
    "cm_aggregated = np.zeros((4, 4), dtype=int)\n",
    "\n",
    "# Plot each fold's confusion matrix\n",
    "for idx, result in enumerate(fold_results):\n",
    "    ax = axes[idx]\n",
    "    fold_num = result['fold']\n",
    "    \n",
    "    # Get confusion matrix for this fold (stored during CV)\n",
    "    if 'confusion_matrix' in result:\n",
    "        cm = np.array(result['confusion_matrix'])\n",
    "        cm_aggregated += cm  # Add to aggregated matrix\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax,\n",
    "                    xticklabels=class_names,\n",
    "                    yticklabels=class_names,\n",
    "                    annot_kws={'size': 10})\n",
    "        \n",
    "        ax.set_title(f'Fold {fold_num}\\nAcc: {result[\"val_acc\"]:.3f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Predicted', fontsize=10)\n",
    "        ax.set_ylabel('True', fontsize=10)\n",
    "\n",
    "# Plot aggregated confusion matrix in the 6th subplot\n",
    "ax = axes[5]\n",
    "sns.heatmap(cm_aggregated, annot=True, fmt='d', cmap='Greens', cbar=False, ax=ax,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            annot_kws={'size': 11, 'weight': 'bold'})\n",
    "\n",
    "# Calculate aggregated accuracy\n",
    "aggregated_acc = np.trace(cm_aggregated) / np.sum(cm_aggregated)\n",
    "ax.set_title(f'Aggregated (All Folds)\\nAcc: {aggregated_acc:.3f}', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('True', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add title to entire figure\n",
    "fig.suptitle('5-Fold Cross-Validation - Confusion Matrices', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/cnn_cv_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Confusion matrices saved to: ../results/plots/cnn_cv_confusion_matrices.png\")\n",
    "print(f\"\\nAggregated Confusion Matrix:\")\n",
    "print(cm_aggregated)\n",
    "print(f\"\\nAggregated Accuracy: {aggregated_acc:.4f} ({aggregated_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for 5-Fold CV\n",
    "\n",
    "Visualize confusion matrices from cross-validation to understand model performance across different folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Train FINAL Model on Full Train+Val Data\n",
    "\n",
    "**MÔ HÌNH CUỐI CÙNG:**\n",
    "- Train trên **TOÀN BỘ** 85% train+val data (~2236 mẫu)\n",
    "- Train từ đầu (không dùng bất kỳ model nào từ CV)\n",
    "- Đây là model sẽ được dùng để prediction và đánh giá cuối cùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 3: TRAIN FINAL MODEL ON FULL TRAIN+VAL DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract patches for FULL train+val data\n",
    "print(\"\\nExtracting patches for full train+val data...\")\n",
    "patch_extractor_full = PatchExtractor(patch_size=CONFIG['patch_size'])\n",
    "\n",
    "X_trainval_full, y_trainval_full, _ = patch_extractor_full.extract_patches_at_points(\n",
    "    feature_stack,\n",
    "    ground_truth_trainval,\n",
    "    metadata['s2_before']['transform'],\n",
    "    valid_mask\n",
    ")\n",
    "\n",
    "# Use same normalization as test set\n",
    "X_trainval_full, _ = patch_extractor_full.normalize_patches(method=CONFIG['normalize_method'])\n",
    "\n",
    "print(f\"  Full train+val patches: {len(X_trainval_full)}\")\n",
    "\n",
    "# Calculate class weights\n",
    "unique, counts = np.unique(y_trainval_full, return_counts=True)\n",
    "class_weights_final = [len(y_trainval_full) / (len(unique) * c) for c in counts]\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label in sorted(np.unique(y_trainval_full)):\n",
    "    count = (y_trainval_full == label).sum()\n",
    "    print(f\"  Class {label}: {count} ({count/len(y_trainval_full)*100:.1f}%)\")\n",
    "\n",
    "# Create FINAL model\n",
    "print(f\"\\nCreating FINAL model...\")\n",
    "model_final = create_model(\n",
    "    model_type='standard',\n",
    "    patch_size=CONFIG['patch_size'],\n",
    "    n_features=CONFIG['n_features'],\n",
    "    n_classes=CONFIG['n_classes'],\n",
    "    dropout_rate=CONFIG['dropout_rate']\n",
    ")\n",
    "\n",
    "print(model_final.get_model_summary())\n",
    "\n",
    "# Train FINAL model\n",
    "print(f\"\\nTraining FINAL model on {len(X_trainval_full)} samples...\")\n",
    "print(\"(This is the model that will be used for prediction)\")\n",
    "\n",
    "trainer_final = CNNTrainer(\n",
    "    model=model_final,\n",
    "    device=CONFIG['device'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    class_weights=class_weights_final,\n",
    "    use_lr_scheduler=CONFIG['use_lr_scheduler'],\n",
    "    lr_scheduler_patience=CONFIG['lr_scheduler_patience'],\n",
    "    lr_scheduler_factor=CONFIG['lr_scheduler_factor'],\n",
    "    lr_min=CONFIG['lr_min']\n",
    ")\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "# Note: No validation set here - train on ALL train+val data\n",
    "# We use a small portion (10%) for monitoring only\n",
    "split_idx = int(0.9 * len(X_trainval_full))\n",
    "X_train_final = X_trainval_full[:split_idx]\n",
    "y_train_final = y_trainval_full[:split_idx]\n",
    "X_monitor = X_trainval_full[split_idx:]\n",
    "y_monitor = y_trainval_full[split_idx:]\n",
    "\n",
    "history_final = trainer_final.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    X_monitor, y_monitor,  # Just for monitoring, not for model selection\n",
    "    epochs=CONFIG['epochs'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    early_stopping_patience=CONFIG['early_stopping_patience']\n",
    ")\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "\n",
    "print(f\"\\n✅ FINAL model trained in {training_time/60:.2f} minutes\")\n",
    "print(f\"  Final train accuracy: {history_final['train_acc'][-1]:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training History - Loss & Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for final model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Get number of epochs\n",
    "epochs_range = range(1, len(history_final['train_loss']) + 1)\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "ax1 = axes[0]\n",
    "ax1.plot(epochs_range, history_final['train_loss'], 'b-', linewidth=2, markersize=4, label='Train Loss')\n",
    "ax1.plot(epochs_range, history_final['val_loss'], 'r-', linewidth=2, markersize=4, label='Val Loss')\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy curves\n",
    "ax2 = axes[1]\n",
    "ax2.plot(epochs_range, history_final['train_acc'], 'b-', linewidth=2, markersize=4, label='Train Accuracy')\n",
    "ax2.plot(epochs_range, history_final['val_acc'], 'r-', linewidth=2, markersize=4, label='Val Accuracy')\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/cnn_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Training history plot saved to: ../results/plots/cnn_training_history.png\")\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Total epochs: {len(epochs_range)}\")\n",
    "print(f\"  Final train loss: {history_final['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final train acc: {history_final['train_acc'][-1]:.2f}%\")\n",
    "print(f\"  Best val loss: {min(history_final['val_loss']):.4f} (epoch {np.argmin(history_final['val_loss']) + 1})\")\n",
    "print(f\"  Best val acc: {max(history_final['val_acc']):.2f}% (epoch {np.argmax(history_final['val_acc']) + 1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Evaluate FINAL Model on Fixed Test Set\n",
    "\n",
    "**ĐÂY LÀ ĐÁNH GIÁ CUỐI CÙNG:**\n",
    "- Dùng mô hình cuối (trained on 80%)\n",
    "- Đánh giá trên 20% fixed test set (chưa từng thấy)\n",
    "- Đây là metrics để báo cáo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 4: EVALUATE FINAL MODEL ON FIXED TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nEvaluating on fixed test set ({len(X_test_fixed)} samples)...\")\n",
    "\n",
    "test_metrics = trainer_final.evaluate(X_test_fixed, y_test_fixed, batch_size=CONFIG['batch_size'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL PERFORMANCE ON FIXED TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']*100:.2f}%)\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f} ({test_metrics['precision']*100:.2f}%)\")\n",
    "print(f\"Recall:    {test_metrics['recall']:.4f} ({test_metrics['recall']*100:.2f}%)\")\n",
    "print(f\"F1-Score:  {test_metrics['f1_score']:.4f} ({test_metrics['f1_score']*100:.2f}%)\")\n",
    "print(f\"ROC-AUC:   {test_metrics['roc_auc']:.4f} ({test_metrics['roc_auc']*100:.2f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✅ This is the FINAL performance to report!\")\n",
    "print(\"   Model: Trained on 80% train+val\")\n",
    "print(\"   Evaluated on: 20% fixed test set (never seen before)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (Fixed Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix for fixed test set\n",
    "cm_test = confusion_matrix(test_metrics['labels'], test_metrics['predictions'])\n",
    "\n",
    "# Class names for 4 classes\n",
    "class_names = ['Forest Stable', 'Deforestation', 'Non-forest', 'Reforestation']\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            annot_kws={'size': 14})\n",
    "ax.set_xlabel('Predicted', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('True', fontsize=13, fontweight='bold')\n",
    "ax.set_title(f'Confusion Matrix - Test Set (20  %)\\nAccuracy: {test_metrics[\"accuracy\"]:.4f} ({test_metrics[\"accuracy\"]*100:.2f}%)',\n",
    "                 fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/cnn_confusion_matrix_final.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report (Fixed Test Set):\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(test_metrics['labels'], test_metrics['predictions'],\n",
    "                          target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for Multi-class (One-vs-Rest)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "\n",
    "n_classes = 4\n",
    "y_test_bin = label_binarize(test_metrics['labels'], classes=[0, 1, 2, 3])\n",
    "y_score = test_metrics['probabilities']\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = cycle(['blue', 'red', 'green', 'orange'])\n",
    "class_names = ['Forest Stable (0)', 'Deforestation (1)', 'Non-forest (2)', 'Reforestation (3)']\n",
    "\n",
    "for i, color, name in zip(range(n_classes), colors, class_names):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'{name} (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - Multi-class (One-vs-Rest)', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/cnn_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC-AUC Scores:\")\n",
    "print(f\"  Class 0 (Forest Stable): {roc_auc[0]:.4f}\")\n",
    "print(f\"  Class 1 (Deforestation): {roc_auc[1]:.4f}\")\n",
    "print(f\"  Class 2 (Non-forest): {roc_auc[2]:.4f}\")\n",
    "print(f\"  Class 3 (Reforestation): {roc_auc[3]:.4f}\")\n",
    "print(f\"  Average: {np.mean(list(roc_auc.values())):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Predict Full Raster\n",
    "\n",
    "**Note:** This step may take 10-15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn.predictor import RasterPredictor\n",
    "\n",
    "print(\"Creating raster predictor...\")\n",
    "predictor = RasterPredictor(\n",
    "    model=model_final,  # Use FINAL model\n",
    "    device=CONFIG['device'],\n",
    "    patch_size=CONFIG['patch_size'],\n",
    "    batch_size=CONFIG['pred_batch_size']\n",
    ")\n",
    "\n",
    "print(\"\\nPredicting full raster using FINAL model...\")\n",
    "print(\"(This may take 10-15 minutes)\\n\")\n",
    "\n",
    "prediction_start = time.time()\n",
    "\n",
    "# Use same normalization as training\n",
    "multiclass_map = predictor.predict_raster(\n",
    "    feature_stack,\n",
    "    valid_mask,\n",
    "    stride=CONFIG['pred_stride'],\n",
    "    normalize=True,\n",
    "    normalization_stats=normalization_stats\n",
    ")\n",
    "\n",
    "prediction_time = time.time() - prediction_start\n",
    "print(f\"\\n✓ Prediction completed in {prediction_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rasters\n",
    "print(\"Saving results...\")\n",
    "\n",
    "output_dir = Path('../results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "(output_dir / 'rasters').mkdir(exist_ok=True)\n",
    "(output_dir / 'models').mkdir(exist_ok=True)\n",
    "(output_dir / 'data').mkdir(exist_ok=True)\n",
    "(output_dir / 'plots').mkdir(exist_ok=True)\n",
    "\n",
    "# Save multiclass raster only\n",
    "predictor.save_rasters(\n",
    "    metadata['s2_before'],\n",
    "    multiclass_path=output_dir / 'rasters' / 'cnn_multiclass_final.tif'\n",
    ")\n",
    "\n",
    "# Save FINAL model\n",
    "trainer_final.save_model(output_dir / 'models' / 'cnn_final_model.pth')\n",
    "\n",
    "# Compute confusion matrix for saving\n",
    "cm_test = confusion_matrix(test_metrics['labels'], test_metrics['predictions'])\n",
    "\n",
    "# Save comprehensive metrics\n",
    "metrics_to_save = {\n",
    "    'workflow': {\n",
    "        'description': 'Fixed test set (15%) + 5-fold CV + Final model training',\n",
    "        'step1': 'Split 15% fixed test set',\n",
    "        'step2': '5-fold CV on 85% train+val',\n",
    "        'step3': 'Train final model on full 85%',\n",
    "        'step4': 'Evaluate final model on 15% test'\n",
    "    },\n",
    "    'cv_results': {\n",
    "        'n_folds': 5,\n",
    "        'fold_results': fold_results,\n",
    "        'mean_val_acc': float(np.mean([r['val_acc'] for r in fold_results])),\n",
    "        'std_val_acc': float(np.std([r['val_acc'] for r in fold_results])),\n",
    "        'mean_val_f1': float(np.mean([r['val_f1'] for r in fold_results])),\n",
    "        'std_val_f1': float(np.std([r['val_f1'] for r in fold_results]))\n",
    "    },\n",
    "    'final_test': {\n",
    "        'accuracy': float(test_metrics['accuracy']),\n",
    "        'precision': float(test_metrics['precision']),\n",
    "        'recall': float(test_metrics['recall']),\n",
    "        'f1_score': float(test_metrics['f1_score']),\n",
    "        'roc_auc': float(test_metrics['roc_auc']),\n",
    "        'confusion_matrix': cm_test.tolist()\n",
    "    },\n",
    "    'data_split': {\n",
    "        'total_samples': len(ground_truth),\n",
    "        'trainval_samples': len(trainval_indices),\n",
    "        'test_samples': len(test_indices_fixed),\n",
    "        'trainval_percent': len(trainval_indices)/len(ground_truth)*100,\n",
    "        'test_percent': len(test_indices_fixed)/len(ground_truth)*100\n",
    "    },\n",
    "    'execution_times': {\n",
    "        'cv_seconds': float(cv_total_time),\n",
    "        'cv_minutes': float(cv_total_time / 60),\n",
    "        'training_seconds': float(training_time),\n",
    "        'training_minutes': float(training_time / 60),\n",
    "        'prediction_seconds': float(prediction_time),\n",
    "        'prediction_minutes': float(prediction_time / 60),\n",
    "        'total_minutes': float((cv_total_time + training_time + prediction_time) / 60)\n",
    "    },\n",
    "    'model_config': CONFIG\n",
    "}\n",
    "\n",
    "with open(output_dir / 'data' / 'cnn_final_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_to_save, f, indent=2)\n",
    "\n",
    "# Save training history\n",
    "with open(output_dir / 'data' / 'cnn_final_training_history.json', 'w') as f:\n",
    "    json.dump(history_final, f, indent=2)\n",
    "\n",
    "# Save CV results separately\n",
    "with open(output_dir / 'data' / 'cnn_cv_results.json', 'w') as f:\n",
    "    json.dump({'fold_results': fold_results}, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ All results saved successfully!\")\n",
    "print(\"\\n📁 Saved files:\")\n",
    "print(\"  - cnn_multiclass_final.tif (4-class raster)\")\n",
    "print(\"  - cnn_final_model.pth (model)\")\n",
    "print(\"  - cnn_final_metrics.json (all metrics)\")\n",
    "print(\"  - cnn_final_training_history.json\")\n",
    "print(\"  - cnn_cv_results.json (5-fold CV results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔟 Visualization using src.core.visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.visualization import Visualizer\n",
    "\n",
    "print(\"Creating visualizations using src.core.visualization...\")\n",
    "\n",
    "# Initialize Visualizer\n",
    "visualizer = Visualizer()\n",
    "\n",
    "# Plot 4-class multiclass map\n",
    "print(\"\\n1. Plotting 4-class multiclass map...\")\n",
    "visualizer.plot_multiclass_map(\n",
    "    multiclass_map=multiclass_map,\n",
    "    valid_mask=valid_mask,\n",
    "    output_path=output_dir / 'plots' / 'cnn_multiclass_map.png'\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Visualization created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION STATISTICS (4-Class)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count pixels for each class\n",
    "class_0_pixels = np.sum(multiclass_map[valid_mask] == 0)  # Forest Stable\n",
    "class_1_pixels = np.sum(multiclass_map[valid_mask] == 1)  # Deforestation\n",
    "class_2_pixels = np.sum(multiclass_map[valid_mask] == 2)  # Non-forest\n",
    "class_3_pixels = np.sum(multiclass_map[valid_mask] == 3)  # Reforestation\n",
    "total_valid = np.sum(valid_mask)\n",
    "\n",
    "print(f\"\\nTotal pixels:\")\n",
    "print(f\"  Valid pixels:          {total_valid:,}\")\n",
    "print(f\"  Invalid pixels:        {np.sum(~valid_mask):,}\")\n",
    "\n",
    "print(f\"\\n4-Class breakdown:\")\n",
    "print(f\"  Forest Stable (0):     {class_0_pixels:,} ({class_0_pixels/total_valid*100:.2f}%)\")\n",
    "print(f\"  Deforestation (1):     {class_1_pixels:,} ({class_1_pixels/total_valid*100:.2f}%)\")\n",
    "print(f\"  Non-forest (2):        {class_2_pixels:,} ({class_2_pixels/total_valid*100:.2f}%)\")\n",
    "print(f\"  Reforestation (3):     {class_3_pixels:,} ({class_3_pixels/total_valid*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nArea estimates (10m resolution):\")\n",
    "print(f\"  Forest Stable area:    {class_0_pixels * 100 / 10000:.2f} ha\")\n",
    "print(f\"  Deforestation area:    {class_1_pixels * 100 / 10000:.2f} ha\")\n",
    "print(f\"  Non-forest area:       {class_2_pixels * 100 / 10000:.2f} ha\")\n",
    "print(f\"  Reforestation area:    {class_3_pixels * 100 / 10000:.2f} ha\")\n",
    "print(f\"  Total valid area:      {total_valid * 100 / 10000:.2f} ha\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CNN DEFORESTATION DETECTION - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n🔄 WORKFLOW:\")\n",
    "print(\"  1. Split 15% fixed test set (~{} samples)\".format(len(test_indices_fixed)))\n",
    "print(\"  2. 5-fold CV on 85% train+val (~{} samples)\".format(len(trainval_indices)))\n",
    "print(\"  3. Train final model on full 85%\")\n",
    "print(\"  4. Evaluate final model on 15% test\")\n",
    "\n",
    "print(\"\\n📊 5-FOLD CV RESULTS (for hyperparameter selection):\")\n",
    "val_accs = [r['val_acc'] for r in fold_results]\n",
    "val_f1s = [r['val_f1'] for r in fold_results]\n",
    "print(f\"  Validation Accuracy: {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}\")\n",
    "print(f\"  Validation F1-Score: {np.mean(val_f1s):.4f} ± {np.std(val_f1s):.4f}\")\n",
    "\n",
    "print(\"\\n🎯 FINAL MODEL PERFORMANCE (on fixed test set):\")\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f} ({test_metrics['precision']*100:.2f}%)\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f} ({test_metrics['recall']*100:.2f}%)\")\n",
    "print(f\"  F1-Score:  {test_metrics['f1_score']:.4f} ({test_metrics['f1_score']*100:.2f}%)\")\n",
    "print(f\"  ROC-AUC:   {test_metrics['roc_auc']:.4f} ({test_metrics['roc_auc']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n⏱️  EXECUTION TIMES:\")\n",
    "print(f\"  5-Fold CV:  {cv_total_time/60:7.2f} minutes\")\n",
    "print(f\"  Training:   {training_time/60:7.2f} minutes\")\n",
    "print(f\"  Prediction: {prediction_time/60:7.2f} minutes\")\n",
    "print(f\"  Total:      {(cv_total_time + training_time + prediction_time)/60:7.2f} minutes\")\n",
    "\n",
    "print(\"\\n📁 OUTPUT FILES:\")\n",
    "print(f\"  Rasters:\")\n",
    "print(f\"    - cnn_multiclass_final.tif (4-class map)\")\n",
    "print(f\"  Models:\")\n",
    "print(f\"    - cnn_final_model.pth\")\n",
    "print(f\"  Data:\")\n",
    "print(f\"    - cnn_final_metrics.json (CV + final test metrics)\")\n",
    "print(f\"    - cnn_final_training_history.json\")\n",
    "print(f\"    - cnn_cv_results.json\")\n",
    "print(f\"  Plots:\")\n",
    "print(f\"    - cnn_confusion_matrix_final.png\")\n",
    "print(f\"    - cnn_roc_curve.png\")\n",
    "print(f\"    - cnn_multiclass_map.png (4-class visualization)\")\n",
    "\n",
    "print(\"\\n📍 RESULTS LOCATION:\")\n",
    "print(f\"  All results saved to: ../results/\")\n",
    "\n",
    "print(\"\\n✅ CNN PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"   ✓ Fixed test set approach (no data leakage)\")\n",
    "print(\"   ✓ 5-fold CV for hyperparameter validation\")\n",
    "print(\"   ✓ Final model trained on full train+val\")\n",
    "print(\"   ✓ Evaluation on truly held-out test set\")\n",
    "print(\"   ✓ 4-class multiclass map generated\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
