# PHÃ‚N TÃCH 3 MODELS: BAN, CHANGER, SNUNET-CD

**TÃ¡c giáº£:** Ninh Háº£i ÄÄƒng (21021411)
**NgÃ y:** 2025-10-17

## TÃ“M Táº®T QUYáº¾T Äá»ŠNH

**âœ… 3 models Ä‘Æ°á»£c chá»n: BAN, Changer, SNUNet-CD**

**LÃ½ do chÃ­nh:**
- âœ… **Táº¥t cáº£ Ä‘á»u há»— trá»£ 9 channels natively** (yÃªu cáº§u báº¯t buá»™c)
- âœ… Äa dáº¡ng vá» kiáº¿n trÃºc: Heavy Transformer / Medium Transformer / Lightweight CNN
- âœ… Äa dáº¡ng vá» design: Asymmetric / Symmetric + Interaction / Dense Connections
- âœ… PhÃ¹ há»£p GPU 16GB: Batch sizes 4/6/8 Ä‘á»u fit trong VRAM
- âœ… State-of-the-art: 2022-2024 (má»›i nháº¥t)

---

## PHÃ‚N TÃCH CHI TIáº¾T

### 1. BAN (Bi-temporal Adapter Network)

**ğŸ“„ Paper:** Chen et al., IEEE TGRS 2024

**ğŸ—ï¸ Architecture:**
- **Main Encoder:** CLIP ViT-B/16 (86M params)
  - Pretrained on 400M image-text pairs
  - Patch size: 16Ã—16
  - Embedding: 768 dimensions
- **Side Encoder:** MiT-B0 (3.3M params)
  - Pretrained on ImageNet
  - Hierarchical multi-scale features
- **Fusion:** Adapter-based bi-temporal fusion
- **Total Params:** ~90M

**ğŸ”§ Technical Details:**
- Input: 256Ã—256 (resized to 224Ã—224 for ViT)
- Batch size: 4
- Learning rate: 1e-4
- Training time: ~100 epochs Ã— 257 iters = 25,700 iterations
- **9-channel support:** âœ… Vision Transformer uses Conv2d patch embedding with `in_channels` parameter

**âš¡ Performance:**
- Speed: ~3-4s per 256Ã—256 tile
- Memory: ~8-10GB VRAM
- Expected F1: 0.89-0.91

**ğŸ‘ Æ¯u Ä‘iá»ƒm:**
- Strong semantic understanding tá»« CLIP
- Asymmetric design: CLIP cho high-level, MiT cho low-level
- State-of-the-art architecture (2024)

**ğŸ‘ NhÆ°á»£c Ä‘iá»ƒm:**
- Heavy model (~90M params)
- Slow inference
- Cáº§n pretrained CLIP weights

---

### 2. Changer (Feature Interaction Network)

**ğŸ“„ Paper:** Fang et al., IEEE TGRS 2023

**ğŸ—ï¸ Architecture:**
- **Backbone:** IA_MixVisionTransformer (MiT-B0)
  - Interaction-aware design
  - Spatial Exchange + Channel Exchange modules
  - Hierarchical features: [32, 64, 160, 256]
- **Total Params:** ~8-10M (dual encoder + decoder)

**ğŸ”§ Technical Details:**
- Input: 256Ã—256
- Batch size: 6
- Learning rate: 1e-4
- Training time: ~100 epochs Ã— 171 iters = 17,100 iterations
- **9-channel support:** âœ… MixVisionTransformer uses Conv2d patch embedding

**âš¡ Performance:**
- Speed: ~1.5-2s per 256Ã—256 tile
- Memory: ~6-8GB VRAM
- Expected F1: 0.88-0.90

**ğŸ‘ Æ¯u Ä‘iá»ƒm:**
- Explicit bi-temporal interaction (Spatial + Channel Exchange)
- Medium size (~8-10M params)
- Good balance: accuracy vs efficiency
- Pretrained on ImageNet

**ğŸ‘ NhÆ°á»£c Ä‘iá»ƒm:**
- Phá»©c táº¡p hÆ¡n CNN truyá»n thá»‘ng
- Cáº§n pretrained MiT weights

---

### 3. SNUNet-CD (Dense Siamese Network)

**ğŸ“„ Paper:** Fang et al., IEEE GRSL 2022

**ğŸ—ï¸ Architecture:**
- **Backbone:** Nested UNet with Dense Connections
  - Encoder: 5 levels [32, 64, 128, 256, 512]
  - Decoder: Dense skip connections (0_1, 0_2, 0_3, 0_4)
- **ECAM:** Enhanced Channel Attention Module
  - Inter-layer attention (ca1)
  - Intra-layer attention (ca)
- **Total Params:** ~4-8M (base_channels=32)

**ğŸ”§ Technical Details:**
- Input: 256Ã—256
- Batch size: 8
- Learning rate: 1e-3
- Training time: ~100 epochs Ã— 128 iters = 12,800 iterations
- **9-channel support:** âœ… Standard Conv2d with `in_channels` parameter

**âš¡ Performance:**
- Speed: ~0.8-1s per 256Ã—256 tile (fastest)
- Memory: ~4-6GB VRAM (smallest)
- Expected F1: 0.86-0.88

**ğŸ‘ Æ¯u Ä‘iá»ƒm:**
- Lightweight (~4-8M params)
- Fast inference (fastest trong 3 models)
- Train from scratch (khÃ´ng cáº§n pretrained)
- Pure CNN â†’ stable training
- Channel attention cho multi-spectral data

**ğŸ‘ NhÆ°á»£c Ä‘iá»ƒm:**
- Accuracy tháº¥p hÆ¡n Transformer models
- KhÃ´ng táº­n dá»¥ng pretrained weights

---

## SO SÃNH

| Criterion | BAN | Changer | SNUNet-CD |
|-----------|-----|---------|-----------|
| **Architecture** | Asymmetric Dual Transformer | Symmetric Transformer + Interaction | Dense Siamese CNN |
| **Parameters** | ~90M | ~8-10M | ~4-8M |
| **Pretrained** | CLIP + ImageNet | ImageNet | None (from scratch) |
| **Batch Size** | 4 | 6 | 8 |
| **Speed** | ~3-4s/tile (slow) | ~1.5-2s/tile (medium) | ~0.8-1s/tile (fast) |
| **Memory** | ~8-10GB | ~6-8GB | ~4-6GB |
| **Expected F1** | 0.89-0.91 (highest) | 0.88-0.90 (medium) | 0.86-0.88 (lowest) |
| **9-ch Support** | âœ… Patch embedding | âœ… Patch embedding | âœ… Conv2d |
| **Complexity** | High | Medium | Low |

---

## DIVERSITY ANALYSIS

### Architecture Diversity âœ…
- **BAN:** Heavy Transformer (ViT + MiT)
- **Changer:** Medium Transformer (MiT with Interaction)
- **SNUNet-CD:** Lightweight CNN (Nested UNet)

### Design Philosophy Diversity âœ…
- **BAN:** Asymmetric (different encoders for different purposes)
- **Changer:** Symmetric with explicit interaction (Spatial + Channel Exchange)
- **SNUNet-CD:** Symmetric with attention (Dense connections + ECAM)

### Pretrained Strategy Diversity âœ…
- **BAN:** CLIP (vision-language) + ImageNet (visual)
- **Changer:** ImageNet (visual only)
- **SNUNet-CD:** From scratch (no pretrained)

### Inference Speed Diversity âœ…
- **BAN:** Slow (~3-4s)
- **Changer:** Medium (~1.5-2s)
- **SNUNet-CD:** Fast (~0.8-1s)

---

## Táº I SAO KHÃ”NG CHá»ŒN TINYCD/TINYCDV2?

**âŒ TinyCDv2 bá»‹ loáº¡i vÃ¬:**

1. **Architecture hardcoded cho 3 channels:**
```python
# File: open-cd/opencd/models/backbones/tinycd.py:164
self._first_mix = MixingMaskAttentionBlock(6, 3, [3, 10, 5], [10, 5, 1])
```
- Sá»‘ `6` lÃ  cá»‘ Ä‘á»‹nh = 3 channels Ã— 2 timesteps
- `MixingBlock` dÃ¹ng grouped convolution vá»›i `groups=3`
- Nháº­n 18 channels nhÆ°ng expect 6 â†’ Error

2. **EfficientNet backbone pretrained trÃªn RGB:**
```python
entire_model = torchvision.models.efficientnet_b4(pretrained=True).features
```
- Pretrained weights cho 3 channels
- KhÃ´ng thá»ƒ load weights cho 9 channels

3. **Äá»ƒ sá»­a pháº£i rewrite toÃ n bá»™ architecture:**
- Thay Ä‘á»•i táº¥t cáº£ MixingBlock layers
- Loáº¡i bá» pretrained weights
- Máº¥t Æ°u Ä‘iá»ƒm "lightweight + pretrained"

**Quyáº¿t Ä‘á»‹nh:** Thay TinyCDv2 â†’ SNUNet-CD
- SNUNet-CD cÅ©ng lightweight (~4-8M vs ~1.5M)
- Native support 9 channels
- Pure CNN nhÆ° TinyCDv2
- CÃ³ ECAM attention mechanism

---

## Káº¾T LUáº¬N

### âœ… Bá»™ 3 models (BAN, Changer, SNUNet-CD) lÃ  lá»±a chá»n tá»‘t nháº¥t vÃ¬:

1. **Táº¥t cáº£ Ä‘á»u há»— trá»£ 9 channels natively** â† YÃªu cáº§u báº¯t buá»™c
2. **Äa dáº¡ng tá»‘i Ä‘a:**
   - Architecture: Transformer (Heavy/Medium) vs CNN (Lightweight)
   - Design: Asymmetric vs Symmetric + Interaction vs Dense + Attention
   - Pretrained: CLIP+ImageNet vs ImageNet vs From Scratch
   - Speed: Slow vs Medium vs Fast
3. **Fit GPU 16GB:** Batch sizes 4/6/8 Ä‘á»u cháº¡y Ä‘Æ°á»£c
4. **State-of-the-art:** Papers tá»« 2022-2024
5. **Comprehensive comparison:** Cover nhiá»u khÃ­a cáº¡nh khÃ¡c nhau

### ğŸ“Š Expected Outcome:
- BAN: Highest accuracy (0.89-0.91) but slowest
- Changer: Good balance (0.88-0.90) between accuracy and speed
- SNUNet-CD: Fastest (0.86-0.88) but lowest accuracy

### ğŸ¯ Thesis Value:
- So sÃ¡nh Transformer vs CNN cho multi-spectral change detection
- PhÃ¢n tÃ­ch trade-off: accuracy vs speed vs model size
- ÄÃ¡nh giÃ¡ vai trÃ² cá»§a pretrained weights (CLIP/ImageNet/None)
- Khuyáº¿n nghá»‹ deployment cho production (SNUNet-CD for speed, BAN for accuracy)

---

**Status:** âœ… ÄÃ£ xÃ¡c nháº­n 3 models phÃ¹ há»£p vá» má»i máº·t
**Next:** Báº¯t Ä‘áº§u training (SNUNet-CD â†’ Changer â†’ BAN)
