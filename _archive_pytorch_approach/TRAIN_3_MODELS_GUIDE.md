# ğŸ¯ Training All 3 Models - Complete Guide

## ğŸ“‹ Overview

Báº¡n giá» cÃ³ **4 notebooks riÃªng** Ä‘á»ƒ train vÃ  compare 3 models:

1. **`1a_train_unet_mobilenet.ipynb`** - UNet-MobileNetV2 (Fastest)
2. **`1b_train_unet_efficientnet.ipynb`** - UNet-EfficientNet-B0 (Balanced) â­
3. **`1c_train_fpn_efficientnet.ipynb`** - FPN-EfficientNet-B0 (Most Accurate)
4. **`1d_compare_all_models.ipynb`** - Compare all 3 models

---

## ğŸ¯ Model Specifications

### Optimized for 12GB VRAM Usage

| Model | Batch Size | VRAM Usage | Parameters | Speed | Accuracy |
|-------|-----------|------------|------------|-------|----------|
| **UNet-MobileNetV2** | 64 | ~3GB | ~2M | âš¡âš¡âš¡âš¡ | â­â­â­ |
| **UNet-EfficientNet** â­ | 48 | ~10-12GB | ~5M | âš¡âš¡âš¡ | â­â­â­â­ |
| **FPN-EfficientNet** | 32 | ~12GB | ~6M | âš¡âš¡ | â­â­â­â­â­ |

â­ = Recommended (best balance)

---

## ğŸš€ Training Workflow

### Option 1: Train All 3 Models Sequentially

**Recommended approach:**

```bash
jupyter notebook
```

Then run notebooks in order:
1. `1a_train_unet_mobilenet.ipynb` â†’ ~8-10 giá»
2. `1b_train_unet_efficientnet.ipynb` â†’ ~10-12 giá»
3. `1c_train_fpn_efficientnet.ipynb` â†’ ~12-14 giá»
4. `1d_compare_all_models.ipynb` â†’ ~5 phÃºt

**Total time:** ~30-36 giá» (cÃ³ thá»ƒ Ä‘á»ƒ cháº¡y xuyÃªn Ä‘Ãªm)

### Option 2: Train Parallel (Multiple GPUs)

Náº¿u cÃ³ nhiá»u GPUs, má»Ÿ 3 Jupyter sessions riÃªng:

```bash
# Terminal 1
CUDA_VISIBLE_DEVICES=0 jupyter notebook 1a_train_unet_mobilenet.ipynb

# Terminal 2
CUDA_VISIBLE_DEVICES=1 jupyter notebook 1b_train_unet_efficientnet.ipynb

# Terminal 3 (náº¿u cÃ³ GPU thá»© 3)
CUDA_VISIBLE_DEVICES=2 jupyter notebook 1c_train_fpn_efficientnet.ipynb
```

### Option 3: Train Only Best Model

Náº¿u chá»‰ muá»‘n train 1 model tá»‘t nháº¥t:

```bash
# Recommended
jupyter notebook notebooks/1b_train_unet_efficientnet.ipynb
```

---

## ğŸ“Š Batch Size Optimization

### Táº¡i sao batch size khÃ¡c nhau?

```python
# UNet-MobileNetV2 (lightweight)
BATCH_SIZE = 64  # Nháº¹ â†’ batch size lá»›n â†’ train nhanh hÆ¡n

# UNet-EfficientNet (medium)
BATCH_SIZE = 48  # Vá»«a â†’ batch size vá»«a â†’ sá»­ dá»¥ng ~10-12GB VRAM

# FPN-EfficientNet (heavy)
BATCH_SIZE = 32  # Náº·ng â†’ batch size nhá» â†’ trÃ¡nh OOM, sá»­ dá»¥ng ~12GB VRAM
```

### Náº¿u GPU cá»§a báº¡n khÃ¡c 16GB:

**8GB VRAM:**
```python
# Giáº£m batch size:
BATCH_SIZE = 32  # UNet-MobileNetV2
BATCH_SIZE = 16  # UNet-EfficientNet
BATCH_SIZE = 8   # FPN-EfficientNet
```

**24GB+ VRAM:**
```python
# TÄƒng batch size Ä‘á»ƒ train nhanh hÆ¡n:
BATCH_SIZE = 128  # UNet-MobileNetV2
BATCH_SIZE = 64   # UNet-EfficientNet
BATCH_SIZE = 48   # FPN-EfficientNet
```

---

## ğŸ“ Output Structure

Sau khi train xong, báº¡n sáº½ cÃ³:

```
models/
â”œâ”€â”€ unet_mobilenet/
â”‚   â”œâ”€â”€ best_model.pth              # Best checkpoint
â”‚   â”œâ”€â”€ checkpoint_epoch_10.pth     # Checkpoint @ epoch 10
â”‚   â”œâ”€â”€ checkpoint_epoch_20.pth     # Checkpoint @ epoch 20
â”‚   â””â”€â”€ training_history.png        # Training curves
â”‚
â”œâ”€â”€ unet_efficientnet/
â”‚   â”œâ”€â”€ best_model.pth
â”‚   â”œâ”€â”€ checkpoint_epoch_10.pth
â”‚   â””â”€â”€ training_history.png
â”‚
â””â”€â”€ fpn_efficientnet/
    â”œâ”€â”€ best_model.pth
    â”œâ”€â”€ checkpoint_epoch_10.pth
    â””â”€â”€ training_history.png

results/
â”œâ”€â”€ model_comparison.csv           # Comparison table
â””â”€â”€ model_comparison.png           # Comparison charts
```

---

## ğŸ“ˆ Expected Results

### Performance Range (on test set):

| Model | Accuracy | F1 Score | Training Time |
|-------|----------|----------|---------------|
| UNet-MobileNetV2 | 83-87% | 0.82-0.86 | ~8-10 giá» |
| UNet-EfficientNet | 87-91% | 0.86-0.90 | ~10-12 giá» |
| FPN-EfficientNet | 89-93% | 0.88-0.92 | ~12-14 giá» |

**Note:** Actual results depend on data quality vÃ  random seed.

---

## ğŸ¨ Live Monitoring Features

Má»—i notebook cÃ³:

### 1. Progress Bars
```
Overall Progress: 20%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/50 epochs
Epoch 10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:30<00:00]
  â””â”€ loss: 0.3214  acc: 87.32%
Epoch 10 [Val]  : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00]
```

### 2. Live Plots
- Loss curve (train vs val)
- Accuracy curve (train vs val)
- F1 Score curve
- Learning Rate schedule

### 3. Auto-save
- Best model checkpoint
- Training history plots
- Epoch-wise checkpoints (every 10 epochs)

---

## ğŸ”§ Customization

### Training Config

Trong má»—i notebook cell 2:

```python
# Adjust these:
BATCH_SIZE = 48       # Change based on your GPU
EPOCHS = 50           # Increase/decrease
LEARNING_RATE = 1e-4  # Try 1e-3 for faster convergence
EARLY_STOPPING_PATIENCE = 10  # Patience for early stopping
```

### Quick Test Run

Äá»ƒ test nhanh (5-10 phÃºt):

```python
EPOCHS = 5
EARLY_STOPPING_PATIENCE = 3
```

### Production Training

Äá»ƒ train ká»¹ hÆ¡n (24+ giá»):

```python
EPOCHS = 100
EARLY_STOPPING_PATIENCE = 20
```

---

## ğŸ’¡ Tips

### 1. Monitor GPU Usage

```python
# Trong cell Ä‘áº§u tiÃªn cá»§a notebook
import torch
print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Sau má»—i epoch, check:
print(f"GPU Memory Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
```

### 2. Save Intermediate Results

Models tá»± Ä‘á»™ng save má»—i 10 epochs:
- `checkpoint_epoch_10.pth`
- `checkpoint_epoch_20.pth`
- etc.

Náº¿u training bá»‹ interrupt, cÃ³ thá»ƒ resume tá»« checkpoint cuá»‘i.

### 3. Compare Before Choosing

Cháº¡y notebook `1d_compare_all_models.ipynb` sau khi train xong Ä‘á»ƒ xem model nÃ o tá»‘t nháº¥t cho data cá»§a báº¡n.

### 4. Use Best Model for Inference

Sau khi compare, dÃ¹ng model tá»‘t nháº¥t trong notebook 2 (inference):

```python
# Trong notebook 2_inference_wholescene.ipynb
MODEL_NAME = 'unet_efficientnet'  # Hoáº·c model nÃ o tá»‘t nháº¥t
```

---

## ğŸ¯ Training Schedule

### Káº¿ hoáº¡ch training 3 models:

**Day 1 (Evening):**
- 6 PM: Start `1a_train_unet_mobilenet.ipynb`
- 2 AM next day: Finish

**Day 2 (Evening):**
- 6 PM: Start `1b_train_unet_efficientnet.ipynb`
- 4 AM next day: Finish

**Day 3 (Evening):**
- 6 PM: Start `1c_train_fpn_efficientnet.ipynb`
- 6 AM next day: Finish

**Day 4:**
- Run `1d_compare_all_models.ipynb` (5 phÃºt)
- Choose best model
- Run inference vá»›i best model

---

## ğŸ” Troubleshooting

### OOM (Out of Memory)

```python
# Giáº£m batch size trong cell 2
BATCH_SIZE = 16  # Thay vÃ¬ 48
```

### Training Too Slow

```python
# TÄƒng batch size (náº¿u GPU cho phÃ©p)
BATCH_SIZE = 64

# Hoáº·c giáº£m epochs
EPOCHS = 30
```

### Model Not Improving

- Check learning rate (cÃ³ thá»ƒ quÃ¡ cao hoáº·c quÃ¡ tháº¥p)
- Check data quality
- Try different optimizer:
  ```python
  optimizer = get_optimizer(model, 'adam', lr=1e-3)  # Thay vÃ¬ adamw
  ```

---

## âœ… Checklist

Before starting training:

- [ ] GPU cÃ³ Ä‘á»§ VRAM (12GB+ recommended)
- [ ] Data Ä‘Ã£ Ä‘áº·t trong `data/raw/`
- [ ] CSV file cÃ³ Ä‘á»§ 1,285 points
- [ ] 4 áº£nh TIFF tá»“n táº¡i vÃ  readable
- [ ] ÄÃ£ cÃ i `segmentation-models-pytorch`
- [ ] CÃ³ Ä‘á»§ disk space (~5GB cho models + logs)

During training:

- [ ] Monitor GPU usage
- [ ] Check live plots má»—i 5-10 epochs
- [ ] Note down best validation accuracy
- [ ] Save training logs/screenshots

After training:

- [ ] Run comparison notebook
- [ ] Compare all 3 models
- [ ] Choose best model
- [ ] Use best model for inference

---

## ğŸ‰ Summary

Báº¡n giá» cÃ³ **complete pipeline** Ä‘á»ƒ train vÃ  compare 3 models:

âœ… **3 training notebooks** - Má»—i model cÃ³ notebook riÃªng vá»›i batch size tá»‘i Æ°u
âœ… **1 comparison notebook** - So sÃ¡nh káº¿t quáº£ tá»± Ä‘á»™ng
âœ… **12GB VRAM optimized** - Sá»­ dá»¥ng Ä‘á»§ GPU memory
âœ… **Live monitoring** - Progress bars + real-time plots
âœ… **Auto-save** - KhÃ´ng máº¥t cÃ´ng training náº¿u interrupt

ChÃºc báº¡n training thÃ nh cÃ´ng! ğŸš€
