{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b351ab8a",
   "metadata": {},
   "source": [
    "# ECOSTRESS H5 to GeoTIFF Converter\n",
    "\n",
    "Script để chuyển đổi các file ECOSTRESS HDF5 (.h5) thành định dạng GeoTIFF (.tif) cho việc phân tích dữ liệu độ ẩm đất.\n",
    "\n",
    "**Tác giả**: [Tên của bạn]  \n",
    "**Ngày tạo**: 12/09/2025  \n",
    "**Mục đích**: Chuyển đổi batch các file ECOSTRESS từ HDF5 sang GeoTIFF format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980171b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55b6280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "h5py version: 3.14.0\n",
      "rasterio version: 1.4.3\n",
      "numpy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.crs import CRS\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"h5py version:\", h5py.version.version)\n",
    "print(\"rasterio version:\", rasterio.__version__)\n",
    "print(\"numpy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35291bce",
   "metadata": {},
   "source": [
    "## 2. Define File Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76570c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/ninhhaidang/Library/CloudStorage/GoogleDrive-ninhhailongg@gmail.com/My Drive/Cac_mon_hoc/Do_an_tot_nghiep/25-26_HKI_DATN_21021411_DangNH/Data/ECOSTRESS Gridded Downscaled Soil Moisture Instantaneous L3 Global 70 m V002\n",
      "Output directory: /Users/ninhhaidang/Library/CloudStorage/GoogleDrive-ninhhailongg@gmail.com/My Drive/Cac_mon_hoc/Do_an_tot_nghiep/25-26_HKI_DATN_21021411_DangNH/Data/ECOSTRESS Gridded Downscaled Soil Moisture Instantaneous L3 Global 70 m V002/output_tiff_converted\n",
      "\n",
      "Found 4 H5 files:\n",
      "  - ECOv002_L3G_SM_27979_003_20230612T235750_0712_01.h5\n",
      "  - ECOv002_L3G_SM_27857_003_20230605T031507_0712_01.h5\n",
      "  - ECOv002_L3G_SM_27811_005_20230602T040448_0712_01.h5\n",
      "  - ECOv002_L3G_SM_27796_006_20230601T045347_0712_01.h5\n"
     ]
    }
   ],
   "source": [
    "# Define paths and configuration\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Input directory containing H5 files\n",
    "input_dir = current_dir\n",
    "output_dir = os.path.join(current_dir, \"output_tiff_converted\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Find all H5 files in the current directory\n",
    "h5_files = glob.glob(os.path.join(input_dir, \"*.h5\"))\n",
    "print(f\"\\nFound {len(h5_files)} H5 files:\")\n",
    "for file in h5_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Configuration parameters\n",
    "FILL_VALUE = -9999.0  # Fill value for invalid pixels\n",
    "SCALE_FACTOR = 0.0001  # Scale factor for soil moisture data\n",
    "CRS_EPSG = 4326  # WGS84 coordinate system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b85c31",
   "metadata": {},
   "source": [
    "## 3. Read HDF5 File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df8df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring structure of: ECOv002_L3G_SM_27979_003_20230612T235750_0712_01.h5\n",
      "============================================================\n",
      "HDFEOS/ (Group)\n",
      "HDFEOS/ADDITIONAL/ (Group)\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/ (Group)\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/ProductMetadata/ (Group)\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/ProductMetadata/AncillaryNWP (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/ProductMetadata/BandSpecification (Dataset): shape=(6,), dtype=float32\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/ProductMetadata/NWPSource (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/ProductMetadata/NumberOfBands (Dataset): shape=(1,), dtype=uint8\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/ProductMetadata/OrbitCorrectionPerformed (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/ProductMetadata/QAPercentCloudCover (Dataset): shape=(1,), dtype=int32\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/ProductMetadata/QAPercentGoodQuality (Dataset): shape=(1,), dtype=int32\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ (Group)\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/AncillaryInputPointer (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/AutomaticQualityFlag (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/BuildID (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/CRS (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/CampaignShortName (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/CollectionLabel (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/DataFormatType (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/DayNightFlag (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/EastBoundingCoordinate (Dataset): shape=(1,), dtype=float64\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/FieldOfViewObstruction (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/HDFVersionID (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ImageLineSpacing (Dataset): shape=(1,), dtype=float64\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ImageLines (Dataset): shape=(1,), dtype=int32\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ImagePixelSpacing (Dataset): shape=(1,), dtype=float64\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ImagePixels (Dataset): shape=(1,), dtype=int32\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/InputPointer (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/InstrumentShortName (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/LocalGranuleID (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/LongName (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/NorthBoundingCoordinate (Dataset): shape=(1,), dtype=float64\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/PGEName (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/PGEVersion (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/PlatformLongName (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/PlatformShortName (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/PlatformType (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ProcessingEnvironment (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ProcessingLevelDescription (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ProcessingLevelID (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ProducerAgency (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ProducerInstitution (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ProductionDateTime (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ProductionLocation (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/RangeBeginningDate (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/RangeBeginningTime (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/RangeEndingDate (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/RangeEndingTime (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/RegionID (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/SISName (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/SISVersion (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/SceneBoundaryLatLonWKT (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/SceneID (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/ShortName (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/SouthBoundingCoordinate (Dataset): shape=(1,), dtype=float64\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/StartOrbitNumber (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/StopOrbitNumber (Dataset): shape=(), dtype=object\n",
      "HDFEOS/ADDITIONAL/FILE_ATTRIBUTES/StandardMetadata/WestBoundingCoordinate (Dataset): shape=(1,), dtype=float64\n",
      "HDFEOS/GRIDS/ (Group)\n",
      "HDFEOS/GRIDS/ECO_L3G_SM_70m/ (Group)\n",
      "HDFEOS/GRIDS/ECO_L3G_SM_70m/Data Fields/ (Group)\n",
      "HDFEOS/GRIDS/ECO_L3G_SM_70m/Data Fields/SM (Dataset): shape=(6970, 7800), dtype=float32\n",
      "HDFEOS/GRIDS/ECO_L3G_SM_70m/Data Fields/cloud (Dataset): shape=(6970, 7800), dtype=float32\n",
      "HDFEOS/GRIDS/ECO_L3G_SM_70m/Data Fields/water (Dataset): shape=(6970, 7800), dtype=float32\n",
      "HDFEOS INFORMATION/ (Group)\n",
      "HDFEOS INFORMATION/StructMetadata.0 (Dataset): shape=(), dtype=|S32000\n",
      "\n",
      "File-level attributes:\n"
     ]
    }
   ],
   "source": [
    "# Explore structure of a sample H5 file\n",
    "def explore_h5_structure(h5_file_path):\n",
    "    \"\"\"Explore the structure of an HDF5 file\"\"\"\n",
    "    print(f\"Exploring structure of: {os.path.basename(h5_file_path)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        def print_structure(name, obj, level=0):\n",
    "            indent = \"  \" * level\n",
    "            if isinstance(obj, h5py.Group):\n",
    "                print(f\"{indent}{name}/ (Group)\")\n",
    "            elif isinstance(obj, h5py.Dataset):\n",
    "                print(f\"{indent}{name} (Dataset): shape={obj.shape}, dtype={obj.dtype}\")\n",
    "                # Print attributes\n",
    "                for attr_name, attr_value in obj.attrs.items():\n",
    "                    print(f\"{indent}  @{attr_name}: {attr_value}\")\n",
    "        \n",
    "        f.visititems(print_structure)\n",
    "        \n",
    "        # Print file-level attributes\n",
    "        print(f\"\\nFile-level attributes:\")\n",
    "        for attr_name, attr_value in f.attrs.items():\n",
    "            print(f\"  @{attr_name}: {attr_value}\")\n",
    "\n",
    "# Explore the first H5 file\n",
    "if h5_files:\n",
    "    explore_h5_structure(h5_files[0])\n",
    "else:\n",
    "    print(\"No H5 files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210f453",
   "metadata": {},
   "source": [
    "## 4. Extract Soil Moisture Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b649a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting data: Could not find soil moisture dataset\n",
      "Extraction failed!\n"
     ]
    }
   ],
   "source": [
    "def extract_soil_moisture_data(h5_file_path):\n",
    "    \"\"\"Extract soil moisture data from ECOSTRESS HDF5 file\"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        # Extract soil moisture data\n",
    "        # The exact path may vary based on the file structure\n",
    "        # Common paths in ECOSTRESS files:\n",
    "        try:\n",
    "            # Try different possible paths for soil moisture data\n",
    "            possible_sm_paths = [\n",
    "                'SoilMoisture',\n",
    "                'Soil_Moisture', \n",
    "                'SM',\n",
    "                'Soil_Moisture_sm',\n",
    "                'data/SoilMoisture'\n",
    "            ]\n",
    "            \n",
    "            sm_data = None\n",
    "            sm_path = None\n",
    "            \n",
    "            for path in possible_sm_paths:\n",
    "                if path in f:\n",
    "                    sm_data = f[path][:]\n",
    "                    sm_path = path\n",
    "                    print(f\"Found soil moisture data at: {path}\")\n",
    "                    break\n",
    "            \n",
    "            if sm_data is None:\n",
    "                # If standard paths don't work, look for datasets with 'soil' or 'moisture' in name\n",
    "                def find_sm_dataset(name, obj):\n",
    "                    if isinstance(obj, h5py.Dataset):\n",
    "                        if 'soil' in name.lower() or 'moisture' in name.lower():\n",
    "                            return name\n",
    "                    return None\n",
    "                \n",
    "                # Search through all datasets\n",
    "                for name, obj in f.items():\n",
    "                    if isinstance(obj, h5py.Dataset):\n",
    "                        if 'soil' in name.lower() or 'moisture' in name.lower():\n",
    "                            sm_data = obj[:]\n",
    "                            sm_path = name\n",
    "                            print(f\"Found soil moisture data at: {name}\")\n",
    "                            break\n",
    "            \n",
    "            if sm_data is None:\n",
    "                raise ValueError(\"Could not find soil moisture dataset\")\n",
    "                \n",
    "            data_dict['soil_moisture'] = sm_data\n",
    "            data_dict['sm_path'] = sm_path\n",
    "            \n",
    "            # Get data attributes if available\n",
    "            sm_dataset = f[sm_path]\n",
    "            data_dict['attributes'] = dict(sm_dataset.attrs)\n",
    "            \n",
    "            # Extract geospatial information\n",
    "            # Look for latitude and longitude datasets\n",
    "            for coord_name in ['lat', 'latitude', 'Latitude', 'lon', 'longitude', 'Longitude']:\n",
    "                if coord_name in f:\n",
    "                    if 'lat' in coord_name.lower():\n",
    "                        data_dict['latitude'] = f[coord_name][:]\n",
    "                    else:\n",
    "                        data_dict['longitude'] = f[coord_name][:]\n",
    "            \n",
    "            # If coordinates are not found as separate datasets, check for geospatial metadata\n",
    "            if 'latitude' not in data_dict or 'longitude' not in data_dict:\n",
    "                # Look for geospatial metadata in attributes\n",
    "                for attr_name, attr_value in f.attrs.items():\n",
    "                    if 'north' in attr_name.lower() or 'south' in attr_name.lower():\n",
    "                        if 'north' in attr_name.lower():\n",
    "                            data_dict['north_bound'] = float(attr_value)\n",
    "                        else:\n",
    "                            data_dict['south_bound'] = float(attr_value)\n",
    "                    elif 'east' in attr_name.lower() or 'west' in attr_name.lower():\n",
    "                        if 'east' in attr_name.lower():\n",
    "                            data_dict['east_bound'] = float(attr_value)\n",
    "                        else:\n",
    "                            data_dict['west_bound'] = float(attr_value)\n",
    "            \n",
    "            print(f\"Data shape: {sm_data.shape}\")\n",
    "            print(f\"Data type: {sm_data.dtype}\")\n",
    "            print(f\"Data range: {np.nanmin(sm_data)} to {np.nanmax(sm_data)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# Test extraction with the first file\n",
    "if h5_files:\n",
    "    sample_data = extract_soil_moisture_data(h5_files[0])\n",
    "    if sample_data:\n",
    "        print(\"\\nExtraction successful!\")\n",
    "        print(f\"Available keys: {list(sample_data.keys())}\")\n",
    "    else:\n",
    "        print(\"Extraction failed!\")\n",
    "else:\n",
    "    print(\"No H5 files available for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25dc2e",
   "metadata": {},
   "source": [
    "## 5. Handle Geospatial Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd20c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geospatial_info(data_dict, h5_file_path):\n",
    "    \"\"\"Create geospatial information for the raster\"\"\"\n",
    "    \n",
    "    # Try to get geospatial info from the data or calculate from coordinates\n",
    "    if 'latitude' in data_dict and 'longitude' in data_dict:\n",
    "        # If we have coordinate arrays\n",
    "        lat = data_dict['latitude']\n",
    "        lon = data_dict['longitude']\n",
    "        \n",
    "        # Calculate bounds\n",
    "        north = np.max(lat)\n",
    "        south = np.min(lat)\n",
    "        east = np.max(lon)\n",
    "        west = np.min(lon)\n",
    "        \n",
    "    elif all(key in data_dict for key in ['north_bound', 'south_bound', 'east_bound', 'west_bound']):\n",
    "        # If we have bounds from metadata\n",
    "        north = data_dict['north_bound']\n",
    "        south = data_dict['south_bound']\n",
    "        east = data_dict['east_bound']\n",
    "        west = data_dict['west_bound']\n",
    "        \n",
    "    else:\n",
    "        # Default bounds if no geospatial info found (this is a fallback)\n",
    "        print(\"Warning: No geospatial information found, using default global bounds\")\n",
    "        north, south, east, west = 90, -90, 180, -180\n",
    "    \n",
    "    # Get data dimensions\n",
    "    sm_data = data_dict['soil_moisture']\n",
    "    height, width = sm_data.shape[-2:]  # Get last two dimensions\n",
    "    \n",
    "    # Create affine transform\n",
    "    transform = from_bounds(west, south, east, north, width, height)\n",
    "    \n",
    "    geospatial_info = {\n",
    "        'transform': transform,\n",
    "        'crs': CRS.from_epsg(CRS_EPSG),\n",
    "        'height': height,\n",
    "        'width': width,\n",
    "        'bounds': {\n",
    "            'north': north,\n",
    "            'south': south,\n",
    "            'east': east,\n",
    "            'west': west\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Geospatial bounds: North={north:.6f}, South={south:.6f}, East={east:.6f}, West={west:.6f}\")\n",
    "    print(f\"Raster dimensions: {height} x {width}\")\n",
    "    print(f\"Transform: {transform}\")\n",
    "    \n",
    "    return geospatial_info\n",
    "\n",
    "# Test geospatial info creation\n",
    "if 'sample_data' in globals() and sample_data:\n",
    "    geo_info = create_geospatial_info(sample_data, h5_files[0])\n",
    "    print(\"\\\\nGeospatial info created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b2762",
   "metadata": {},
   "source": [
    "## 6. Convert to GeoTIFF Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb31e128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conversion with the first file...\n",
      "\\nProcessing: ECOv002_L3G_SM_27979_003_20230612T235750_0712_01.h5\n",
      "--------------------------------------------------\n",
      "Error extracting data: Could not find soil moisture dataset\n",
      "Failed to extract data from H5 file\n",
      "Test conversion failed!\n"
     ]
    }
   ],
   "source": [
    "def h5_to_geotiff(h5_file_path, output_dir, output_filename=None):\n",
    "    \"\"\"\n",
    "    Convert ECOSTRESS H5 file to GeoTIFF format\n",
    "    \n",
    "    Parameters:\n",
    "    h5_file_path: path to input H5 file\n",
    "    output_dir: directory to save output TIFF file\n",
    "    output_filename: custom output filename (optional)\n",
    "    \n",
    "    Returns:\n",
    "    output_path: path to created TIFF file, or None if failed\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\\\nProcessing: {os.path.basename(h5_file_path)}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Extract data from H5 file\n",
    "        data_dict = extract_soil_moisture_data(h5_file_path)\n",
    "        if not data_dict:\n",
    "            print(\"Failed to extract data from H5 file\")\n",
    "            return None\n",
    "        \n",
    "        # Create geospatial information\n",
    "        geo_info = create_geospatial_info(data_dict, h5_file_path)\n",
    "        \n",
    "        # Get soil moisture data\n",
    "        sm_data = data_dict['soil_moisture']\n",
    "        \n",
    "        # Handle data preprocessing\n",
    "        # Convert to float32 and handle fill values\n",
    "        sm_data = sm_data.astype(np.float32)\n",
    "        \n",
    "        # Apply scale factor if available in attributes\n",
    "        if 'scale_factor' in data_dict['attributes']:\n",
    "            scale_factor = data_dict['attributes']['scale_factor']\n",
    "            print(f\"Applying scale factor: {scale_factor}\")\n",
    "            sm_data = sm_data * scale_factor\n",
    "        elif SCALE_FACTOR != 1.0:\n",
    "            print(f\"Applying default scale factor: {SCALE_FACTOR}\")\n",
    "            sm_data = sm_data * SCALE_FACTOR\n",
    "        \n",
    "        # Handle fill values\n",
    "        if '_FillValue' in data_dict['attributes']:\n",
    "            fill_val = data_dict['attributes']['_FillValue']\n",
    "            sm_data[sm_data == fill_val] = np.nan\n",
    "        \n",
    "        # Set very high or low values to NaN (likely invalid)\n",
    "        sm_data[sm_data < 0] = np.nan\n",
    "        sm_data[sm_data > 1] = np.nan  # Soil moisture should be between 0 and 1\n",
    "        \n",
    "        # Generate output filename if not provided\n",
    "        if output_filename is None:\n",
    "            # Extract date from filename\n",
    "            base_name = os.path.basename(h5_file_path)\n",
    "            # Try to extract date (format: YYYYMMDD)\n",
    "            date_match = re.search(r'(\\\\d{8})', base_name)\n",
    "            if date_match:\n",
    "                date_str = date_match.group(1)\n",
    "            else:\n",
    "                # Fallback to timestamp\n",
    "                date_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            output_filename = f\"ECOSTRESS_SM_{date_str}.tif\"\n",
    "        \n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        # Write GeoTIFF file\n",
    "        with rasterio.open(\n",
    "            output_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=geo_info['height'],\n",
    "            width=geo_info['width'],\n",
    "            count=1,\n",
    "            dtype=sm_data.dtype,\n",
    "            crs=geo_info['crs'],\n",
    "            transform=geo_info['transform'],\n",
    "            compress='lzw',  # Use LZW compression\n",
    "            nodata=np.nan\n",
    "        ) as dst:\n",
    "            # Ensure data is 2D\n",
    "            if sm_data.ndim > 2:\n",
    "                sm_data = sm_data.squeeze()\n",
    "            \n",
    "            dst.write(sm_data, 1)\n",
    "            \n",
    "            # Add metadata\n",
    "            dst.update_tags(\n",
    "                source_file=os.path.basename(h5_file_path),\n",
    "                creation_date=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                data_type=\"ECOSTRESS Soil Moisture\",\n",
    "                units=\"m³/m³\",\n",
    "                valid_range=\"0.0 to 1.0\"\n",
    "            )\n",
    "        \n",
    "        print(f\"Successfully created: {output_filename}\")\n",
    "        print(f\"Output path: {output_path}\")\n",
    "        \n",
    "        # Verify the created file\n",
    "        with rasterio.open(output_path) as src:\n",
    "            print(f\"Verification - Shape: {src.shape}, CRS: {src.crs}\")\n",
    "            print(f\"Bounds: {src.bounds}\")\n",
    "            data_sample = src.read(1)\n",
    "            valid_pixels = np.sum(~np.isnan(data_sample))\n",
    "            total_pixels = data_sample.size\n",
    "            print(f\"Valid pixels: {valid_pixels}/{total_pixels} ({valid_pixels/total_pixels*100:.1f}%)\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {os.path.basename(h5_file_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the conversion function with one file\n",
    "if h5_files:\n",
    "    print(\"Testing conversion with the first file...\")\n",
    "    test_output = h5_to_geotiff(h5_files[0], output_dir)\n",
    "    if test_output:\n",
    "        print(f\"\\\\nTest conversion successful!\")\n",
    "        print(f\"Output file: {test_output}\")\n",
    "    else:\n",
    "        print(\"Test conversion failed!\")\n",
    "else:\n",
    "    print(\"No H5 files available for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc439c",
   "metadata": {},
   "source": [
    "## 7. Batch Process Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0377358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch conversion of all H5 files...\n",
      "Found 4 H5 files to convert\n",
      "============================================================\n",
      "\\n[1/4] Processing: ECOv002_L3G_SM_27979_003_20230612T235750_0712_01.h5\n",
      "\\nProcessing: ECOv002_L3G_SM_27979_003_20230612T235750_0712_01.h5\n",
      "--------------------------------------------------\n",
      "Error extracting data: Could not find soil moisture dataset\n",
      "Failed to extract data from H5 file\n",
      "✗ Failed: ECOv002_L3G_SM_27979_003_20230612T235750_0712_01.h5\n",
      "\\n[2/4] Processing: ECOv002_L3G_SM_27857_003_20230605T031507_0712_01.h5\n",
      "\\nProcessing: ECOv002_L3G_SM_27857_003_20230605T031507_0712_01.h5\n",
      "--------------------------------------------------\n",
      "Error extracting data: Could not find soil moisture dataset\n",
      "Failed to extract data from H5 file\n",
      "✗ Failed: ECOv002_L3G_SM_27857_003_20230605T031507_0712_01.h5\n",
      "\\n[3/4] Processing: ECOv002_L3G_SM_27811_005_20230602T040448_0712_01.h5\n",
      "\\nProcessing: ECOv002_L3G_SM_27811_005_20230602T040448_0712_01.h5\n",
      "--------------------------------------------------\n",
      "Error extracting data: Could not find soil moisture dataset\n",
      "Failed to extract data from H5 file\n",
      "✗ Failed: ECOv002_L3G_SM_27811_005_20230602T040448_0712_01.h5\n",
      "\\n[4/4] Processing: ECOv002_L3G_SM_27796_006_20230601T045347_0712_01.h5\n",
      "\\nProcessing: ECOv002_L3G_SM_27796_006_20230601T045347_0712_01.h5\n",
      "--------------------------------------------------\n",
      "Error extracting data: Could not find soil moisture dataset\n",
      "Failed to extract data from H5 file\n",
      "✗ Failed: ECOv002_L3G_SM_27796_006_20230601T045347_0712_01.h5\n",
      "\\n============================================================\n",
      "BATCH CONVERSION SUMMARY\n",
      "============================================================\n",
      "Total files processed: 4\n",
      "Successful conversions: 0\n",
      "Failed conversions: 4\n",
      "Success rate: 0.0%\n",
      "Processing time: 0:00:00.004814\n",
      "\\nFailed conversions:\n",
      "  ✗ ECOv002_L3G_SM_27979_003_20230612T235750_0712_01.h5: Conversion returned None or file not created\n",
      "  ✗ ECOv002_L3G_SM_27857_003_20230605T031507_0712_01.h5: Conversion returned None or file not created\n",
      "  ✗ ECOv002_L3G_SM_27811_005_20230602T040448_0712_01.h5: Conversion returned None or file not created\n",
      "  ✗ ECOv002_L3G_SM_27796_006_20230601T045347_0712_01.h5: Conversion returned None or file not created\n",
      "\\nOutput directory: /Users/ninhhaidang/Library/CloudStorage/GoogleDrive-ninhhailongg@gmail.com/My Drive/Cac_mon_hoc/Do_an_tot_nghiep/25-26_HKI_DATN_21021411_DangNH/Data/ECOSTRESS Gridded Downscaled Soil Moisture Instantaneous L3 Global 70 m V002/output_tiff_converted\n",
      "\\nBatch processing completed!\n"
     ]
    }
   ],
   "source": [
    "def batch_convert_h5_to_geotiff(input_dir, output_dir, file_pattern=\"*.h5\"):\n",
    "    \"\"\"\n",
    "    Batch convert all H5 files in a directory to GeoTIFF format\n",
    "    \n",
    "    Parameters:\n",
    "    input_dir: directory containing H5 files\n",
    "    output_dir: directory to save TIFF files\n",
    "    file_pattern: pattern to match H5 files\n",
    "    \n",
    "    Returns:\n",
    "    dict: conversion results with success/failure counts and file lists\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all H5 files\n",
    "    h5_files = glob.glob(os.path.join(input_dir, file_pattern))\n",
    "    \n",
    "    if not h5_files:\n",
    "        print(f\"No files found matching pattern '{file_pattern}' in {input_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(h5_files)} H5 files to convert\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize results\n",
    "    results = {\n",
    "        'total_files': len(h5_files),\n",
    "        'successful': [],\n",
    "        'failed': [],\n",
    "        'start_time': datetime.now()\n",
    "    }\n",
    "    \n",
    "    # Process each file\n",
    "    for i, h5_file in enumerate(h5_files, 1):\n",
    "        print(f\"\\\\n[{i}/{len(h5_files)}] Processing: {os.path.basename(h5_file)}\")\n",
    "        \n",
    "        try:\n",
    "            output_path = h5_to_geotiff(h5_file, output_dir)\n",
    "            \n",
    "            if output_path and os.path.exists(output_path):\n",
    "                results['successful'].append({\n",
    "                    'input_file': h5_file,\n",
    "                    'output_file': output_path,\n",
    "                    'size_mb': os.path.getsize(output_path) / (1024*1024)\n",
    "                })\n",
    "                print(f\"✓ Success: {os.path.basename(output_path)}\")\n",
    "            else:\n",
    "                results['failed'].append({\n",
    "                    'input_file': h5_file,\n",
    "                    'error': 'Conversion returned None or file not created'\n",
    "                })\n",
    "                print(f\"✗ Failed: {os.path.basename(h5_file)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            results['failed'].append({\n",
    "                'input_file': h5_file,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            print(f\"✗ Error: {e}\")\n",
    "    \n",
    "    # Calculate end time and duration\n",
    "    results['end_time'] = datetime.now()\n",
    "    results['duration'] = results['end_time'] - results['start_time']\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"BATCH CONVERSION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total files processed: {results['total_files']}\")\n",
    "    print(f\"Successful conversions: {len(results['successful'])}\")\n",
    "    print(f\"Failed conversions: {len(results['failed'])}\")\n",
    "    print(f\"Success rate: {len(results['successful'])/results['total_files']*100:.1f}%\")\n",
    "    print(f\"Processing time: {results['duration']}\")\n",
    "    \n",
    "    if results['successful']:\n",
    "        print(f\"\\\\nSuccessful conversions:\")\n",
    "        total_size = 0\n",
    "        for item in results['successful']:\n",
    "            size_mb = item['size_mb']\n",
    "            total_size += size_mb\n",
    "            print(f\"  ✓ {os.path.basename(item['output_file'])} ({size_mb:.1f} MB)\")\n",
    "        print(f\"  Total output size: {total_size:.1f} MB\")\n",
    "    \n",
    "    if results['failed']:\n",
    "        print(f\"\\\\nFailed conversions:\")\n",
    "        for item in results['failed']:\n",
    "            print(f\"  ✗ {os.path.basename(item['input_file'])}: {item['error']}\")\n",
    "    \n",
    "    print(f\"\\\\nOutput directory: {output_dir}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run batch conversion\n",
    "print(\"Starting batch conversion of all H5 files...\")\n",
    "batch_results = batch_convert_h5_to_geotiff(input_dir, output_dir)\n",
    "\n",
    "if batch_results:\n",
    "    print(\"\\\\nBatch processing completed!\")\n",
    "else:\n",
    "    print(\"Batch processing failed to start!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606f27a",
   "metadata": {},
   "source": [
    "## 8. Verify Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479baffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying output TIFF files...\n",
      "No TIFF files found in /Users/ninhhaidang/Library/CloudStorage/GoogleDrive-ninhhailongg@gmail.com/My Drive/Cac_mon_hoc/Do_an_tot_nghiep/25-26_HKI_DATN_21021411_DangNH/Data/ECOSTRESS Gridded Downscaled Soil Moisture Instantaneous L3 Global 70 m V002/output_tiff_converted\n"
     ]
    }
   ],
   "source": [
    "def verify_geotiff_files(output_dir, detailed=True):\n",
    "    \"\"\"\n",
    "    Verify the created GeoTIFF files\n",
    "    \n",
    "    Parameters:\n",
    "    output_dir: directory containing TIFF files\n",
    "    detailed: whether to show detailed information for each file\n",
    "    \n",
    "    Returns:\n",
    "    verification results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all TIFF files in output directory\n",
    "    tiff_files = glob.glob(os.path.join(output_dir, \"*.tif\"))\n",
    "    \n",
    "    if not tiff_files:\n",
    "        print(f\"No TIFF files found in {output_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Verifying {len(tiff_files)} TIFF files...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    verification_results = []\n",
    "    \n",
    "    for i, tiff_file in enumerate(tiff_files, 1):\n",
    "        print(f\"\\\\n[{i}/{len(tiff_files)}] Verifying: {os.path.basename(tiff_file)}\")\n",
    "        \n",
    "        try:\n",
    "            with rasterio.open(tiff_file) as src:\n",
    "                # Basic file information\n",
    "                file_info = {\n",
    "                    'filename': os.path.basename(tiff_file),\n",
    "                    'filepath': tiff_file,\n",
    "                    'size_mb': os.path.getsize(tiff_file) / (1024*1024),\n",
    "                    'shape': src.shape,\n",
    "                    'crs': str(src.crs),\n",
    "                    'bounds': src.bounds,\n",
    "                    'dtype': str(src.dtype),\n",
    "                    'nodata': src.nodata,\n",
    "                    'compression': src.compression.value if src.compression else None,\n",
    "                }\n",
    "                \n",
    "                # Read data for statistics\n",
    "                data = src.read(1)\n",
    "                \n",
    "                # Calculate statistics\n",
    "                valid_mask = ~np.isnan(data)\n",
    "                total_pixels = data.size\n",
    "                valid_pixels = np.sum(valid_mask)\n",
    "                \n",
    "                if valid_pixels > 0:\n",
    "                    valid_data = data[valid_mask]\n",
    "                    stats = {\n",
    "                        'total_pixels': total_pixels,\n",
    "                        'valid_pixels': valid_pixels,\n",
    "                        'valid_percentage': (valid_pixels / total_pixels) * 100,\n",
    "                        'min_value': np.min(valid_data),\n",
    "                        'max_value': np.max(valid_data),\n",
    "                        'mean_value': np.mean(valid_data),\n",
    "                        'std_value': np.std(valid_data)\n",
    "                    }\n",
    "                else:\n",
    "                    stats = {\n",
    "                        'total_pixels': total_pixels,\n",
    "                        'valid_pixels': 0,\n",
    "                        'valid_percentage': 0,\n",
    "                        'min_value': np.nan,\n",
    "                        'max_value': np.nan,\n",
    "                        'mean_value': np.nan,\n",
    "                        'std_value': np.nan\n",
    "                    }\n",
    "                \n",
    "                file_info.update(stats)\n",
    "                \n",
    "                # Check for common issues\n",
    "                issues = []\n",
    "                if valid_pixels == 0:\n",
    "                    issues.append(\"No valid data pixels\")\n",
    "                elif valid_pixels / total_pixels < 0.01:\n",
    "                    issues.append(\"Very low valid pixel percentage (<1%)\")\n",
    "                \n",
    "                if stats['min_value'] < 0:\n",
    "                    issues.append(\"Contains negative values\")\n",
    "                if stats['max_value'] > 1:\n",
    "                    issues.append(\"Contains values > 1 (unusual for soil moisture)\")\n",
    "                \n",
    "                file_info['issues'] = issues\n",
    "                file_info['status'] = 'OK' if not issues else 'Warning'\n",
    "                \n",
    "                verification_results.append(file_info)\n",
    "                \n",
    "                if detailed:\n",
    "                    print(f\"  Shape: {src.shape}\")\n",
    "                    print(f\"  CRS: {src.crs}\")\n",
    "                    print(f\"  Bounds: {src.bounds}\")\n",
    "                    print(f\"  Valid pixels: {valid_pixels:,}/{total_pixels:,} ({stats['valid_percentage']:.1f}%)\")\n",
    "                    if valid_pixels > 0:\n",
    "                        print(f\"  Data range: {stats['min_value']:.6f} to {stats['max_value']:.6f}\")\n",
    "                        print(f\"  Mean: {stats['mean_value']:.6f} ± {stats['std_value']:.6f}\")\n",
    "                    if issues:\n",
    "                        print(f\"  Issues: {', '.join(issues)}\")\n",
    "                    print(f\"  Status: {file_info['status']}\")\n",
    "                else:\n",
    "                    status_symbol = \"✓\" if file_info['status'] == 'OK' else \"⚠\"\n",
    "                    print(f\"  {status_symbol} {file_info['status']} - {valid_pixels:,} valid pixels ({stats['valid_percentage']:.1f}%)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error reading file: {e}\")\n",
    "            verification_results.append({\n",
    "                'filename': os.path.basename(tiff_file),\n",
    "                'filepath': tiff_file,\n",
    "                'status': 'Error',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"VERIFICATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ok_files = [r for r in verification_results if r.get('status') == 'OK']\n",
    "    warning_files = [r for r in verification_results if r.get('status') == 'Warning']\n",
    "    error_files = [r for r in verification_results if r.get('status') == 'Error']\n",
    "    \n",
    "    print(f\"Total files: {len(verification_results)}\")\n",
    "    print(f\"OK: {len(ok_files)}\")\n",
    "    print(f\"Warnings: {len(warning_files)}\")\n",
    "    print(f\"Errors: {len(error_files)}\")\n",
    "    \n",
    "    if ok_files:\n",
    "        total_size = sum([f.get('size_mb', 0) for f in ok_files])\n",
    "        print(f\"\\\\nTotal size of valid files: {total_size:.1f} MB\")\n",
    "        \n",
    "        # Data quality summary\n",
    "        valid_percentages = [f.get('valid_percentage', 0) for f in ok_files if 'valid_percentage' in f]\n",
    "        if valid_percentages:\n",
    "            print(f\"Average valid pixel percentage: {np.mean(valid_percentages):.1f}%\")\n",
    "    \n",
    "    return verification_results\n",
    "\n",
    "# Run verification\n",
    "print(\"Verifying output TIFF files...\")\n",
    "verification_results = verify_geotiff_files(output_dir, detailed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8617743e",
   "metadata": {},
   "source": [
    "## Kết luận và Sử dụng\n",
    "\n",
    "Notebook này cung cấp một giải pháp hoàn chỉnh để chuyển đổi các file ECOSTRESS HDF5 (.h5) sang định dạng GeoTIFF (.tif). \n",
    "\n",
    "### Các tính năng chính:\n",
    "1. **Tự động phát hiện cấu trúc file H5** - Script có thể tự động tìm dữ liệu độ ẩm đất trong file H5\n",
    "2. **Xử lý thông tin địa lý** - Tự động trích xuất và xử lý thông tin tọa độ\n",
    "3. **Chuyển đổi batch** - Có thể xử lý nhiều file cùng lúc\n",
    "4. **Kiểm tra chất lượng** - Tự động verify file output\n",
    "5. **Metadata preservation** - Giữ lại thông tin metadata quan trọng\n",
    "\n",
    "### Cách sử dụng:\n",
    "1. Đặt các file H5 trong cùng thư mục với notebook\n",
    "2. Chạy từng cell theo thứ tự\n",
    "3. Kết quả sẽ được lưu trong thư mục `output_tiff_converted/`\n",
    "\n",
    "### Lưu ý:\n",
    "- Script sẽ tự động xử lý các giá trị invalid và áp dụng scale factor\n",
    "- File output sử dụng nén LZW để tiết kiệm dung lượng\n",
    "- Coordinate system mặc định là WGS84 (EPSG:4326)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
