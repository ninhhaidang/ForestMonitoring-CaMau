\section{Học sâu và mạng nơ-ron tích chập}

Học sâu là một nhánh của học máy dựa trên mạng nơ-ron nhân tạo nhiều lớp, cho phép tự động học các biểu diễn đặc trưng phân cấp từ dữ liệu thô mà không cần thiết kế đặc trưng thủ công \cite{lecun2015}. Phần này trình bày các khái niệm nền tảng về học sâu, làm cơ sở lý thuyết cho kiến trúc mô hình được đề xuất trong Chương~\ref{chap:methodology}.

\subsection{Kiến trúc mạng nơ-ron tích chập}

Mạng nơ-ron tích chập đã trở thành công cụ chủ đạo trong xử lý ảnh viễn thám và phân loại lớp phủ đất \cite{zhu2017, zhang2016}. Kiến trúc CNN được xây dựng từ các thành phần cơ bản sau.

Perceptron là đơn vị tính toán cơ bản của mạng nơ-ron nhân tạo. Mỗi perceptron thực hiện phép biến đổi tuyến tính kết hợp với hàm kích hoạt phi tuyến theo công thức:
\begin{equation}
y = f(\mathbf{w}^T \mathbf{x} + b)
\end{equation}
trong đó $\mathbf{x} \in \mathbb{R}^n$ là vector đầu vào, $\mathbf{w} \in \mathbb{R}^n$ là vector trọng số, $b \in \mathbb{R}$ là độ lệch, $f(\cdot)$ là hàm kích hoạt phi tuyến, và $y$ là đầu ra. Cơ chế này mô phỏng cách nơ-ron sinh học xử lý tín hiệu: nhận nhiều đầu vào, tính tổng có trọng số, và kích hoạt khi vượt ngưỡng. Multi-Layer Perceptron (MLP) mở rộng perceptron thành mạng nhiều lớp, trong đó mỗi lớp nhận đầu ra của lớp trước làm đầu vào. Các lớp ẩn cho phép mạng học các biểu diễn trừu tượng của dữ liệu với mức độ phức tạp tăng dần theo chiều sâu.

Phép tích chập 2D là thành phần cốt lõi của CNN. Với đầu vào $I$ và kernel $K$ kích thước $k_h \times k_w$, phép tích chập được định nghĩa:
\begin{equation}
(I * K)(i, j) = \sum_m \sum_n I(i+m, j+n) \times K(m, n)
\end{equation}
trong đó $(i,j)$ là vị trí trong đầu ra và $(m,n)$ là vị trí trong kernel. Kernel trượt qua đầu vào với bước nhảy xác định; tại mỗi vị trí, phép nhân từng phần tử giữa kernel và vùng tương ứng được thực hiện, sau đó cộng tổng để tạo ra một giá trị đầu ra. Với đầu vào nhiều kênh, kernel có cùng số kênh và phép tích chập được thực hiện đồng thời trên tất cả các kênh. Phép tích chập mang lại ba ưu điểm quan trọng. Thứ nhất, cơ chế chia sẻ tham số cho phép cùng một kernel áp dụng cho toàn bộ đầu vào, giúp giảm đáng kể số lượng tham số cần học. Thứ hai, tính bất biến tịnh tiến giúp mạng có khả năng nhận diện đặc trưng bất kể vị trí xuất hiện trong ảnh. Thứ ba, kết nối cục bộ đảm bảo mỗi nơ-ron chỉ kết nối với vùng lân cận của đầu vào, phù hợp với đặc tính không gian của dữ liệu ảnh.

Hàm kích hoạt đưa tính phi tuyến vào mạng nơ-ron, cho phép học các mối quan hệ phức tạp trong dữ liệu. ReLU (Rectified Linear Unit) là hàm kích hoạt phổ biến nhất với công thức $f(x) = \max(0, x)$. ReLU giữ nguyên giá trị dương và chuyển giá trị âm về 0, mang lại các ưu điểm như tính toán đơn giản, giảm vấn đề triệt tiêu gradient, và tạo ra tính thưa trong kích hoạt. Đối với bài toán phân loại đa lớp, hàm softmax được sử dụng ở lớp đầu ra để chuyển đổi các giá trị logits thành phân phối xác suất theo công thức $\text{softmax}(\mathbf{x})_i = \exp(x_i) / \sum_j \exp(x_j)$, trong đó $\mathbf{x}$ là vector logits từ lớp cuối cùng và lớp có xác suất cao nhất được chọn làm kết quả dự đoán.

Pooling là phép toán giảm chiều không gian của bản đồ đặc trưng (feature map), giúp giảm số lượng tham số và tăng tính bất biến tịnh tiến của mạng \citeen{lecun2015}. Hai loại pooling phổ biến là Max Pooling (chọn giá trị lớn nhất trong mỗi vùng) và Average Pooling (tính trung bình các giá trị). Lớp gộp trung bình toàn cục (GAP) là trường hợp đặc biệt trong đó vùng pooling bao trùm toàn bộ bản đồ đặc trưng, biến đổi mỗi bản đồ đặc trưng thành một giá trị vô hướng \citeen{lin2014}. GAP giúp giảm mạnh số lượng tham số và có tính chất điều chuẩn tự nhiên. Trong nghiên cứu này, GAP được sử dụng để tổng hợp thông tin không gian trước lớp phân loại.

\subsection{Huấn luyện mạng nơ-ron}

Hàm mất mát đo lường sự khác biệt giữa dự đoán của mô hình và nhãn thực tế, đóng vai trò là mục tiêu tối ưu hóa trong quá trình huấn luyện. Đối với bài toán phân loại đa lớp, Cross-Entropy Loss được định nghĩa:
\begin{equation}
L = -\sum_i y_i \log(\hat{y}_i)
\end{equation}
trong đó $y_i$ là nhãn thực tế (mã hóa one-hot) và $\hat{y}_i$ là xác suất dự đoán từ hàm softmax. Hàm này đo lường khoảng cách giữa phân phối dự đoán và phân phối thực tế: giá trị mất mát cao khi mô hình gán xác suất thấp cho lớp đúng, và thấp khi dự đoán chính xác. Cross-Entropy Loss có tính chất lồi (convex) theo tham số của lớp softmax, giúp quá trình tối ưu hóa ổn định và hội tụ nhanh hơn so với các hàm mất mát khác trong bài toán phân loại.

Thuật toán tối ưu hóa đóng vai trò cập nhật trọng số của mạng dựa trên gradient của hàm mất mát. Adam là một trong những thuật toán phổ biến nhất hiện nay. Adam kết hợp ưu điểm của hai phương pháp: momentum sử dụng trung bình động bậc nhất của gradient để tăng tốc hội tụ theo hướng nhất quán, và RMSprop sử dụng trung bình động bậc hai để điều chỉnh tốc độ học theo từng tham số. Sự kết hợp này giúp Adam hoạt động hiệu quả trên nhiều loại bài toán với ít điều chỉnh siêu tham số. AdamW là biến thể cải tiến với cơ chế phân rã trọng số tách biệt khỏi cập nhật gradient, giúp điều chuẩn hiệu quả hơn và tránh tương tác không mong muốn giữa hai cơ chế.

Các kỹ thuật điều chuẩn giúp ngăn chặn hiện tượng quá khớp, khi mô hình học quá chi tiết trên tập huấn luyện nhưng tổng quát hóa kém trên dữ liệu mới. Chuẩn hóa theo lô được đề xuất bởi Ioffe và Szegedy \citeen{ioffe2015} nhằm tăng tốc quá trình huấn luyện bằng cách chuẩn hóa các giá trị kích hoạt trong mỗi lô nhỏ về phân phối chuẩn (trung bình 0, phương sai 1), sau đó áp dụng phép biến đổi tuyến tính với tham số học được. Kỹ thuật này giúp cho phép sử dụng tốc độ học cao hơn và giảm độ nhạy với khởi tạo trọng số. Dropout được đề xuất bởi Srivastava và cộng sự \citeen{srivastava2014} như một kỹ thuật điều chuẩn hiệu quả: trong quá trình huấn luyện, mỗi nơ-ron bị tắt ngẫu nhiên với xác suất xác định, buộc mạng học các biểu diễn bền vững và tránh phụ thuộc quá mức vào một số nơ-ron cụ thể. Dropout2d là biến thể tắt toàn bộ bản đồ đặc trưng thay vì từng nơ-ron riêng lẻ, phù hợp cho CNN do tương quan không gian cao giữa các giá trị trong cùng bản đồ đặc trưng.

\subsection{Ứng dụng CNN trong phân loại ảnh viễn thám}

Trong phân loại ảnh viễn thám, có hai phương pháp tiếp cận chính. Phương pháp phân loại dựa trên điểm ảnh thực hiện phân loại mỗi điểm ảnh độc lập dựa trên vector đặc trưng phổ của nó. Phương pháp này có ưu điểm là đơn giản trong triển khai và tốc độ xử lý nhanh, tuy nhiên không tận dụng được thông tin ngữ cảnh không gian và dễ tạo ra nhiễu muối-tiêu trong kết quả phân loại, đặc biệt ở các vùng ranh giới giữa các lớp. Phương pháp phân loại dựa trên patch trích xuất patch xung quanh mỗi điểm ảnh trung tâm và thực hiện phân loại dựa trên toàn bộ thông tin trong patch. Phương pháp này tận dụng được ngữ cảnh không gian, cho kết quả phân loại mượt mà hơn và đặc biệt phù hợp với kiến trúc CNN vốn được thiết kế để học các đặc trưng không gian cục bộ. Kích thước patch cần được lựa chọn phù hợp với độ phân giải ảnh và đặc tính của đối tượng cần phân loại: patch quá nhỏ không nắm bắt đủ ngữ cảnh, trong khi patch quá lớn có thể đưa vào nhiễu từ các lớp phủ lân cận.

Chuẩn hóa dữ liệu là bước tiền xử lý quan trọng trong học máy nói chung và CNN nói riêng, đảm bảo các đặc trưng có cùng thang đo để quá trình học diễn ra ổn định. Trong ảnh viễn thám đa nguồn, các đặc trưng thường có thang đo rất khác nhau: chỉ số thực vật NDVI nằm trong khoảng $[-1, 1]$, phản xạ bề mặt quang học trong khoảng $[0, 1]$, trong khi tán xạ ngược ra-đa có thể trong khoảng $[-25, 0]$ dB. Nếu không chuẩn hóa, các đặc trưng có giá trị lớn sẽ chi phối quá trình học và gradient có thể không ổn định. Chuẩn hóa Z-score chuyển đổi dữ liệu về phân phối với trung bình bằng 0 và độ lệch chuẩn bằng 1 (chi tiết công thức được trình bày trong Chương~\ref{chap:methodology}). Điểm quan trọng cần lưu ý là các tham số chuẩn hóa (trung bình $\mu$ và độ lệch chuẩn $\sigma$) phải được tính toán dựa trên tập huấn luyện và áp dụng nhất quán cho cả tập kiểm tra, nhằm đảm bảo tính khách quan trong đánh giá mô hình và tránh rò rỉ thông tin từ tập kiểm tra.
