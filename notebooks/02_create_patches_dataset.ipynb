{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ Create Patches Dataset\n",
    "\n",
    "**Objective:** Extract 128√ó128√ó14 patches from 4 TIFF files at ground truth locations\n",
    "\n",
    "**Input:**\n",
    "- 4 TIFF files (S1 2024/2025, S2 2024/2025)\n",
    "- Ground truth CSV with 1,285 points\n",
    "\n",
    "**Output:**\n",
    "- Train patches: ~900 files (.npy)\n",
    "- Val patches: ~190 files\n",
    "- Test patches: ~195 files\n",
    "\n",
    "**Processing:**\n",
    "1. Load 4 TIFF files\n",
    "2. Stack into 14-channel array\n",
    "3. Extract patches at ground truth coordinates\n",
    "4. Handle NaN values\n",
    "5. Normalize bands\n",
    "6. Split train/val/test\n",
    "7. Save as .npy files\n",
    "\n",
    "**Expected time:** 10-15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Python Path and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root and src to Python path\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(\"‚úÖ Python path configured:\")\n",
    "print(f\"   Project root: {project_root}\")\n",
    "print(f\"   Source dir: {src_path}\")\n",
    "\n",
    "# Now import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import from src\n",
    "from src.preprocessing import create_patches_dataset\n",
    "from src.utils import load_ground_truth\n",
    "\n",
    "print(\"\\n‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Data Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('../data/raw')\n",
    "\n",
    "files = {\n",
    "    'S1_2024': base_dir / 'sentinel1' / 'S1_2024_02_04_matched_S2_2024_01_30.tif',\n",
    "    'S1_2025': base_dir / 'sentinel1' / 'S1_2025_02_22_matched_S2_2025_02_28.tif',\n",
    "    'S2_2024': base_dir / 'sentinel2' / 'S2_2024_01_30.tif',\n",
    "    'S2_2025': base_dir / 'sentinel2' / 'S2_2025_02_28.tif',\n",
    "    'GT_CSV': base_dir / 'ground_truth' / 'Training_Points_CSV.csv'\n",
    "}\n",
    "\n",
    "print(\"üìä Checking data availability...\\n\")\n",
    "all_exist = True\n",
    "for name, filepath in files.items():\n",
    "    exists = filepath.exists()\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    size = f\"{filepath.stat().st_size / (1024**2):.0f} MB\" if exists else \"N/A\"\n",
    "    print(f\"{status} {name:10s}: {size:>8s}\")\n",
    "    all_exist = all_exist and exists\n",
    "\n",
    "if all_exist:\n",
    "    print(\"\\n‚úÖ All files available! Ready to create patches.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some files missing! Please check data directory.\")\n",
    "    raise FileNotFoundError(\"Required data files not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Ground Truth Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = load_ground_truth(files['GT_CSV'])\n",
    "\n",
    "print(f\"üìä Ground Truth Summary:\")\n",
    "print(f\"  Total points: {len(gt_df)}\")\n",
    "print(f\"\\n  Class distribution:\")\n",
    "for label in [0, 1]:\n",
    "    count = (gt_df['label'] == label).sum()\n",
    "    pct = 100 * count / len(gt_df)\n",
    "    label_name = \"No deforestation\" if label == 0 else \"Deforestation\"\n",
    "    print(f\"    Class {label} ({label_name}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  Expected split (70/15/15):\")\n",
    "print(f\"    Train: ~{int(len(gt_df) * 0.70)} patches\")\n",
    "print(f\"    Val:   ~{int(len(gt_df) * 0.15)} patches\")\n",
    "print(f\"    Test:  ~{int(len(gt_df) * 0.15)} patches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Patches Dataset\n",
    "\n",
    "**‚ö†Ô∏è WARNING:** This will take 10-15 minutes. The function will:\n",
    "1. Load ~4GB of TIFF files into memory\n",
    "2. Extract 1,285 patches (128√ó128√ó14 each)\n",
    "3. Process NaN values\n",
    "4. Normalize 14 channels\n",
    "5. Save ~1,285 .npy files\n",
    "\n",
    "**Progress bars (tqdm) will show real-time progress.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting dataset creation...\\n\")\n",
    "print(\"‚è±Ô∏è This will take approximately 10-15 minutes.\")\n",
    "print(\"‚òï Time for coffee!\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    counts = create_patches_dataset(\n",
    "        s1_2024_path=files['S1_2024'],\n",
    "        s1_2025_path=files['S1_2025'],\n",
    "        s2_2024_path=files['S2_2024'],\n",
    "        s2_2025_path=files['S2_2025'],\n",
    "        ground_truth_csv=files['GT_CSV'],\n",
    "        output_dir='../data/patches',\n",
    "        patch_size=128,\n",
    "        train_ratio=0.70,\n",
    "        val_ratio=0.15,\n",
    "        test_ratio=0.15,\n",
    "        normalize=True,\n",
    "        handle_nan_method='fill',\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ DATASET CREATION COMPLETED!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n‚è±Ô∏è Time elapsed: {elapsed_time/60:.1f} minutes\")\n",
    "    print(f\"\\nüìä Patches created:\")\n",
    "    print(f\"  Train: {counts['train']} patches\")\n",
    "    print(f\"  Val:   {counts['val']} patches\")\n",
    "    print(f\"  Test:  {counts['test']} patches\")\n",
    "    print(f\"  Total: {sum(counts.values())} patches\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Patches Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_dir = Path('../data/patches')\n",
    "\n",
    "print(\"üìÅ Checking patches directory...\\n\")\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = patches_dir / split\n",
    "    if split_dir.exists():\n",
    "        files = list(split_dir.glob('*.npy'))\n",
    "        total_size = sum(f.stat().st_size for f in files) / (1024**2)\n",
    "        print(f\"‚úÖ {split.upper()}:\")\n",
    "        print(f\"   Files: {len(files)}\")\n",
    "        print(f\"   Size: {total_size:.1f} MB\")\n",
    "        print(f\"   Example: {files[0].name if files else 'N/A'}\\n\")\n",
    "    else:\n",
    "        print(f\"‚ùå {split.upper()}: Directory not found\\n\")\n",
    "\n",
    "# Check summary file\n",
    "summary_file = patches_dir / 'dataset_summary.txt'\n",
    "if summary_file.exists():\n",
    "    print(f\"‚úÖ Summary file: {summary_file.name}\")\n",
    "    print(\"\\nContent:\")\n",
    "    print(summary_file.read_text())\n",
    "else:\n",
    "    print(f\"‚ùå Summary file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Visualize Sample Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Loading sample patches...\\n\")\n",
    "\n",
    "# Load one patch from each class\n",
    "train_dir = patches_dir / 'train'\n",
    "\n",
    "# Find patches for each class\n",
    "class_0_files = sorted(list(train_dir.glob('*_label0.npy')))\n",
    "class_1_files = sorted(list(train_dir.glob('*_label1.npy')))\n",
    "\n",
    "if class_0_files and class_1_files:\n",
    "    # Load patches\n",
    "    patch_0 = np.load(class_0_files[0])\n",
    "    patch_1 = np.load(class_1_files[0])\n",
    "    \n",
    "    print(f\"‚úÖ Loaded patches:\")\n",
    "    print(f\"   Class 0: {class_0_files[0].name}\")\n",
    "    print(f\"   Shape: {patch_0.shape}\")\n",
    "    print(f\"   Dtype: {patch_0.dtype}\")\n",
    "    print(f\"   Range: [{patch_0.min():.3f}, {patch_0.max():.3f}]\")\n",
    "    print()\n",
    "    print(f\"   Class 1: {class_1_files[0].name}\")\n",
    "    print(f\"   Shape: {patch_1.shape}\")\n",
    "    print(f\"   Dtype: {patch_1.dtype}\")\n",
    "    print(f\"   Range: [{patch_1.min():.3f}, {patch_1.max():.3f}]\")\n",
    "    \n",
    "    # Visualize key bands\n",
    "    print(\"\\nüìä Visualizing key bands...\")\n",
    "    \n",
    "    band_indices = [4, 5, 6, 11, 12, 13]  # S2 indices only (NDVI, NBR, NDMI for 2024 and 2025)\n",
    "    band_names = ['NDVI_2024', 'NBR_2024', 'NDMI_2024',\n",
    "              'NDVI_2025', 'NBR_2025', 'NDMI_2025']\n",
    "    \n",
    "    # Plot Class 0\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    fig.suptitle('Class 0 - No Deforestation', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for idx, (band_idx, band_name) in enumerate(zip(band_indices, band_names)):\n",
    "        ax = axes[idx // 3, idx % 4]\n",
    "        im = ax.imshow(patch_0[:, :, band_idx], cmap='RdYlGn')\n",
    "        ax.set_title(band_name, fontsize=10)\n",
    "        ax.axis('off')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Class 1\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    fig.suptitle('Class 1 - Deforestation', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for idx, (band_idx, band_name) in enumerate(zip(band_indices, band_names)):\n",
    "        ax = axes[idx // 3, idx % 4]\n",
    "        im = ax.imshow(patch_1[:, :, band_idx], cmap='RdYlGn')\n",
    "        ax.set_title(band_name, fontsize=10)\n",
    "        ax.axis('off')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Could not find sample patches for both classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check for NaN Values in Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Checking for NaN values in sample patches...\\n\")\n",
    "\n",
    "# Check 10 random patches\n",
    "import random\n",
    "\n",
    "all_files = list(train_dir.glob('*.npy'))\n",
    "sample_files = random.sample(all_files, min(10, len(all_files)))\n",
    "\n",
    "nan_counts = []\n",
    "for f in sample_files:\n",
    "    patch = np.load(f)\n",
    "    nan_count = np.isnan(patch).sum()\n",
    "    nan_counts.append(nan_count)\n",
    "    if nan_count > 0:\n",
    "        print(f\"‚ö†Ô∏è {f.name}: {nan_count} NaN values\")\n",
    "\n",
    "if sum(nan_counts) == 0:\n",
    "    print(\"‚úÖ No NaN values found in sampled patches!\")\n",
    "    print(\"   NaN handling was successful.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {sum(nan_counts)} NaN values across {sum(c > 0 for c in nan_counts)} patches\")\n",
    "    print(\"   You may need to adjust NaN handling method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DATASET CREATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Completed Tasks:\")\n",
    "print(\"  1. Loaded 4 TIFF files (~4GB)\")\n",
    "print(\"  2. Stacked into 14-channel array\")\n",
    "print(\"  3. Extracted patches at ground truth locations\")\n",
    "print(\"  4. Handled NaN values\")\n",
    "print(\"  5. Normalized all bands\")\n",
    "print(\"  6. Split into train/val/test sets\")\n",
    "print(f\"  7. Saved {sum(counts.values())} .npy files\")\n",
    "\n",
    "print(\"\\nüìÅ Output Directory:\")\n",
    "print(f\"  {patches_dir.resolve()}\")\n",
    "\n",
    "print(\"\\nüì¶ Patch Specifications:\")\n",
    "print(f\"  Size: 128 √ó 128 pixels\")\n",
    "print(f\"  Channels: 14 (7 S2 bands √ó 2 time points)\")\n",
    "print(f\"  Data type: float32\")\n",
    "print(f\"  Normalization: Mixed (S2 reflectance: clip, S2 indices: scaled)\")\n",
    "print(f\"  NaN handling: fill with 0\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"  1. ‚úÖ Patches created and verified\")\n",
    "print(\"  2. ‚¨ú Train models (notebook 03_train_models.ipynb)\")\n",
    "print(\"  3. ‚¨ú Evaluate results (notebook 04_evaluate_and_visualize_results.ipynb)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}