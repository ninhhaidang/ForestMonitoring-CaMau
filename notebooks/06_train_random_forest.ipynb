{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 06. Train Random Forest Model\n",
    "\n",
    "This notebook trains a Random Forest classifier for deforestation detection.\n",
    "\n",
    "**Approach:**\n",
    "- Traditional machine learning (no deep learning)\n",
    "- Flattened features: 128×128×14 = 229,376 features per sample\n",
    "- Ensemble of decision trees (100 trees)\n",
    "- Feature importance analysis\n",
    "\n",
    "**Outputs:**\n",
    "- Trained model: `checkpoints/random_forest_best.pkl`\n",
    "- Training metrics\n",
    "- Feature importance analysis\n",
    "- Comparison with CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Source dir: {src_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_models import RandomForestModel, load_patches_for_ml\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "patches_dir = project_root / 'data' / 'patches'\n",
    "checkpoints_dir = project_root / 'checkpoints'\n",
    "logs_dir = project_root / 'logs'\n",
    "figures_dir = project_root / 'figures'\n",
    "\n",
    "checkpoints_dir.mkdir(exist_ok=True)\n",
    "logs_dir.mkdir(exist_ok=True)\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Directories created/verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING PATCHES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Load train set\n",
    "X_train, y_train = load_patches_for_ml(patches_dir, 'train')\n",
    "print()\n",
    "\n",
    "# Load validation set\n",
    "X_val, y_val = load_patches_for_ml(patches_dir, 'val')\n",
    "print()\n",
    "\n",
    "# Load test set\n",
    "X_test, y_test = load_patches_for_ml(patches_dir, 'test')\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train: {len(X_train)} samples\")\n",
    "print(f\"Val: {len(X_val)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")\n",
    "print(f\"Total: {len(X_train) + len(X_val) + len(X_test)} samples\")\n",
    "print()\n",
    "print(f\"Features per sample: {X_train.shape[1]:,} (128×128×14)\")\n",
    "print(f\"Memory: Train={X_train.nbytes/1e6:.1f}MB, Val={X_val.nbytes/1e6:.1f}MB, Test={X_test.nbytes/1e6:.1f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "model_config = {\n",
    "    'n_estimators': 100,        # Number of trees\n",
    "    'max_depth': 20,            # Maximum depth of trees\n",
    "    'min_samples_split': 10,    # Min samples to split a node\n",
    "    'min_samples_leaf': 4,      # Min samples at leaf node\n",
    "    'random_state': 42,         # For reproducibility\n",
    "    'n_jobs': -1                # Use all CPU cores\n",
    "}\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(\"=\"*80)\n",
    "for key, value in model_config.items():\n",
    "    print(f\"  {key:20s}: {value}\")\n",
    "print()\n",
    "print(\"Note: n_jobs=-1 means using all available CPU cores for parallel training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Create model\n",
    "model = RandomForestModel(**model_config)\n",
    "\n",
    "# Train model\n",
    "start_time = datetime.now()\n",
    "metrics = model.train(X_train, y_train, X_val, y_val)\n",
    "end_time = datetime.now()\n",
    "\n",
    "training_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print()\n",
    "print(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING METRICS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"Train Set:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  Accuracy:  {metrics['train_acc']:.4f}\")\n",
    "print(f\"  Precision: {metrics['train_precision']:.4f}\")\n",
    "print(f\"  Recall:    {metrics['train_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {metrics['train_f1']:.4f}\")\n",
    "print(f\"  AUC:       {metrics['train_auc']:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Validation Set:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  Accuracy:  {metrics['val_acc']:.4f}\")\n",
    "print(f\"  Precision: {metrics['val_precision']:.4f}\")\n",
    "print(f\"  Recall:    {metrics['val_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {metrics['val_f1']:.4f}\")\n",
    "print(f\"  AUC:       {metrics['val_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Test Set:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"  AUC:       {test_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print()\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_test_pred, \n",
    "                          target_names=['No Deforestation', 'Deforestation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Deforestation', 'Deforestation'],\n",
    "            yticklabels=['No Deforestation', 'Deforestation'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title('Random Forest - Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'random_forest_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confusion matrix saved to: {figures_dir / 'random_forest_confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "        label=f'Random Forest (AUC = {roc_auc:.4f})')\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('Random Forest - ROC Curve (Test Set)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'random_forest_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC curve saved to: {figures_dir / 'random_forest_roc_curve.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance = model.get_feature_importance()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"Total features: {len(importance):,}\")\n",
    "print(f\"Feature importance range: [{importance.min():.6f}, {importance.max():.6f}]\")\n",
    "print(f\"Mean importance: {importance.mean():.6f}\")\n",
    "print(f\"Std importance: {importance.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape importance to (128, 128, 14)\n",
    "importance_map = importance.reshape(128, 128, 14)\n",
    "\n",
    "# Calculate band-wise importance (average over spatial dimensions)\n",
    "band_importance = importance_map.mean(axis=(0, 1))\n",
    "\n",
    "# Band names\n",
    "band_names = [\n",
    "    'Blue_2024', 'Green_2024', 'Red_2024', 'NIR_2024',\n",
    "    'NDVI_2024', 'NBR_2024', 'NDMI_2024',\n",
    "    'Blue_2025', 'Green_2025', 'Red_2025', 'NIR_2025',\n",
    "    'NDVI_2025', 'NBR_2025', 'NDMI_2025'\n",
    "]\n",
    "\n",
    "print()\n",
    "print(\"Average Importance by Band:\")\n",
    "print(\"-\"*80)\n",
    "for i, (name, imp) in enumerate(zip(band_names, band_importance)):\n",
    "    bar = '█' * int(imp * 1000)\n",
    "    print(f\"  Band {i:2d} ({name:12s}): {imp:.6f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot band importance\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['steelblue']*7 + ['coral']*7  # Different colors for 2024 vs 2025\n",
    "bars = ax.bar(range(14), band_importance, color=colors, edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Band', fontsize=12)\n",
    "ax.set_ylabel('Average Importance', fontsize=12)\n",
    "ax.set_title('Random Forest - Feature Importance by Band', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(14))\n",
    "ax.set_xticklabels(band_names, rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='steelblue', edgecolor='black', label='2024'),\n",
    "    Patch(facecolor='coral', edgecolor='black', label='2025')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'random_forest_band_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Band importance plot saved to: {figures_dir / 'random_forest_band_importance.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spatial importance (average over bands)\n",
    "spatial_importance = importance_map.mean(axis=2)\n",
    "\n",
    "# Plot spatial importance heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "im = ax.imshow(spatial_importance, cmap='hot', interpolation='nearest')\n",
    "ax.set_title('Random Forest - Spatial Feature Importance\\n(Averaged over all 14 bands)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('X (pixels)', fontsize=12)\n",
    "ax.set_ylabel('Y (pixels)', fontsize=12)\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Importance', fontsize=12, rotation=270, labelpad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'random_forest_spatial_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Spatial importance heatmap saved to: {figures_dir / 'random_forest_spatial_importance.png'}\")\n",
    "print()\n",
    "print(\"Note: Brighter areas indicate more important spatial locations for classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model_path = checkpoints_dir / 'random_forest_best.pkl'\n",
    "model.save(model_path)\n",
    "\n",
    "print()\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Model size: {model_path.stat().st_size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 11. Save Training Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training log\n",
    "log_path = logs_dir / 'random_forest_training.txt'\n",
    "\n",
    "with open(log_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"RANDOM FOREST TRAINING LOG\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Training date: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\\n\\n\")\n",
    "    \n",
    "    f.write(\"MODEL CONFIGURATION:\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    for key, value in model_config.items():\n",
    "        f.write(f\"  {key}: {value}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"DATASET:\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"  Train: {len(X_train)} samples\\n\")\n",
    "    f.write(f\"  Val: {len(X_val)} samples\\n\")\n",
    "    f.write(f\"  Test: {len(X_test)} samples\\n\")\n",
    "    f.write(f\"  Features: {X_train.shape[1]:,}\\n\\n\")\n",
    "    \n",
    "    f.write(\"TRAINING METRICS:\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(\"Train set:\\n\")\n",
    "    f.write(f\"  Accuracy:  {metrics['train_acc']:.4f}\\n\")\n",
    "    f.write(f\"  Precision: {metrics['train_precision']:.4f}\\n\")\n",
    "    f.write(f\"  Recall:    {metrics['train_recall']:.4f}\\n\")\n",
    "    f.write(f\"  F1 Score:  {metrics['train_f1']:.4f}\\n\")\n",
    "    f.write(f\"  AUC:       {metrics['train_auc']:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Validation set:\\n\")\n",
    "    f.write(f\"  Accuracy:  {metrics['val_acc']:.4f}\\n\")\n",
    "    f.write(f\"  Precision: {metrics['val_precision']:.4f}\\n\")\n",
    "    f.write(f\"  Recall:    {metrics['val_recall']:.4f}\\n\")\n",
    "    f.write(f\"  F1 Score:  {metrics['val_f1']:.4f}\\n\")\n",
    "    f.write(f\"  AUC:       {metrics['val_auc']:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"TEST METRICS:\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\\n\")\n",
    "    f.write(f\"  Precision: {test_metrics['precision']:.4f}\\n\")\n",
    "    f.write(f\"  Recall:    {test_metrics['recall']:.4f}\\n\")\n",
    "    f.write(f\"  F1 Score:  {test_metrics['f1']:.4f}\\n\")\n",
    "    f.write(f\"  AUC:       {test_metrics['auc']:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"FEATURE IMPORTANCE BY BAND:\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    for i, (name, imp) in enumerate(zip(band_names, band_importance)):\n",
    "        f.write(f\"  Band {i:2d} ({name:12s}): {imp:.6f}\\n\")\n",
    "\n",
    "print(f\"Training log saved to: {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Output Files:\")\n",
    "print(f\"  1. Model: {model_path}\")\n",
    "print(f\"  2. Training log: {log_path}\")\n",
    "print(f\"  3. Confusion matrix: {figures_dir / 'random_forest_confusion_matrix.png'}\")\n",
    "print(f\"  4. ROC curve: {figures_dir / 'random_forest_roc_curve.png'}\")\n",
    "print(f\"  5. Band importance: {figures_dir / 'random_forest_band_importance.png'}\")\n",
    "print(f\"  6. Spatial importance: {figures_dir / 'random_forest_spatial_importance.png'}\")\n",
    "print()\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.2%}\")\n",
    "print(f\"  F1 Score:  {test_metrics['f1']:.2%}\")\n",
    "print(f\"  AUC:       {test_metrics['auc']:.2%}\")\n",
    "print()\n",
    "print(\"Key Insights:\")\n",
    "print(f\"  - Training time: {training_time/60:.2f} minutes\")\n",
    "print(f\"  - Most important band: {band_names[band_importance.argmax()]}\")\n",
    "print(f\"  - Model size: {model_path.stat().st_size / 1e6:.2f} MB\")\n",
    "print()\n",
    "print(\"Next Steps:\")\n",
    "print(\"  - Compare with CNN models in notebook 04\")\n",
    "print(\"  - Analyze feature importance for insights\")\n",
    "print(\"  - Consider hyperparameter tuning if needed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
