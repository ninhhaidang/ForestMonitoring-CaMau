{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Clean Old Training Data\n",
    "\n",
    "**Purpose:** Delete all old training outputs to start fresh\n",
    "\n",
    "**What will be deleted:**\n",
    "1. Patches: data/patches/train, val, test\n",
    "2. Checkpoints: checkpoints/*.pth\n",
    "3. Outputs: outputs/*\n",
    "4. Logs: logs/*\n",
    "5. Figures: figures/*.png and subdirectories\n",
    "\n",
    "**‚ö†Ô∏è WARNING:** This operation cannot be undone!\n",
    "\n",
    "**When to use:**\n",
    "- Before retraining with different channel configurations\n",
    "- After changing data preprocessing\n",
    "- To free up disk space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Working directory: d:\\HaiDang\\25-26_HKI_DATN_21021411_DangNH\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print('Libraries imported successfully!')\n",
    "print(f'Working directory: {Path.cwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Current Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CURRENT STATUS\n",
      "================================================================================\n",
      "\n",
      "1. Patches:\n",
      "   train: Not found\n",
      "   val: Not found\n",
      "   test: Not found\n",
      "\n",
      "2. Checkpoints: 0 files, 0.3 MB\n",
      "\n",
      "3. Outputs: 0 files, 0.0 MB\n",
      "\n",
      "4. Logs: 0 files, 0.0 MB\n",
      "\n",
      "5. Figures: 8 files, 5.8 MB\n",
      "\n",
      "Total size: 6.1 MB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    if path.exists():\n",
    "        for f in path.rglob('*'):\n",
    "            if f.is_file():\n",
    "                total += f.stat().st_size\n",
    "    return total / (1024**2)\n",
    "\n",
    "def count_files(path, pattern='*'):\n",
    "    if path.exists():\n",
    "        return len(list(path.rglob(pattern)))\n",
    "    return 0\n",
    "\n",
    "print('='*80)\n",
    "print('CURRENT STATUS')\n",
    "print('='*80)\n",
    "print()\n",
    "\n",
    "patches_dir = Path('../data/patches')\n",
    "print('1. Patches:')\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = patches_dir / split\n",
    "    if split_dir.exists():\n",
    "        n_files = len(list(split_dir.glob('*.npy')))\n",
    "        size_mb = get_dir_size(split_dir)\n",
    "        print(f'   {split}: {n_files} files, {size_mb:.1f} MB')\n",
    "    else:\n",
    "        print(f'   {split}: Not found')\n",
    "print()\n",
    "\n",
    "checkpoints_dir = Path('../checkpoints')\n",
    "n_checkpoints = count_files(checkpoints_dir, '*.pth')\n",
    "checkpoint_size = get_dir_size(checkpoints_dir)\n",
    "print(f'2. Checkpoints: {n_checkpoints} files, {checkpoint_size:.1f} MB')\n",
    "print()\n",
    "\n",
    "outputs_dir = Path('../outputs')\n",
    "n_outputs = count_files(outputs_dir)\n",
    "output_size = get_dir_size(outputs_dir)\n",
    "print(f'3. Outputs: {n_outputs} files, {output_size:.1f} MB')\n",
    "print()\n",
    "\n",
    "logs_dir = Path('../logs')\n",
    "n_logs = count_files(logs_dir)\n",
    "log_size = get_dir_size(logs_dir)\n",
    "print(f'4. Logs: {n_logs} files, {log_size:.1f} MB')\n",
    "print()\n",
    "\n",
    "figures_dir = Path('../figures')\n",
    "n_figures = count_files(figures_dir, '*.png')\n",
    "figure_size = get_dir_size(figures_dir)\n",
    "print(f'5. Figures: {n_figures} files, {figure_size:.1f} MB')\n",
    "print()\n",
    "\n",
    "total_size = (get_dir_size(patches_dir) + checkpoint_size + output_size + log_size + figure_size)\n",
    "print(f'Total size: {total_size:.1f} MB')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Delete Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting patches...\n",
      "  train: Not found\n",
      "  val: Not found\n",
      "  test: Not found\n",
      "\n",
      "Total patches deleted: 0\n"
     ]
    }
   ],
   "source": [
    "print('Deleting patches...')\n",
    "patches_dir = Path('../data/patches')\n",
    "\n",
    "deleted_count = 0\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = patches_dir / split\n",
    "    if split_dir.exists():\n",
    "        n_files = len(list(split_dir.glob('*.npy')))\n",
    "        shutil.rmtree(split_dir)\n",
    "        print(f'  Deleted {split}: {n_files} files')\n",
    "        deleted_count += n_files\n",
    "    else:\n",
    "        print(f'  {split}: Not found')\n",
    "\n",
    "summary_file = patches_dir / 'dataset_summary.txt'\n",
    "if summary_file.exists():\n",
    "    summary_file.unlink()\n",
    "    print('  Deleted summary file')\n",
    "\n",
    "print(f'\\nTotal patches deleted: {deleted_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Delete Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting checkpoints...\n",
      "  No checkpoints found\n"
     ]
    }
   ],
   "source": [
    "print('Deleting checkpoints...')\n",
    "checkpoints_dir = Path('../checkpoints')\n",
    "\n",
    "deleted_count = 0\n",
    "for f in checkpoints_dir.glob('*.pth'):\n",
    "    size_mb = f.stat().st_size / (1024**2)\n",
    "    f.unlink()\n",
    "    print(f'  Deleted {f.name} ({size_mb:.1f} MB)')\n",
    "    deleted_count += 1\n",
    "\n",
    "if deleted_count == 0:\n",
    "    print('  No checkpoints found')\n",
    "else:\n",
    "    print(f'\\nTotal checkpoints deleted: {deleted_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Delete Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting outputs...\n",
      "  Deleted outputs: 0 files\n"
     ]
    }
   ],
   "source": [
    "print('Deleting outputs...')\n",
    "outputs_dir = Path('../outputs')\n",
    "\n",
    "if outputs_dir.exists():\n",
    "    n_files = count_files(outputs_dir)\n",
    "    shutil.rmtree(outputs_dir)\n",
    "    outputs_dir.mkdir()\n",
    "    print(f'  Deleted outputs: {n_files} files')\n",
    "else:\n",
    "    print('  Outputs directory not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Delete Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting logs...\n",
      "  Deleted logs: 0 files\n"
     ]
    }
   ],
   "source": [
    "print('Deleting logs...')\n",
    "logs_dir = Path('../logs')\n",
    "\n",
    "if logs_dir.exists():\n",
    "    n_files = count_files(logs_dir)\n",
    "    shutil.rmtree(logs_dir)\n",
    "    logs_dir.mkdir()\n",
    "    print(f'  Deleted logs: {n_files} files')\n",
    "else:\n",
    "    print('  Logs directory not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Delete Training Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting training figures...\n",
      "  No training figures found\n",
      "\n",
      "Note: Exploration figures (from notebooks 01, 02) are kept.\n"
     ]
    }
   ],
   "source": [
    "print('Deleting training figures...')\n",
    "figures_dir = Path('../figures')\n",
    "\n",
    "deleted_count = 0\n",
    "\n",
    "subdirs = ['training_curves', 'confusion_matrices', 'sample_predictions']\n",
    "for subdir in subdirs:\n",
    "    subdir_path = figures_dir / subdir\n",
    "    if subdir_path.exists():\n",
    "        n_files = len(list(subdir_path.glob('*.png')))\n",
    "        shutil.rmtree(subdir_path)\n",
    "        print(f'  Deleted {subdir}: {n_files} files')\n",
    "        deleted_count += n_files\n",
    "\n",
    "files_to_delete = [\n",
    "    'roc_curves_all_models.png',\n",
    "    'model_agreement_analysis.png',\n",
    "    'spatial_distribution_grid.png',\n",
    "    'probability_distribution.png',\n",
    "    'full_probability_map.png',\n",
    "    'full_binary_map.png',\n",
    "    'full_map_visualization.png',\n",
    "    'comparison_prob_vs_binary.png',\n",
    "    'regional_analysis_zoomed.png'\n",
    "]\n",
    "\n",
    "for filename in files_to_delete:\n",
    "    filepath = figures_dir / filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "        print(f'  Deleted {filename}')\n",
    "        deleted_count += 1\n",
    "\n",
    "if deleted_count == 0:\n",
    "    print('  No training figures found')\n",
    "else:\n",
    "    print(f'\\nTotal figures deleted: {deleted_count}')\n",
    "\n",
    "print('\\nNote: Exploration figures (from notebooks 01, 02) are kept.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLEANUP COMPLETED\n",
      "================================================================================\n",
      "\n",
      "Remaining data:\n",
      "  Patches/train: Deleted\n",
      "  Patches/val: Deleted\n",
      "  Patches/test: Deleted\n",
      "  Checkpoints: 0 files\n",
      "  Outputs: 0 files\n",
      "  Logs: 0 files\n",
      "  Figures: 8 files (exploration only)\n",
      "\n",
      "================================================================================\n",
      "Ready to retrain!\n",
      "================================================================================\n",
      "\n",
      "Next steps:\n",
      "  1. Run notebook 02: Create patches\n",
      "  2. Run notebook 03: Train models\n",
      "  3. Run notebook 04: Evaluate results\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('='*80)\n",
    "print('CLEANUP COMPLETED')\n",
    "print('='*80)\n",
    "print()\n",
    "print('Remaining data:')\n",
    "\n",
    "patches_dir = Path('../data/patches')\n",
    "checkpoints_dir = Path('../checkpoints')\n",
    "outputs_dir = Path('../outputs')\n",
    "logs_dir = Path('../logs')\n",
    "figures_dir = Path('../figures')\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = patches_dir / split\n",
    "    status = 'Exists' if split_dir.exists() else 'Deleted'\n",
    "    print(f'  Patches/{split}: {status}')\n",
    "\n",
    "n_checkpoints = count_files(checkpoints_dir, '*.pth')\n",
    "print(f'  Checkpoints: {n_checkpoints} files')\n",
    "\n",
    "n_outputs = count_files(outputs_dir)\n",
    "print(f'  Outputs: {n_outputs} files')\n",
    "\n",
    "n_logs = count_files(logs_dir)\n",
    "print(f'  Logs: {n_logs} files')\n",
    "\n",
    "n_figures = count_files(figures_dir, '*.png')\n",
    "print(f'  Figures: {n_figures} files (exploration only)')\n",
    "\n",
    "print()\n",
    "print('='*80)\n",
    "print('Ready to retrain!')\n",
    "print('='*80)\n",
    "print()\n",
    "print('Next steps:')\n",
    "print('  1. Run notebook 02: Create patches')\n",
    "print('  2. Run notebook 03: Train models')\n",
    "print('  3. Run notebook 04: Evaluate results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
