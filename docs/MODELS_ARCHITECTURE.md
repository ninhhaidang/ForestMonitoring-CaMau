# ğŸ§  Giáº£i ThÃ­ch Chi Tiáº¿t 3 Kiáº¿n TrÃºc Deep Learning Models

## ğŸ“‹ Má»¥c Lá»¥c
- [Tá»•ng Quan](#tá»•ng-quan)
- [1. Spatial Context CNN](#1-spatial-context-cnn)
- [2. Multi-Scale CNN](#2-multi-scale-cnn)
- [3. Shallow U-Net](#3-shallow-u-net)
- [4. Multi-Scale CNN (NDVI-Weighted)](#4-multi-scale-cnn-ndvi-weighted)
- [So SÃ¡nh CÃ¡c Models](#so-sÃ¡nh-cÃ¡c-models)
- [Khi NÃ o DÃ¹ng Model NÃ o](#khi-nÃ o-dÃ¹ng-model-nÃ o)

---

## ğŸ¯ Tá»•ng Quan

Cáº£ 3 models Ä‘á»u lÃ  **shallow CNN** (CNN nÃ´ng) Ä‘Æ°á»£c thiáº¿t káº¿ cho:
- âœ… Dá»¯ liá»‡u háº¡n cháº¿ (~900 training samples)
- âœ… Input: 14 channels (Sentinel-2: 2024 + 2025)
- âœ… Output: Probability map (128Ã—128) cho má»—i pixel
- âœ… Task: Binary classification (Deforestation vs No Deforestation)

**Táº¡i sao dÃ¹ng shallow CNN thay vÃ¬ deep CNN (ResNet, VGG)?**
- Deep CNN cáº§n hÃ ng triá»‡u training samples
- Vá»›i 900 samples, deep CNN sáº½ **overfit náº·ng**
- Shallow CNN há»c Ä‘Æ°á»£c basic spatial patterns lÃ  Ä‘á»§

---

## 1. Spatial Context CNN

### ğŸ—ï¸ Kiáº¿n TrÃºc

```
Input (14, 128, 128)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv 3Ã—3 (32)   â”‚  â† Layer 1: Extract basic features
â”‚  BatchNorm       â”‚
â”‚  ReLU            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv 3Ã—3 (32)   â”‚  â† Layer 2: Spatial smoothing
â”‚  BatchNorm       â”‚
â”‚  ReLU            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv 1Ã—1 (1)    â”‚  â† Layer 3: Output projection
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
Output (1, 128, 128)  â† Logits (before sigmoid)
```

### ğŸ” CÃ¡ch Hoáº¡t Äá»™ng (Chi Tiáº¿t)

#### **BÆ°á»›c 1: Input Processing**
```python
# Input: Batch of patches
x = torch.randn(64, 14, 128, 128)  # (Batch, Channels, Height, Width)

# 14 channels breakdown:
# Channels 0-6:  S2_2024 [Blue, Green, Red, NIR, NDVI, NBR, NDMI]
# Channels 7-13: S2_2025 [Blue, Green, Red, NIR, NDVI, NBR, NDMI]
```

#### **BÆ°á»›c 2: First Convolution Layer**
```python
# Conv2d(14 â†’ 32, kernel=3Ã—3, padding=1)
# Má»—i filter há»c 1 pattern tá»« 14 input channels

Filter 1: CÃ³ thá»ƒ há»c "NDVI giáº£m máº¡nh" pattern
  - Weight cao cho NDVI_2024 (positive)
  - Weight cao cho NDVI_2025 (negative)
  - â†’ KÃ­ch hoáº¡t máº¡nh khi NDVI giáº£m

Filter 2: Há»c "NIR thay Ä‘á»•i" pattern
Filter 3: Há»c "edge detection" pattern
...
Filter 32: 32 patterns khÃ¡c nhau
```

**Receptive Field sau layer 1:**
- Má»—i pixel output nhÃ¬n tháº¥y vÃ¹ng **3Ã—3 pixels** tá»« input
- TÆ°Æ¡ng Ä‘Æ°Æ¡ng **30m Ã— 30m** trÃªn thá»±c Ä‘á»‹a (Sentinel-2: 10m/pixel)

#### **BÆ°á»›c 3: Second Convolution Layer**
```python
# Conv2d(32 â†’ 32, kernel=3Ã—3, padding=1)
# Combines patterns from layer 1

# VÃ­ dá»¥:
# Layer 1 output: [NDVI_drop, NIR_change, edge, texture, ...]
# Layer 2 learns: "NDVI_drop AND NIR_change = likely deforestation"
```

**Receptive Field sau layer 2:**
- Má»—i pixel nhÃ¬n tháº¥y **5Ã—5 pixels** tá»« input gá»‘c
- TÆ°Æ¡ng Ä‘Æ°Æ¡ng **50m Ã— 50m**

**BatchNorm + ReLU:**
```python
# BatchNorm: Normalize activations (giÃºp training stable)
# ReLU: max(0, x) - Loáº¡i bá» negative values, thÃªm non-linearity
```

#### **BÆ°á»›c 4: Output Layer**
```python
# Conv 1Ã—1 (32 â†’ 1): Weighted sum of 32 features â†’ single output
# KhÃ´ng cÃ³ activation (sigmoid sáº½ apply sau khi tÃ­nh loss)

output_logits = conv3(x)  # (64, 1, 128, 128)
probabilities = torch.sigmoid(output_logits)  # (64, 1, 128, 128)
# Each pixel: probability of deforestation [0, 1]
```

### ğŸ“Š ThÃ´ng Sá»‘

| Metric | Value |
|--------|-------|
| **Tá»•ng tham sá»‘** | ~13,500 |
| **Receptive field** | 5Ã—5 pixels (50m Ã— 50m) |
| **Layers** | 3 conv layers |
| **Æ¯u Ä‘iá»ƒm** | Nhanh nháº¥t, Ã­t tham sá»‘ nháº¥t |
| **NhÆ°á»£c Ä‘iá»ƒm** | Receptive field nhá», chá»‰ nhÃ¬n tháº¥y context gáº§n |

### ğŸ’¡ Khi NÃ o DÃ¹ng

- âœ… Cáº§n inference nhanh
- âœ… TÃ i nguyÃªn háº¡n cháº¿ (embedded devices)
- âœ… Baseline Ä‘Æ¡n giáº£n
- âŒ KhÃ´ng phÃ¹ há»£p khi cáº§n context rá»™ng

---

## 2. Multi-Scale CNN

### ğŸ—ï¸ Kiáº¿n TrÃºc

```
Input (14, 128, 128)
       â†“
    â•”â•â•â•â•¦â•â•â•â•—
    â•‘   â•‘   â•‘
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Branch A  Branch B â”‚  â† Layer 1: Multi-scale extraction
â”‚ Conv 3Ã—3  Conv 5Ã—5 â”‚     3Ã—3: Fine details
â”‚   (32)     (32)    â”‚     5Ã—5: Coarse patterns
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
     â””â”€â”€â”€â”¬â”€â”€â”€â”˜
         â†“
   Concat (64)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv 3Ã—3 (64)   â”‚  â† Layer 2: Fuse multi-scale info
â”‚  Conv 5Ã—5 (64)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
   Concat (128)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv 3Ã—3 (64)   â”‚  â† Layer 3: Refine features
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv 3Ã—3 (32)   â”‚  â† Layer 4: Reduce dimensions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv 1Ã—1 (1)    â”‚  â† Output
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
Output (1, 128, 128)
```

### ğŸ” CÃ¡ch Hoáº¡t Äá»™ng (Chi Tiáº¿t)

#### **Äáº·c Äiá»ƒm ChÃ­nh: Multi-Scale Branches**

```python
# Layer 1: Parallel branches
branch_3x3 = Conv2d(14, 32, kernel=3)  # Fine-grained
branch_5x5 = Conv2d(14, 32, kernel=5)  # Coarse

# Example:
# Branch 3Ã—3 learns: Small clearings, edges, local changes
# Branch 5Ã—5 learns: Large patches, spatial patterns, context
```

**Táº¡i sao cáº§n multi-scale?**
```
Deforestation patterns cÃ³ nhiá»u scales khÃ¡c nhau:

Scale nhá» (3Ã—3):        Scale lá»›n (5Ã—5):
â”Œâ”€â”¬â”€â”¬â”€â”                â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”
â”‚ â”‚â–“â”‚ â”‚   â† CÃ¢y        â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
â”œâ”€â”¼â”€â”¼â”€â”¤    Ä‘Æ¡n láº»      â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”‚â–“â”‚â–“â”‚â–“â”‚    cháº·t phÃ¡     â”‚ â”‚â–“â”‚â–“â”‚â–“â”‚ â”‚  â† Khu vá»±c
â”œâ”€â”¼â”€â”¼â”€â”¤                â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤    rá»™ng lá»›n
â”‚ â”‚ â”‚ â”‚                â”‚â–“â”‚â–“â”‚â–“â”‚â–“â”‚â–“â”‚    bá»‹ cháº·t
â””â”€â”´â”€â”´â”€â”˜                â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜
```

#### **BÆ°á»›c 2: Feature Fusion**

```python
# Concatenate multi-scale features
x = torch.cat([branch_3x3, branch_5x5], dim=1)  # (64 channels)

# Now cÃ³ cáº£ fine details VÃ€ coarse context!
# Model cÃ³ thá»ƒ há»c:
# - "Náº¿u 3Ã—3 detect edge VÃ€ 5Ã—5 detect large clearing â†’ Deforestation!"
```

#### **Receptive Fields**

```
Layer 1:
- Branch 3Ã—3: RF = 3Ã—3 (30m Ã— 30m)
- Branch 5Ã—5: RF = 5Ã—5 (50m Ã— 50m)

Layer 2:
- RF = 7Ã—7 (3Ã—3 path) hoáº·c 9Ã—9 (5Ã—5 path)
- TÆ°Æ¡ng Ä‘Æ°Æ¡ng 70m - 90m

Final RF: ~9Ã—9 pixels (90m Ã— 90m)
â†’ Gáº¥p Ä‘Ã´i Spatial Context CNN!
```

### ğŸ“Š ThÃ´ng Sá»‘

| Metric | Value |
|--------|-------|
| **Tá»•ng tham sá»‘** | ~90,000 |
| **Receptive field** | 9Ã—9 pixels (90m Ã— 90m) |
| **Layers** | 5 conv layers (2 branches) |
| **Æ¯u Ä‘iá»ƒm** | CÃ¢n báº±ng tá»‘t, multi-scale features |
| **NhÆ°á»£c Ä‘iá»ƒm** | Cháº­m hÆ¡n Spatial CNN ~2Ã— |

### ğŸ’¡ Khi NÃ o DÃ¹ng

- âœ… **Production use** (khuyáº¿n nghá»‹)
- âœ… Cáº§n detect cáº£ small vÃ  large deforestation
- âœ… CÃ¢n báº±ng giá»¯a accuracy vÃ  speed
- âŒ KhÃ´ng phÃ¹ há»£p khi cáº§n real-time

---

## 3. Shallow U-Net

### ğŸ—ï¸ Kiáº¿n TrÃºc

```
Input (14, 128Ã—128)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder Block 1  â”‚  32 channels, 128Ã—128
â”‚  Conv + Conv     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Skip Connection 1
         â†“
    MaxPool 2Ã—2
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder Block 2  â”‚  64 channels, 64Ã—64
â”‚  Conv + Conv     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Skip Connection 2
         â†“
    MaxPool 2Ã—2
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder Block 3  â”‚  128 channels, 32Ã—32
â”‚  Conv + Conv     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Skip Connection 3
         â†“
    MaxPool 2Ã—2
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Bottleneck     â”‚  256 channels, 16Ã—16
â”‚  Conv + Conv     â”‚  â† Deepest point
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Upsample 2Ã—2
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Decoder Block 3  â”‚  128 channels, 32Ã—32
â”‚ Concat Skip 3    â”‚  â† Fuse encoder features
â”‚  Conv + Conv     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Upsample 2Ã—2
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Decoder Block 2  â”‚  64 channels, 64Ã—64
â”‚ Concat Skip 2    â”‚
â”‚  Conv + Conv     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Upsample 2Ã—2
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Decoder Block 1  â”‚  32 channels, 128Ã—128
â”‚ Concat Skip 1    â”‚
â”‚  Conv + Conv     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv 1Ã—1 (1)    â”‚  Output
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
Output (1, 128Ã—128)
```

### ğŸ” CÃ¡ch Hoáº¡t Äá»™ng (Chi Tiáº¿t)

#### **KhÃ¡i Niá»‡m U-Net**

U-Net giá»‘ng nhÆ° "zoom out â†’ zoom in":
1. **Encoder** (downsampling): Thu nhá» áº£nh, tÄƒng features â†’ NhÃ¬n context rá»™ng
2. **Bottleneck**: Representation á»Ÿ level cao nháº¥t
3. **Decoder** (upsampling): PhÃ³ng to láº¡i, giáº£m features â†’ Recover spatial details
4. **Skip Connections**: GhÃ©p ná»‘i encoder-decoder â†’ Giá»¯ láº¡i chi tiáº¿t

#### **BÆ°á»›c 1: Encoder (Contracting Path)**

```python
# Encoder Block 1 (128Ã—128)
x1 = conv_block(input, out_channels=32)
# Learn: Low-level features (edges, textures)

# Downsample
x_pool = MaxPool2d(2)(x1)  # â†’ 64Ã—64

# Encoder Block 2 (64Ã—64)
x2 = conv_block(x_pool, out_channels=64)
# Learn: Mid-level features (small objects, patterns)

# Downsample
x_pool = MaxPool2d(2)(x2)  # â†’ 32Ã—32

# Encoder Block 3 (32Ã—32)
x3 = conv_block(x_pool, out_channels=128)
# Learn: High-level features (large structures)

# Downsample
x_pool = MaxPool2d(2)(x3)  # â†’ 16Ã—16
```

**Receptive Field tÄƒng dáº§n:**
- Block 1 (128Ã—128): RF ~ 5Ã—5 pixels (50m)
- Block 2 (64Ã—64): RF ~ 13Ã—13 pixels (130m)
- Block 3 (32Ã—32): RF ~ 29Ã—29 pixels (290m)
- Bottleneck (16Ã—16): RF ~ 61Ã—61 pixels (610m) â† NhÃ¬n ráº¥t rá»™ng!

#### **BÆ°á»›c 2: Bottleneck**

```python
# Smallest spatial resolution, highest channels
bottleneck = conv_block(x_pool, out_channels=256)  # 16Ã—16Ã—256

# Táº¡i Ä‘Ã¢y model cÃ³ "global understanding" cá»§a patch
# Má»—i pixel trong bottleneck nhÃ¬n tháº¥y ~600m Ã— 600m!
```

#### **BÆ°á»›c 3: Decoder (Expanding Path)**

```python
# Upsample + Skip Connection 3
up3 = Upsample(bottleneck)  # 16Ã—16 â†’ 32Ã—32
concat3 = torch.cat([up3, x3], dim=1)  # Fuse vá»›i encoder features
dec3 = conv_block(concat3, out_channels=128)

# Skip connection lÃ  QUAN TRá»ŒNG:
# - x3 chá»©a spatial details tá»« encoder
# - up3 chá»©a semantic info tá»« bottleneck
# â†’ Concat = Chi tiáº¿t + Ngá»¯ nghÄ©a!

# TÆ°Æ¡ng tá»± cho decoder 2, 1
```

**Táº¡i sao cáº§n skip connections?**

```
KhÃ´ng cÃ³ skip:              CÃ³ skip:
Encoder â†’ Bottleneck      Encoder â”€â”€â”
   â†“                         â†“       â”‚
Decoder (máº¥t details)     Decoder â†â”€â”˜ (keep details)

Output: Blurry            Output: Sharp
```

#### **BÆ°á»›c 4: Output**

```python
# Final conv 1Ã—1
output = Conv2d(32, 1, kernel=1)(dec1)  # 128Ã—128Ã—1

# Output combines:
# - Low-level spatial details (tá»« skip connections)
# - High-level semantic understanding (tá»« bottleneck)
# â†’ Best of both worlds!
```

### ğŸ“Š ThÃ´ng Sá»‘

| Metric | Value |
|--------|-------|
| **Tá»•ng tham sá»‘** | ~476,000 |
| **Receptive field** | 61Ã—61 pixels (610m Ã— 610m) |
| **Layers** | 8-10 conv layers + skip connections |
| **Æ¯u Ä‘iá»ƒm** | RF ráº¥t lá»›n, smoothest output, best accuracy |
| **NhÆ°á»£c Ä‘iá»ƒm** | Cháº­m nháº¥t, nhiá»u tham sá»‘ nháº¥t |

### ğŸ’¡ Khi NÃ o DÃ¹ng

- âœ… Cáº§n **best quality** predictions
- âœ… Cáº§n smooth, connected deforestation maps
- âœ… CÃ³ GPU máº¡nh, khÃ´ng quan tÃ¢m speed
- âŒ KhÃ´ng dÃ¹ng cho embedded/mobile

---

## 4. Multi-Scale CNN (NDVI-Weighted)

### ğŸ—ï¸ Kiáº¿n TrÃºc (Má»›i!)

```
Input (14, 128, 128)
       â†“
  â•”â•â•â•â•â•©â•â•â•â•â•—
  â•‘         â•‘
  â•‘    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â•‘    â”‚  Channel    â”‚  â† Learn importance weights
  â•‘    â”‚  Attention  â”‚     for 14 channels
  â•‘    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â•‘         â†“
  â•‘    Weighted Input
  â•‘         â†“
  â•‘    [Original Multi-Scale CNN Architecture]
  â•‘         â†“
  â•‘    Main Features (128)
  â•‘         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚          â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”‚
      â”‚  NDVI   â”‚     â”‚  â† Explicit NDVI change branch
      â”‚  Change â”‚     â”‚
      â”‚ Branch  â”‚     â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚
           â”‚          â”‚
           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                â†“
          Concat (144)
                â†“
           [Fusion Layers]
                â†“
        Output (1, 128, 128)
```

### ğŸ” CÃ¡ch Hoáº¡t Äá»™ng

#### **Component 1: Channel Attention**

```python
# Squeeze: Global average pooling
gap = AdaptiveAvgPool2d(1)(x)  # (B, 14, 1, 1)

# Excitation: Learn channel weights
weights = FC_layers(gap)  # (B, 14, 1, 1)
weights = sigmoid(weights)  # [0, 1]

# Example learned weights:
# Channel 4 (NDVI_2024): 0.85  â† High!
# Channel 11 (NDVI_2025): 0.90  â† High!
# Channel 0 (Blue_2024): 0.35  â† Low
# ...

# Reweight input
x_weighted = x * weights

# Effect: NDVI channels Ä‘Æ°á»£c "nháº¥n máº¡nh" hÆ¡n!
```

#### **Component 2: NDVI Difference Branch**

```python
# Extract NDVI
ndvi_2024 = input[:, 4, :, :]   # (B, 1, 128, 128)
ndvi_2025 = input[:, 11, :, :]  # (B, 1, 128, 128)

# Compute change
ndvi_change = ndvi_2025 - ndvi_2024  # (B, 1, 128, 128)

# Process with small CNN
ndvi_features = conv_layers(ndvi_change)  # (B, 16, 128, 128)

# Fuse vá»›i main features
final = concat([main_features, ndvi_features])  # (B, 144, 128, 128)
```

**Lá»£i Ã­ch:**
1. **Channel Attention**: Model tá»± há»c NDVI quan trá»ng
2. **NDVI Branch**: Force model pháº£i xem NDVI change
3. **Fusion**: Combine spatial patterns + temporal change

### ğŸ“Š ThÃ´ng Sá»‘

| Metric | Value |
|--------|-------|
| **Tá»•ng tham sá»‘** | ~100,000 (+10K so vá»›i MultiScale) |
| **Receptive field** | 9Ã—9 pixels (90m Ã— 90m) |
| **Æ¯u Ä‘iá»ƒm** | Emphasize NDVI, better align vá»›i ground truth |
| **NhÆ°á»£c Ä‘iá»ƒm** | Phá»©c táº¡p hÆ¡n, cáº§n train riÃªng |

---

## ğŸ“Š So SÃ¡nh CÃ¡c Models

### Báº£ng So SÃ¡nh Tá»•ng Quan

| Feature | Spatial CNN | Multi-Scale CNN | Shallow U-Net | MultiScale NDVI-Weighted |
|---------|-------------|-----------------|---------------|-------------------------|
| **Parameters** | 13K | 90K | 476K | 100K |
| **Receptive Field** | 50m | 90m | 610m | 90m |
| **Layers** | 3 | 5 | 8-10 | 6 |
| **Speed** | âš¡âš¡âš¡ Fastest | âš¡âš¡ Fast | âš¡ Slow | âš¡âš¡ Fast |
| **Accuracy** | â­â­ Good | â­â­â­ Better | â­â­â­â­ Best | â­â­â­? TBD |
| **Smoothness** | Low | Medium | High | Medium-High? |
| **GPU Memory** | 50MB | 150MB | 400MB | 160MB |

### Receptive Field Visualization

```
Spatial CNN (50m Ã— 50m):
â”Œâ”€â”€â”€â”€â”€â”
â”‚ 5Ã—5 â”‚  â† Local context only
â””â”€â”€â”€â”€â”€â”˜

Multi-Scale CNN (90m Ã— 90m):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   9Ã—9   â”‚  â† Medium context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Shallow U-Net (610m Ã— 610m):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           â”‚
â”‚         61Ã—61             â”‚  â† Very large context!
â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Trade-offs

```
                  Simple â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Complex
                  Fast   â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Slow

Spatial CNN â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MultiScale CNN â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

U-Net â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€

         Low Accuracy â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ High Accuracy
         Less Smooth  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ More Smooth
```

---

## ğŸ¯ Khi NÃ o DÃ¹ng Model NÃ o?

### Use Cases

#### **Spatial Context CNN**
```
âœ… Khi nÃ o dÃ¹ng:
- Cáº§n inference real-time
- Deploy trÃªn edge devices (Raspberry Pi, drones)
- Baseline nhanh Ä‘á»ƒ test
- Dá»¯ liá»‡u cá»±c ká»³ háº¡n cháº¿

âŒ KhÃ´ng nÃªn dÃ¹ng khi:
- Cáº§n highest accuracy
- CÃ³ nhiá»u small, scattered deforestation
- CÃ³ GPU máº¡nh
```

#### **Multi-Scale CNN** â­ **KHUYáº¾N NGHá»Š**
```
âœ… Khi nÃ o dÃ¹ng:
- PRODUCTION USE
- Cáº§n balance tá»‘t giá»¯a accuracy & speed
- Detect cáº£ small láº«n large clearings
- GPU trung bÃ¬nh (GTX 1060+)

âŒ KhÃ´ng nÃªn dÃ¹ng khi:
- Cáº§n absolutely best quality
- Speed khÃ´ng quan trá»ng
```

#### **Shallow U-Net**
```
âœ… Khi nÃ o dÃ¹ng:
- Cáº§n BEST QUALITY maps
- Smooth, connected predictions
- Research/analysis purposes
- CÃ³ GPU máº¡nh (RTX 3060+)

âŒ KhÃ´ng nÃªn dÃ¹ng khi:
- Cáº§n real-time inference
- RAM/GPU memory háº¡n cháº¿
- Deploy lÃªn mobile
```

#### **Multi-Scale NDVI-Weighted**
```
âœ… Khi nÃ o dÃ¹ng:
- NDVI change lÃ  strong indicator
- Muá»‘n model align vá»›i physical process
- CÃ³ thá»i gian train thÃªm model
- Cáº§n interpretability

âŒ KhÃ´ng nÃªn dÃ¹ng khi:
- ChÆ°a validate NDVI change effectiveness
- Cáº§n Ä‘Æ¡n giáº£n
```

---

## ğŸ§® VÃ­ Dá»¥ TÃ­nh ToÃ¡n

### Memory Usage (Batch size = 64)

```python
# Spatial CNN
Input: 64 Ã— 14 Ã— 128 Ã— 128 Ã— 4 bytes = 58.7 MB
Features: ~100 MB
Total: ~200 MB

# Multi-Scale CNN
Input: 64 Ã— 14 Ã— 128 Ã— 128 Ã— 4 bytes = 58.7 MB
Features: ~300 MB
Total: ~400 MB

# Shallow U-Net
Input: 64 Ã— 14 Ã— 128 Ã— 128 Ã— 4 bytes = 58.7 MB
Features: ~800 MB (do skip connections)
Total: ~1 GB
```

### Inference Speed (1 patch on RTX A4000)

```
Spatial CNN:      0.5 ms
Multi-Scale CNN:  1.2 ms
Shallow U-Net:    3.5 ms

Full image (10917 Ã— 12547, ~33K patches):
Spatial CNN:      16 seconds
Multi-Scale CNN:  40 seconds
Shallow U-Net:    115 seconds (2 minutes)
```

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

1. **U-Net**: Ronneberger et al. (2015) "U-Net: Convolutional Networks for Biomedical Image Segmentation"
2. **Multi-Scale**: Inception architecture (Szegedy et al., 2015)
3. **Channel Attention**: Hu et al. (2018) "Squeeze-and-Excitation Networks"
4. **Shallow CNNs for Remote Sensing**: Zhong et al. (2020)

---

## ğŸ’¡ Tips Äá»ƒ Hiá»ƒu RÃµ HÆ¡n

### 1. Visualize Receptive Field
```python
# Run this to see what area each model "sees"
from src.models import get_model
model = get_model('shallow_unet', in_channels=14)
# â†’ Receptive field calculator
```

### 2. Xem Feature Maps
```python
# Hook vÃ o intermediate layers Ä‘á»ƒ xem model há»c gÃ¬
# Notebook 04, cell visualization
```

### 3. Compare Predictions
```python
# Run notebook 04 Ä‘á»ƒ xem side-by-side comparison
# RGB | Spatial | MultiScale | U-Net | NDVI Change
```

---

**TÃ³m láº¡i:**
- ğŸƒ **Spatial CNN**: Nhanh nhÆ°ng basic
- ğŸ¯ **Multi-Scale CNN**: Sweet spot cho production
- ğŸ¨ **Shallow U-Net**: Best quality
- ğŸŒ¿ **NDVI-Weighted**: ThÃªm domain knowledge vÃ o CNN
