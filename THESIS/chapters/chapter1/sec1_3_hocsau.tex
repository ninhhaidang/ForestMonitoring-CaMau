\section{Học máy và học sâu}

Mạng nơ-ron tích chập (CNN) đã trở thành công cụ chủ đạo trong xử lý ảnh viễn thám và phân loại lớp phủ đất \citeen{zhu2017, zhang2016}. Phần này trình bày các khái niệm nền tảng về học máy và học sâu làm cơ sở cho kiến trúc đề xuất trong Chương~\ref{chap:methodology}.

\subsection{Kiến trúc cơ bản của mạng nơ-ron tích chập}

\textbf{Perceptron - Đơn vị cơ bản:}

Một nơ-ron nhân tạo thực hiện phép biến đổi tuyến tính và hàm kích hoạt:
\begin{equation}
y = f(\mathbf{w}^T \mathbf{x} + b)
\end{equation}

\textbf{Thành phần:} $\mathbf{x} \in \mathbb{R}^n$ là input vector chứa $n$ feature đầu vào; $\mathbf{w} \in \mathbb{R}^n$ là weight vector (trọng số) --- mỗi $w_i$ thể hiện mức độ quan trọng của feature $x_i$; $b \in \mathbb{R}$ là bias (độ lệch) --- cho phép dịch chuyển đường quyết định; $f(\cdot)$ là hàm kích hoạt phi tuyến; $y$ là output (đầu ra). \textbf{Ý nghĩa:} Perceptron mô phỏng cách nơ-ron sinh học xử lý tín hiệu --- nhận nhiều đầu vào, tính tổng có trọng số, và kích hoạt khi vượt ngưỡng. \textbf{Mục đích:} Là đơn vị tính toán cơ bản của mạng nơ-ron, có khả năng học các mối quan hệ tuyến tính và phi tuyến trong dữ liệu thông qua việc điều chỉnh trọng số $\mathbf{w}$ và bias $b$.

\textbf{Multi-Layer Perceptron (MLP):}

Một mạng nơ-ron gồm nhiều layers, trong đó mỗi layer nhận đầu ra của layer trước làm đầu vào. Mỗi layer thực hiện phép biến đổi tuyến tính (nhân ma trận trọng số $\mathbf{W}$ và cộng bias $\mathbf{b}$) sau đó áp dụng hàm kích hoạt. Các layer ẩn giúp mạng học được các biểu diễn trừu tượng của dữ liệu, với mỗi layer học các đặc trưng ở mức độ phức tạp tăng dần.

\textbf{Phép tích chập 2D (2D Convolution):}

Đây là thành phần cốt lõi của CNN, thực hiện phép tích chập giữa input và kernel:
\begin{equation}
(I * K)(i, j) = \sum_m \sum_n I(i+m, j+n) \times K(m, n)
\end{equation}

\textbf{Thành phần:} $I$ là input feature map (ảnh hoặc feature map từ layer trước) với kích thước height × width × channels; $K$ là kernel (filter) có kích thước $k_h \times k_w$ --- chứa các trọng số được học; $(i,j)$ là vị trí trong output feature map; $(m,n)$ là vị trí trong kernel. \textbf{Ý nghĩa:} Phép tích chập trượt kernel qua input, tại mỗi vị trí tính tổng các tích element-wise giữa kernel và vùng tương ứng của input --- tạo ra một giá trị trong output thể hiện mức độ ``khớp'' giữa kernel và vùng đó. \textbf{Mục đích:} Trích xuất các đặc trưng cục bộ (local features) như cạnh, góc, texture từ input; nhờ chia sẻ trọng số (parameter sharing), CNN tiết kiệm tham số và có khả năng nhận diện đặc trưng bất kể vị trí (translation invariance).

Phép tích chập mang lại nhiều ưu điểm quan trọng. Về parameter sharing, cùng một kernel áp dụng toàn bộ input giúp tiết kiệm tham số. Về translation invariance, mạng có khả năng nhận diện các đặc trưng bất kể vị trí xuất hiện. Về local connectivity, mỗi nơ-ron chỉ kết nối với vùng cục bộ của input, giảm số lượng kết nối cần thiết.

Quá trình tích chập diễn ra như sau: kernel có kích thước $k \times k$ trượt qua toàn bộ input với bước nhảy (stride) xác định. Tại mỗi vị trí, kernel thực hiện phép nhân element-wise với vùng tương ứng của input, sau đó cộng tổng tất cả các tích để tạo ra một giá trị trong output feature map. Với input nhiều kênh (ví dụ: feature stack có 27 kênh như trong nghiên cứu này), kernel cũng có cùng số kênh và phép tích chập được thực hiện đồng thời trên tất cả các kênh, sau đó cộng tổng để tạo ra một feature map đầu ra.

\textbf{Hàm kích hoạt (Activation Functions):}

Hàm kích hoạt đóng vai trò quan trọng trong mạng nơ-ron, giúp mô hình học được các mối quan hệ phi tuyến trong dữ liệu \citeen{goodfellow2016}. ReLU (Rectified Linear Unit) là hàm kích hoạt phổ biến nhất:
\begin{equation}
f(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}
\end{equation}

\textbf{Thành phần:} $x$ là giá trị đầu vào (output của phép biến đổi tuyến tính). \textbf{Ý nghĩa:} ReLU giữ nguyên giá trị dương và chuyển giá trị âm về 0, tạo ra tính phi tuyến đơn giản nhưng hiệu quả. \textbf{Mục đích:} Cho phép mạng học các mối quan hệ phi tuyến phức tạp; ReLU có ưu điểm tính toán nhanh, giảm vấn đề vanishing gradient (gradient không bị triệt tiêu với giá trị dương), và tạo sparse activation (nhiều nơ-ron có output = 0).

Softmax được sử dụng cho lớp đầu ra trong bài toán phân loại đa lớp:
\begin{equation}
\text{softmax}(\mathbf{x})_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
\end{equation}

\textbf{Thành phần:} $\mathbf{x}$ là vector logits (đầu ra thô của layer cuối cùng); $x_i$ là logit của lớp $i$; $\exp(\cdot)$ là hàm mũ. \textbf{Ý nghĩa:} Softmax chuyển đổi các logits thành phân phối xác suất --- giá trị lớn hơn được ánh xạ thành xác suất cao hơn, và tổng tất cả xác suất bằng 1. \textbf{Mục đích:} Cho phép mô hình đưa ra dự đoán xác suất cho bài toán phân loại đa lớp; lớp có xác suất cao nhất được chọn làm kết quả dự đoán.

\subsection{Pooling và các kỹ thuật điều chuẩn}

Pooling là phép toán giảm chiều không gian (spatial dimensions) của feature maps, giúp giảm số lượng tham số, giảm chi phí tính toán và tăng khả năng translation invariance của mạng \citeen{lecun2015}. Có hai loại pooling phổ biến: \textbf{Max Pooling} chọn giá trị lớn nhất trong mỗi vùng cục bộ, giữ lại các đặc trưng nổi bật nhất; \textbf{Average Pooling} tính trung bình các giá trị, giữ lại thông tin tổng quát hơn.

\textbf{Global Average Pooling (GAP)} là trường hợp đặc biệt trong đó vùng pooling bao trùm toàn bộ feature map, biến đổi mỗi feature map thành một giá trị scalar \citeen{lin2014}. GAP mang lại nhiều lợi ích: giảm mạnh số lượng tham số, có tính chất điều chuẩn tự nhiên, và tạo ra spatial invariance hoàn toàn. Trong nghiên cứu này, GAP được sử dụng ở cuối mạng CNN để tổng hợp thông tin từ tất cả các spatial locations trước khi đưa vào lớp classification.

\textbf{Chuẩn hóa theo lô (Batch Normalization):}

Chuẩn hóa theo lô là kỹ thuật chuẩn hóa được đề xuất bởi Ioffe và Szegedy nhằm tăng tốc quá trình huấn luyện mạng nơ-ron sâu \citeen{ioffe2015}. Kỹ thuật này chuẩn hóa các activation trong mỗi mini-batch về phân phối có trung bình 0 và phương sai 1, sau đó áp dụng phép biến đổi tuyến tính với các tham số học được. Chuẩn hóa theo lô giúp tăng tốc độ huấn luyện, giảm độ nhạy với khởi tạo trọng số, và đóng vai trò như một phương pháp điều chuẩn.

\textbf{Dropout:}

Dropout là kỹ thuật điều chuẩn hiệu quả được đề xuất bởi Srivastava và cộng sự để ngăn chặn overfitting trong mạng nơ-ron \citeen{srivastava2014}. Trong quá trình huấn luyện, mỗi nơ-ron bị tắt ngẫu nhiên với một xác suất nhất định, buộc mạng học các biểu diễn robust hơn. Dropout2d (Spatial Dropout) là biến thể dropout toàn bộ feature maps thay vì từng nơ-ron, phù hợp cho CNN vì features trong cùng channel có correlation không gian cao.

\subsection{Hàm mất mát và thuật toán tối ưu}

Hàm mất mát (Loss Function) đo lường sự khác biệt giữa dự đoán của mô hình và nhãn thực tế, là cơ sở để tối ưu hóa tham số mạng \citeen{goodfellow2016}. Cross-Entropy Loss được sử dụng cho bài toán phân loại đa lớp:
\begin{equation}
L = -\sum_i y_i \log(\hat{y}_i)
\end{equation}

\textbf{Thành phần:} $y_i$ là nhãn thực tế (true label) được mã hóa one-hot --- có giá trị 1 tại lớp đúng và 0 ở các lớp khác; $\hat{y}_i$ là xác suất dự đoán của lớp $i$ xuất ra từ hàm softmax; $\log(\cdot)$ là logarit tự nhiên. \textbf{Ý nghĩa:} Cross-Entropy đo lường ``khoảng cách'' giữa phân phối dự đoán và phân phối thực tế --- giá trị loss cao khi mô hình gán xác suất thấp cho lớp đúng, và ngược lại. \textbf{Mục đích:} Làm hàm mục tiêu để huấn luyện mô hình; thuật toán tối ưu sẽ điều chỉnh trọng số để giảm thiểu loss, từ đó tăng xác suất dự đoán đúng.

\textbf{Thuật toán tối ưu:}

Các thuật toán tối ưu hóa đóng vai trò quan trọng trong việc huấn luyện mạng nơ-ron. \textbf{Adam} (Adaptive Moment Estimation) được đề xuất bởi Kingma và Ba là một trong những optimizer phổ biến nhất hiện nay \citeen{kingma2015}. Adam kết hợp ưu điểm của hai phương pháp: momentum (sử dụng trung bình động của gradient) và RMSprop (điều chỉnh learning rate theo từng tham số). \textbf{AdamW} là biến thể cải tiến với phân rã trọng số được tách riêng khỏi gradient update, giúp điều chuẩn hiệu quả hơn. Các hyperparameters thường sử dụng: $\beta_1 = 0.9$, $\beta_2 = 0.999$, learning rate $\eta = 0.001$.

\subsection{Phương pháp phân loại ảnh viễn thám}

Trong phân loại ảnh viễn thám, có hai phương pháp tiếp cận chính: phân loại dựa trên pixel và phân loại dựa trên patch \citeen{blaschke2010, zhang2016}.

\textbf{Pixel-based Classification:} Mỗi pixel được phân loại độc lập dựa trên vector đặc trưng của riêng nó. Ưu điểm là đơn giản, dễ triển khai, tốc độ xử lý nhanh. Nhược điểm là không tận dụng ngữ cảnh không gian, dễ tạo ra nhiễu dạng salt-and-pepper.

\textbf{Patch-based Classification:} Trích xuất các patches (cửa sổ) xung quanh mỗi pixel và phân loại dựa trên toàn bộ patch. Ưu điểm là sử dụng được ngữ cảnh không gian, kết quả mượt hơn và phù hợp với CNN.

\subsection{Ma trận nhầm lẫn và các độ đo đánh giá}

Confusion Matrix (Ma trận nhầm lẫn) là công cụ cơ bản để đánh giá hiệu suất của mô hình phân loại, đặc biệt quan trọng trong các bài toán phân loại ảnh viễn thám \citeen{foody2002}. Ma trận này tổng hợp kết quả dự đoán của mô hình so với nhãn thực tế.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
& \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
\hline
\textbf{Actual Positive} & True Positive (TP) & False Negative (FN) \\
\hline
\textbf{Actual Negative} & False Positive (FP) & True Negative (TN) \\
\hline
\end{tabular}
\caption{Cấu trúc Confusion Matrix nhị phân}
\label{tab:confusion_matrix_binary}
\end{table}

Ma trận nhầm lẫn bao gồm bốn thành phần chính. \textbf{True Positive (TP)} là số mẫu dương được dự đoán đúng là dương — trong bài toán phát hiện mất rừng, đây là các điểm thực sự mất rừng và mô hình dự đoán đúng. \textbf{True Negative (TN)} là số mẫu âm được dự đoán đúng là âm. \textbf{False Positive (FP)} là số mẫu âm bị dự đoán nhầm là dương (lỗi loại I). \textbf{False Negative (FN)} là số mẫu dương bị dự đoán nhầm là âm (lỗi loại II). Với bài toán phân loại $K$ lớp, Confusion Matrix có kích thước $K \times K$, trong đó phần tử $C_{ij}$ thể hiện số mẫu thuộc lớp $i$ được dự đoán là lớp $j$.

\textbf{Các độ đo đánh giá:}

Các chỉ số đánh giá được tính toán dựa trên Confusion Matrix để đo lường hiệu suất phân loại từ nhiều góc độ khác nhau \citeen{sokolova2009}.

\textbf{Accuracy (Độ chính xác):}
\begin{equation}
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

Công thức sử dụng các thành phần $TP$ (True Positive), $TN$ (True Negative), $FP$ (False Positive) và $FN$ (False Negative) như định nghĩa ở trên, cho biết tỷ lệ các mẫu được phân loại đúng trên tổng số mẫu. Chỉ số này đánh giá hiệu suất tổng thể của mô hình, tuy nhiên accuracy có thể gây hiểu lầm với dữ liệu mất cân bằng.

\textbf{Precision (Độ chính xác dương):}
\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}
\end{equation}

Công thức sử dụng $TP$ là số mẫu dương được dự đoán đúng và $FP$ là số mẫu âm bị dự đoán nhầm là dương, cho biết trong số các mẫu được dự đoán là dương, tỷ lệ thực sự là dương. Chỉ số này đánh giá độ tin cậy của dự đoán dương; precision cao quan trọng khi chi phí của false positive cao.

\textbf{Recall (Độ nhạy):}
\begin{equation}
\text{Recall} = \frac{TP}{TP + FN}
\end{equation}

Công thức sử dụng $TP$ là số mẫu dương được dự đoán đúng và $FN$ là số mẫu dương bị bỏ sót, cho biết trong số các mẫu thực sự dương, tỷ lệ được phát hiện đúng. Chỉ số này đánh giá khả năng phát hiện các mẫu dương; recall cao quan trọng khi chi phí của false negative cao.

\textbf{F1-Score (Trung bình điều hòa):}
\begin{equation}
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

F1-Score là trung bình điều hòa của Precision và Recall, cân bằng giữa hai độ đo này và bị ảnh hưởng mạnh bởi giá trị thấp hơn. Chỉ số này đánh giá tổng hợp khi cần cân bằng giữa precision và recall, đặc biệt hữu ích với dữ liệu mất cân bằng.

\textbf{ROC-AUC (Area Under ROC Curve):}

Các tiêu chuẩn diễn giải ROC-AUC theo Hosmer và Lemeshow \citeen{hosmer2013}: AUC = 0.5 tương ứng với classifier ngẫu nhiên; 0.5 < AUC < 0.7 là phân biệt kém; 0.7 $\leq$ AUC < 0.8 là chấp nhận được; 0.8 $\leq$ AUC < 0.9 là xuất sắc; và AUC $\geq$ 0.9 là vượt trội.

\subsection{Cross Validation và chuẩn hóa dữ liệu}

\textbf{Cross Validation (Kiểm định chéo):}

Cross Validation là phương pháp đánh giá mô hình giúp ước lượng khả năng generalization của mô hình trên dữ liệu chưa thấy, đồng thời giảm thiểu bias do cách chia dữ liệu \citeen{kohavi1995}. Trong K-Fold Cross Validation, dữ liệu được chia thành $K$ phần bằng nhau, mỗi vòng lặp sử dụng một fold làm tập kiểm tra và $K-1$ folds còn lại làm tập huấn luyện; kết quả cuối cùng là trung bình của $K$ lần đánh giá. Stratified K-Fold là biến thể đảm bảo tỷ lệ các lớp trong mỗi fold tương đương với tỷ lệ trong toàn bộ tập dữ liệu, đặc biệt quan trọng khi dữ liệu mất cân bằng giữa các lớp.

\textbf{Chuẩn hóa dữ liệu (Data Normalization):}

Chuẩn hóa dữ liệu là bước tiền xử lý quan trọng trong học máy, giúp các features có cùng scale và cải thiện hiệu suất huấn luyện \citeen{sola1997}. Z-score Normalization chuyển đổi dữ liệu về phân phối với trung bình bằng 0 và độ lệch chuẩn bằng 1 (chi tiết công thức được trình bày trong Chương~\ref{chap:methodology}). Việc chuẩn hóa dữ liệu cho CNN là cần thiết vì các features có scale khác nhau (ví dụ: NDVI trong $[-1, 1]$, tán xạ ngược trong $[-25, 0]$ dB) sẽ ảnh hưởng không đều đến gradient. \textbf{Lưu ý quan trọng:} Các tham số chuẩn hóa phải được tính trên tập huấn luyện và áp dụng cho cả tập kiểm tra để tránh rò rỉ dữ liệu.

\textbf{Rò rỉ dữ liệu (Data Leakage):}

Rò rỉ dữ liệu xảy ra khi thông tin từ tập kiểm tra ``rò rỉ'' vào quá trình huấn luyện, dẫn đến kết quả đánh giá quá lạc quan \citeen{kaufman2012}. Có ba dạng rò rỉ phổ biến trong viễn thám. Dạng thứ nhất là rò rỉ không gian, xảy ra khi các điểm train/test nằm gần nhau về địa lý. Dạng thứ hai là rò rỉ thời gian, xảy ra khi sử dụng thông tin từ thời điểm sau để dự đoán thời điểm trước. Dạng thứ ba là rò rỉ đặc trưng, xảy ra khi tính statistics trên toàn bộ dữ liệu thay vì chỉ trên tập huấn luyện.
