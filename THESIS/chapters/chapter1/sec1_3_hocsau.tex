\section{Học máy và học sâu}

Mạng nơ-ron tích chập (CNN) đã trở thành công cụ chủ đạo trong xử lý ảnh viễn thám và phân loại lớp phủ đất \citeen{zhu2017, zhang2016}. Phần này trình bày các khái niệm nền tảng về học máy và học sâu làm cơ sở cho kiến trúc đề xuất trong Chương~\ref{chap:methodology}.

\subsection{Kiến trúc cơ bản của mạng nơ-ron tích chập}

Perceptron là đơn vị tính toán cơ bản của mạng nơ-ron nhân tạo, thực hiện phép biến đổi tuyến tính kết hợp với hàm kích hoạt:
\begin{equation}
y = f(\mathbf{w}^T \mathbf{x} + b)
\end{equation}

Trong công thức trên, $\mathbf{x} \in \mathbb{R}^n$ là vector đầu vào chứa $n$ đặc trưng, $\mathbf{w} \in \mathbb{R}^n$ là vector trọng số với mỗi $w_i$ thể hiện mức độ quan trọng của đặc trưng $x_i$, $b \in \mathbb{R}$ là độ lệch cho phép dịch chuyển đường quyết định, $f(\cdot)$ là hàm kích hoạt phi tuyến, và $y$ là đầu ra. Perceptron mô phỏng cách nơ-ron sinh học xử lý tín hiệu thông qua việc nhận nhiều đầu vào, tính tổng có trọng số và kích hoạt khi vượt ngưỡng. Đây là đơn vị tính toán cơ bản có khả năng học các mối quan hệ tuyến tính và phi tuyến trong dữ liệu thông qua việc điều chỉnh trọng số $\mathbf{w}$ và bias $b$.

Multi-Layer Perceptron (MLP) là mạng nơ-ron gồm nhiều lớp, trong đó mỗi lớp nhận đầu ra của lớp trước làm đầu vào. Mỗi lớp thực hiện phép biến đổi tuyến tính (nhân ma trận trọng số $\mathbf{W}$ và cộng bias $\mathbf{b}$) sau đó áp dụng hàm kích hoạt. Các lớp ẩn giúp mạng học được các biểu diễn trừu tượng của dữ liệu, với mỗi lớp học các đặc trưng ở mức độ phức tạp tăng dần.

Phép tích chập 2D là thành phần cốt lõi của CNN, thực hiện phép tích chập giữa đầu vào và kernel:
\begin{equation}
(I * K)(i, j) = \sum_m \sum_n I(i+m, j+n) \times K(m, n)
\end{equation}

Trong công thức trên, $I$ là feature map đầu vào (ảnh hoặc feature map từ lớp trước) với kích thước height × width × channels, $K$ là kernel có kích thước $k_h \times k_w$ chứa các trọng số được học, $(i,j)$ là vị trí trong feature map đầu ra, và $(m,n)$ là vị trí trong kernel. Phép tích chập trượt kernel qua đầu vào, tại mỗi vị trí tính tổng các tích từng phần tử giữa kernel và vùng tương ứng của đầu vào, tạo ra một giá trị trong đầu ra thể hiện mức độ khớp giữa kernel và vùng đó. Mục đích của phép tích chập là trích xuất các đặc trưng cục bộ như cạnh, góc và kết cấu từ đầu vào. Nhờ cơ chế chia sẻ trọng số, CNN tiết kiệm tham số và có khả năng nhận diện đặc trưng bất kể vị trí xuất hiện.

Phép tích chập mang lại nhiều ưu điểm quan trọng. Về parameter sharing, cùng một kernel áp dụng toàn bộ input giúp tiết kiệm tham số. Về translation invariance, mạng có khả năng nhận diện các đặc trưng bất kể vị trí xuất hiện. Về local connectivity, mỗi nơ-ron chỉ kết nối với vùng cục bộ của input, giảm số lượng kết nối cần thiết.

Quá trình tích chập diễn ra như sau: kernel có kích thước $k \times k$ trượt qua toàn bộ input với bước nhảy (stride) xác định. Tại mỗi vị trí, kernel thực hiện phép nhân element-wise với vùng tương ứng của input, sau đó cộng tổng tất cả các tích để tạo ra một giá trị trong output feature map. Với input nhiều kênh (ví dụ: feature stack có 27 kênh như trong nghiên cứu này), kernel cũng có cùng số kênh và phép tích chập được thực hiện đồng thời trên tất cả các kênh, sau đó cộng tổng để tạo ra một feature map đầu ra.

Hàm kích hoạt đóng vai trò quan trọng trong mạng nơ-ron, giúp mô hình học được các mối quan hệ phi tuyến trong dữ liệu \citeen{goodfellow2016}. ReLU (Rectified Linear Unit) là hàm kích hoạt phổ biến nhất, được định nghĩa như sau:
\begin{equation}
f(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}
\end{equation}

Trong công thức trên, $x$ là giá trị đầu vào từ phép biến đổi tuyến tính. ReLU giữ nguyên giá trị dương và chuyển giá trị âm về 0, tạo ra tính phi tuyến đơn giản nhưng hiệu quả. Hàm này cho phép mạng học các mối quan hệ phi tuyến phức tạp với ưu điểm tính toán nhanh, giảm vấn đề triệt tiêu gradient đối với giá trị dương, và tạo ra kích hoạt thưa khi nhiều nơ-ron có đầu ra bằng 0.

Softmax được sử dụng cho lớp đầu ra trong bài toán phân loại đa lớp:
\begin{equation}
\text{softmax}(\mathbf{x})_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
\end{equation}

Trong công thức trên, $\mathbf{x}$ là vector logits (đầu ra thô của lớp cuối cùng), $x_i$ là logit của lớp $i$, và $\exp(\cdot)$ là hàm mũ. Softmax chuyển đổi các logits thành phân phối xác suất, trong đó giá trị lớn hơn được ánh xạ thành xác suất cao hơn và tổng tất cả xác suất bằng 1. Hàm này cho phép mô hình đưa ra dự đoán xác suất cho bài toán phân loại đa lớp, với lớp có xác suất cao nhất được chọn làm kết quả dự đoán.

\subsection{Pooling và các kỹ thuật điều chuẩn}

Pooling là phép toán giảm chiều không gian (spatial dimensions) của feature maps, giúp giảm số lượng tham số, giảm chi phí tính toán và tăng khả năng translation invariance của mạng \citeen{lecun2015}. Có hai loại pooling phổ biến: \textbf{Max Pooling} chọn giá trị lớn nhất trong mỗi vùng cục bộ, giữ lại các đặc trưng nổi bật nhất; \textbf{Average Pooling} tính trung bình các giá trị, giữ lại thông tin tổng quát hơn.

\textbf{Global Average Pooling (GAP)} là trường hợp đặc biệt trong đó vùng pooling bao trùm toàn bộ feature map, biến đổi mỗi feature map thành một giá trị scalar \citeen{lin2014}. GAP mang lại nhiều lợi ích: giảm mạnh số lượng tham số, có tính chất điều chuẩn tự nhiên, và tạo ra spatial invariance hoàn toàn. Trong nghiên cứu này, GAP được sử dụng ở cuối mạng CNN để tổng hợp thông tin từ tất cả các spatial locations trước khi đưa vào lớp classification.

Chuẩn hóa theo lô (Batch Normalization) là kỹ thuật chuẩn hóa được đề xuất bởi Ioffe và Szegedy nhằm tăng tốc quá trình huấn luyện mạng nơ-ron sâu \citeen{ioffe2015}. Kỹ thuật này chuẩn hóa các activation trong mỗi mini-batch về phân phối có trung bình 0 và phương sai 1, sau đó áp dụng phép biến đổi tuyến tính với các tham số học được. Chuẩn hóa theo lô giúp tăng tốc độ huấn luyện, giảm độ nhạy với khởi tạo trọng số, và đóng vai trò như một phương pháp điều chuẩn.

Dropout là kỹ thuật điều chuẩn hiệu quả được đề xuất bởi Srivastava và cộng sự để ngăn chặn quá khớp trong mạng nơ-ron \citeen{srivastava2014}. Trong quá trình huấn luyện, mỗi nơ-ron bị tắt ngẫu nhiên với một xác suất nhất định, buộc mạng học các biểu diễn bền vững hơn. Dropout2d (Spatial Dropout) là biến thể tắt toàn bộ feature maps thay vì từng nơ-ron, phù hợp cho CNN vì các đặc trưng trong cùng kênh có tương quan không gian cao.

\subsection{Hàm mất mát và thuật toán tối ưu}

Hàm mất mát đo lường sự khác biệt giữa dự đoán của mô hình và nhãn thực tế, là cơ sở để tối ưu hóa tham số mạng \citeen{goodfellow2016}. Cross-Entropy Loss được sử dụng cho bài toán phân loại đa lớp:
\begin{equation}
L = -\sum_i y_i \log(\hat{y}_i)
\end{equation}

Trong công thức trên, $y_i$ là nhãn thực tế được mã hóa one-hot với giá trị 1 tại lớp đúng và 0 ở các lớp khác, $\hat{y}_i$ là xác suất dự đoán của lớp $i$ xuất ra từ hàm softmax, và $\log(\cdot)$ là logarit tự nhiên. Cross-Entropy đo lường khoảng cách giữa phân phối dự đoán và phân phối thực tế, trong đó giá trị mất mát cao khi mô hình gán xác suất thấp cho lớp đúng và ngược lại. Hàm này được sử dụng làm mục tiêu để huấn luyện mô hình, trong đó thuật toán tối ưu điều chỉnh trọng số để giảm thiểu mất mát, từ đó tăng xác suất dự đoán đúng.

Các thuật toán tối ưu hóa đóng vai trò quan trọng trong việc huấn luyện mạng nơ-ron. Adam (Adaptive Moment Estimation) được đề xuất bởi Kingma và Ba là một trong những thuật toán tối ưu phổ biến nhất hiện nay \citeen{kingma2015}. Adam kết hợp ưu điểm của hai phương pháp: momentum (sử dụng trung bình động của gradient) và RMSprop (điều chỉnh tốc độ học theo từng tham số). AdamW là biến thể cải tiến với phân rã trọng số được tách riêng khỏi cập nhật gradient, giúp điều chuẩn hiệu quả hơn. Các siêu tham số thường sử dụng bao gồm $\beta_1 = 0.9$, $\beta_2 = 0.999$ và tốc độ học $\eta = 0.001$.

\subsection{Phương pháp phân loại ảnh viễn thám}

Trong phân loại ảnh viễn thám, có hai phương pháp tiếp cận chính: phân loại dựa trên pixel và phân loại dựa trên patch \citeen{blaschke2010, zhang2016}.

Phương pháp phân loại dựa trên pixel thực hiện phân loại mỗi pixel độc lập dựa trên vector đặc trưng của riêng nó. Ưu điểm của phương pháp này là đơn giản, dễ triển khai và tốc độ xử lý nhanh. Tuy nhiên, nhược điểm là không tận dụng được ngữ cảnh không gian, dễ tạo ra nhiễu dạng muối-tiêu.

Phương pháp phân loại dựa trên patch trích xuất các cửa sổ xung quanh mỗi pixel và phân loại dựa trên toàn bộ patch. Ưu điểm của phương pháp này là sử dụng được ngữ cảnh không gian, cho kết quả mượt hơn và phù hợp với kiến trúc CNN.

\subsection{Chuẩn hóa dữ liệu}

Chuẩn hóa dữ liệu là bước tiền xử lý quan trọng trong học máy, giúp các đặc trưng có cùng thang đo và cải thiện hiệu suất huấn luyện \citeen{sola1997}. Chuẩn hóa Z-score chuyển đổi dữ liệu về phân phối với trung bình bằng 0 và độ lệch chuẩn bằng 1 (chi tiết công thức được trình bày trong Chương~\ref{chap:methodology}). Việc chuẩn hóa dữ liệu cho CNN là cần thiết vì các đặc trưng có thang đo khác nhau (ví dụ: NDVI trong $[-1, 1]$, tán xạ ngược trong $[-25, 0]$ dB) sẽ ảnh hưởng không đều đến gradient. Cần lưu ý rằng các tham số chuẩn hóa ($\mu$ và $\sigma$) phải được tính trên tập huấn luyện và áp dụng cho cả tập kiểm tra để đảm bảo tính khách quan của việc đánh giá mô hình.
