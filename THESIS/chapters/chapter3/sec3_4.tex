\section{Huấn luyện mô hình}

\subsection{Kiến trúc mô hình CNN}

Mô hình nhận đầu vào là tensor kích thước (batch\_size, 3, 3, 27), sau đó được chuyển đổi sang định dạng PyTorch (batch\_size, 27, 3, 3). Kiến trúc bao gồm năm thành phần chính. Thành phần thứ nhất là khối tích chập 1 gồm Conv2D (64 filters, kernel 3×3), BatchNorm2D, ReLU và Dropout2D (p=0.7). Thành phần thứ hai là khối tích chập 2 gồm Conv2D (32 filters, kernel 3×3), BatchNorm2D, ReLU và Dropout2D (p=0.7). Thành phần thứ ba là lớp Global Average Pooling có chức năng gộp thông tin không gian. Thành phần thứ tư là khối kết nối đầy đủ gồm Linear (32→64), BatchNorm1D, ReLU và Dropout (p=0.7). Thành phần thứ năm là lớp đầu ra Linear (64→4) cho 4 lớp phân loại.

Với 36,676 tham số, mô hình có độ phức tạp vừa phải, phù hợp với quy mô bộ dữ liệu 2,630 mẫu. Kiến trúc tổng quan của mô hình được minh họa trong Hình~\ref{fig:cnn_architecture}.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{CNN-architecture.png}
\caption{Kiến trúc mô hình mạng nơ-ron tích chập (CNN) được sử dụng trong nghiên cứu}
\label{fig:cnn_architecture}
\end{figure}

\subsection{Chiến lược điều chuẩn và huấn luyện}

Với bộ dữ liệu có quy mô nhỏ, nghiên cứu kết hợp ba kỹ thuật điều chuẩn. Kỹ thuật thứ nhất là chuẩn hóa theo lô (Batch Normalization) \citeen{ioffe2015} được áp dụng sau mỗi lớp tích chập và fully-connected. Kỹ thuật thứ hai là Dropout với tỷ lệ 70\% \citeen{srivastava2014} --- tỷ lệ cao do tỷ lệ mẫu/tham số thấp (khoảng 72:1). Kỹ thuật thứ ba là phân rã trọng số với hệ số $\lambda = 10^{-3}$ thông qua optimizer AdamW.

\begin{table}[H]
\centering
\caption{Cấu hình siêu tham số huấn luyện}
\label{tab:hyperparams}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Tham số} & \textbf{Giá trị} & \textbf{Giải thích} \\
\hline
epochs & 200 & Số epochs tối đa với early stopping \\
\hline
batch\_size & 64 & Cân bằng giữa độ ổn định và tốc độ \\
\hline
learning\_rate & 0.001 & Learning rate khởi tạo cho AdamW \\
\hline
weight\_decay & $10^{-3}$ & Hệ số điều chuẩn L2 \\
\hline
dropout\_rate & 0.7 & Dropout cao để điều chuẩn mạnh \\
\hline
early\_stopping & 15 epochs & Patience trước khi dừng sớm \\
\hline
\end{tabular}
\end{table}

Nghiên cứu sử dụng AdamW \citeen{loshchilov2019} — biến thể cải tiến của Adam với phân rã trọng số tách rời. ReduceLROnPlateau scheduler tự động giảm learning rate (factor=0.5, patience=10) khi validation loss không cải thiện.

\subsection{Quy trình huấn luyện}

Quy trình huấn luyện được thực hiện qua bốn bước. Đầu tiên, trọng số được khởi tạo theo phương pháp Kaiming/He initialization \citeen{he2015}. Tiếp theo, 5-Fold Cross Validation được thực hiện trên 80\% dữ liệu để đánh giá độ ổn định của mô hình. Sau đó, Final Model được huấn luyện trên toàn bộ 80\% dữ liệu với cơ chế early stopping. Cuối cùng, mô hình được đánh giá trên 20\% test set cố định.

\begin{table}[H]
\centering
\caption{Chi tiết số tham số huấn luyện của mô hình}
\label{tab:model_params}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Lớp} & \textbf{Số tham số} & \textbf{Cách tính} \\
\hline
Tích chập 1 + Chuẩn hóa batch 1 & 15,680 & 27×3×3×64 + 128 \\
\hline
Tích chập 2 + Chuẩn hóa batch 2 & 18,496 & 64×3×3×32 + 64 \\
\hline
Kết nối đầy đủ 1 + Chuẩn hóa batch 3 & 2,240 & 32×64 + 64 + 128 \\
\hline
Kết nối đầy đủ 2 (Đầu ra) & 260 & 64×4 + 4 \\
\hline
\textbf{Tổng cộng} & \textbf{36,676} & \\
\hline
\end{tabular}
\end{table}
