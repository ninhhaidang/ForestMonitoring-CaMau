\section{Huấn luyện mô hình}

\subsection{Kiến trúc mô hình CNN}

Mô hình nhận đầu vào là tensor kích thước (batch\_size, 3, 3, 27), sau đó được chuyển đổi sang định dạng PyTorch (batch\_size, 27, 3, 3). Kiến trúc bao gồm các thành phần chính:

\begin{enumerate}
    \item \textbf{Khối tích chập 1:} Conv2D (64 filters, kernel 3×3) → BatchNorm2D → ReLU → Dropout2D (p=0.7)
    \item \textbf{Khối tích chập 2:} Conv2D (32 filters, kernel 3×3) → BatchNorm2D → ReLU → Dropout2D (p=0.7)
    \item \textbf{Global Average Pooling:} Gộp thông tin không gian
    \item \textbf{Khối kết nối đầy đủ:} Linear (32→64) → BatchNorm1D → ReLU → Dropout (p=0.7)
    \item \textbf{Lớp đầu ra:} Linear (64→4) cho 4 lớp phân loại
\end{enumerate}

Với 36,676 tham số, mô hình có độ phức tạp vừa phải, phù hợp với quy mô bộ dữ liệu 2,630 mẫu. Kiến trúc tổng quan của mô hình được minh họa trong Hình~\ref{fig:cnn_architecture}.

% TODO: Thay thế placeholder bằng hình kiến trúc CNN thực tế
\begin{figure}[H]
\centering
\fbox{\parbox{0.8\textwidth}{\centering\vspace{3cm}\textbf{[PLACEHOLDER: Sơ đồ kiến trúc mô hình CNN]}\\\vspace{0.5cm}Input (3×3×27) → Conv Blocks → GAP → FC → Output (4 classes)\vspace{3cm}}}
\caption{Kiến trúc mô hình CNN phân loại biến động rừng}
\label{fig:cnn_architecture}
\end{figure}

\subsection{Chiến lược điều chuẩn và huấn luyện}

Với bộ dữ liệu có quy mô nhỏ, nghiên cứu kết hợp ba kỹ thuật điều chuẩn: (1) \textbf{Batch Normalization} \cite{ioffe2015} sau mỗi lớp tích chập và fully-connected; (2) \textbf{Dropout 70\%} \cite{srivastava2014} — tỷ lệ cao do tỷ lệ mẫu/tham số thấp (~72:1); (3) \textbf{Weight Decay} $\lambda = 10^{-3}$ thông qua optimizer AdamW.

\begin{table}[H]
\centering
\caption{Cấu hình siêu tham số huấn luyện}
\label{tab:hyperparams}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Tham số} & \textbf{Giá trị} & \textbf{Giải thích} \\
\hline
epochs & 200 & Số epochs tối đa với early stopping \\
\hline
batch\_size & 64 & Cân bằng giữa độ ổn định và tốc độ \\
\hline
learning\_rate & 0.001 & Learning rate khởi tạo cho AdamW \\
\hline
weight\_decay & $10^{-3}$ & Hệ số L2 regularization \\
\hline
dropout\_rate & 0.7 & Dropout cao để regularization mạnh \\
\hline
early\_stopping & 15 epochs & Patience trước khi dừng sớm \\
\hline
\end{tabular}
\end{table}

Nghiên cứu sử dụng AdamW \cite{loshchilov2019} — biến thể cải tiến của Adam với decoupled weight decay. ReduceLROnPlateau scheduler tự động giảm learning rate (factor=0.5, patience=10) khi validation loss không cải thiện.

\subsection{Quy trình huấn luyện}

Quy trình huấn luyện được thực hiện theo các bước: (1) Khởi tạo trọng số theo phương pháp Kaiming/He initialization \cite{he2015}; (2) 5-Fold Cross Validation trên 80\% dữ liệu để đánh giá độ ổn định; (3) Huấn luyện Final Model trên toàn bộ 80\% với early stopping; (4) Đánh giá trên 20\% test set cố định.

\begin{table}[H]
\centering
\caption{Chi tiết số tham số huấn luyện của mô hình}
\label{tab:model_params}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Lớp} & \textbf{Số tham số} & \textbf{Cách tính} \\
\hline
Conv1 + BN1 & 15,680 & 27×3×3×64 + 128 \\
\hline
Conv2 + BN2 & 18,496 & 64×3×3×32 + 64 \\
\hline
FC1 + BN3 & 2,240 & 32×64 + 64 + 128 \\
\hline
FC2 (Output) & 260 & 64×4 + 4 \\
\hline
\textbf{TỔNG} & \textbf{36,676} & \\
\hline
\end{tabular}
\end{table}
