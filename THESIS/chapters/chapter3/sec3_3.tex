\section{Kiến trúc mô hình CNN đề xuất}

\subsection{Thiết kế kiến trúc}

\begin{figure}[H]
    \centering
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{3cm}\textbf{[PLACEHOLDER]}\\ Sơ đồ kiến trúc CNN chi tiết với các layer,\\ kích thước tensor ở mỗi bước, và số parameters\vspace{3cm}}}
    \caption{Kiến trúc mô hình CNN đề xuất}
    \label{fig:cnn_architecture}
\end{figure}

\textbf{Tổng quan architecture:}

\begin{verbatim}
INPUT: (batch_size, 3, 3, 27)
    |
PERMUTE -> (batch_size, 27, 3, 3)  # PyTorch format (N, C, H, W)
    |
CONVOLUTIONAL BLOCK 1
    Conv2D(27 -> 64, kernel=3x3)
    BatchNorm2D(64)
    ReLU()
    Dropout2D(p=0.7)
    | (batch_size, 64, 3, 3)
CONVOLUTIONAL BLOCK 2
    Conv2D(64 -> 32, kernel=3x3)
    BatchNorm2D(32)
    ReLU()
    Dropout2D(p=0.7)
    | (batch_size, 32, 3, 3)
GLOBAL AVERAGE POOLING
    | (batch_size, 32)
FULLY CONNECTED BLOCK
    Linear(32 -> 64)
    BatchNorm1D(64)
    ReLU()
    Dropout(p=0.7)
    | (batch_size, 64)
OUTPUT LAYER
    Linear(64 -> 4)
    |
OUTPUT: (batch_size, 4)  # Logits for 4 classes
\end{verbatim}

\subsection{Chiến lược Regularization}

Với bộ dữ liệu có quy mô nhỏ (2,630 mẫu), việc áp dụng các kỹ thuật regularization mạnh là cần thiết để tránh hiện tượng overfitting. Nghiên cứu này kết hợp ba kỹ thuật regularization chính:

\textbf{(1) Batch Normalization:} Được áp dụng sau mỗi lớp tích chập và lớp fully-connected, giúp ổn định quá trình huấn luyện, cho phép sử dụng learning rate cao hơn và đóng vai trò như một dạng regularization ngầm định.

\textbf{(2) Dropout với tỷ lệ 0.7:} Tỷ lệ dropout 70\% được lựa chọn dựa trên các yếu tố sau:
\begin{itemize}
    \item \textbf{Quy mô dữ liệu nhỏ:} Với chỉ 2,630 mẫu huấn luyện và mô hình có 36,676 tham số, tỷ lệ mẫu/tham số thấp (~72:1) đòi hỏi regularization mạnh để tránh overfitting.
    \item \textbf{Kết quả thực nghiệm:} Qua quá trình thử nghiệm với các giá trị dropout từ 0.3 đến 0.8, tỷ lệ 0.7 cho kết quả tối ưu với khoảng cách nhỏ nhất giữa training accuracy và validation accuracy.
    \item \textbf{Dropout2D cho lớp tích chập:} Thay vì dropout từng neuron, Dropout2D loại bỏ toàn bộ feature maps, phù hợp hơn cho CNN vì các features trong cùng một channel có tương quan không gian cao.
\end{itemize}

\textbf{(3) Weight Decay (L2 Regularization):} Hệ số weight decay $\lambda = 10^{-3}$ được áp dụng thông qua optimizer AdamW, thêm penalty vào hàm loss để hạn chế các trọng số có giá trị lớn.

\subsection{Parameter Count}

\begin{table}[H]
\centering
\caption{Tổng số trainable parameters}
\label{tab:parameters}
\begin{tabular}{|l|l|c|l|}
\hline
\textbf{Layer} & \textbf{Type} & \textbf{Parameters} & \textbf{Calculation} \\
\hline
Conv1 & Weights & 15,552 & 27×3×3×64 \\
\hline
BN1 & $\gamma$, $\beta$ & 128 & 64 + 64 \\
\hline
Conv2 & Weights & 18,432 & 64×3×3×32 \\
\hline
BN2 & $\gamma$, $\beta$ & 64 & 32 + 32 \\
\hline
GAP & - & 0 & No params \\
\hline
FC1 & Weights, bias & 2,112 & 32×64 + 64 \\
\hline
BN3 & $\gamma$, $\beta$ & 128 & 64 + 64 \\
\hline
FC2 & Weights, bias & 260 & 64×4 + 4 \\
\hline
\textbf{TOTAL} & & \textbf{36,676} & \\
\hline
\end{tabular}
\end{table}
