\section{Mạng Neural Tích chập (Convolutional Neural Networks)}

Mạng neural tích chập (CNN) đã trở thành công cụ chủ đạo trong xử lý ảnh viễn thám và phân loại lớp phủ đất \cite{zhu2017, zhang2016}. Phần này trình bày các khái niệm nền tảng về CNN làm cơ sở cho kiến trúc đề xuất trong Chương 3.

\subsection{Giới thiệu về Neural Networks}

\textbf{Perceptron - Đơn vị cơ bản:}

Một neuron nhân tạo thực hiện phép biến đổi tuyến tính và hàm kích hoạt:
\begin{equation}
y = f(\mathbf{w}^T \mathbf{x} + b)
\end{equation}

Trong đó, $\mathbf{x} \in \mathbb{R}^n$ là input vector chứa $n$ feature, $\mathbf{w} \in \mathbb{R}^n$ là weight vector, $b \in \mathbb{R}$ là bias, $f(\cdot)$ là hàm kích hoạt và $y$ là output.

\textbf{Multi-Layer Perceptron (MLP):}

Một mạng neural gồm nhiều layers:
\begin{align}
\text{Layer 1: } & \mathbf{h}_1 = f_1(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) \\
\text{Layer 2: } & \mathbf{h}_2 = f_2(\mathbf{W}_2 \mathbf{h}_1 + \mathbf{b}_2) \\
\text{Output: } & \mathbf{y} = f_n(\mathbf{W}_n \mathbf{h}_{n-1} + \mathbf{b}_n)
\end{align}

\begin{figure}[H]
    \centering
    \fbox{\parbox{0.8\textwidth}{\centering\vspace{2cm}\textbf{[PLACEHOLDER]}\\ Sơ đồ cấu trúc một perceptron\\ với inputs, weights, bias, activation function\vspace{2cm}}}
    \caption{Cấu trúc cơ bản của một perceptron}
    \label{fig:perceptron}
\end{figure}

\subsection{Convolutional Layer}

\textbf{Phép tích chập 2D (2D Convolution):}

Đây là thành phần cốt lõi của CNN, thực hiện phép tích chập giữa input và kernel:
\begin{equation}
(I * K)(i, j) = \sum_m \sum_n I(i+m, j+n) \times K(m, n)
\end{equation}

Trong đó, $I$ là input feature map với kích thước height × width × channels, $K$ là kernel hoặc filter có kích thước $k_h \times k_w$, $(i,j)$ là vị trí trong output và $(m,n)$ là vị trí trong kernel.

\textbf{Ưu điểm của Convolution:}

Phép tích chập mang lại nhiều ưu điểm quan trọng. Về parameter sharing, cùng một kernel áp dụng toàn bộ input giúp tiết kiệm tham số. Về translation invariance, mạng có khả năng nhận diện các đặc trưng bất kể vị trí xuất hiện. Về local connectivity, mỗi neuron chỉ kết nối với vùng cục bộ của input, giảm số lượng kết nối cần thiết.

\begin{figure}[H]
    \centering
    \fbox{\parbox{0.8\textwidth}{\centering\vspace{2cm}\textbf{[PLACEHOLDER]}\\ Hình minh họa phép tích chập 2D\\ với kernel 3×3 trượt trên input\vspace{2cm}}}
    \caption{Minh họa phép tích chập 2D}
    \label{fig:convolution}
\end{figure}

\subsection{Activation Functions}

\textbf{ReLU (Rectified Linear Unit):}
\begin{equation}
f(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}
\end{equation}

ReLU có các ưu điểm như tính toán nhanh, giảm vấn đề vanishing gradient và cho sparse activation.

\textbf{Softmax (cho Output Layer):}
\begin{equation}
\text{softmax}(\mathbf{x})_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
\end{equation}

Các đầu ra của hàm softmax là các xác suất thuộc khoảng [0, 1] và tổng bằng 1, phù hợp cho bài toán phân loại đa lớp.

\subsection{Pooling Layers}

Pooling là phép toán giảm chiều không gian (spatial dimensions) của feature maps, giúp giảm số lượng tham số, giảm chi phí tính toán và tăng khả năng translation invariance của mạng \cite{lecun2015}.

\textbf{Max Pooling:}

Max Pooling chọn giá trị lớn nhất trong mỗi vùng cục bộ:
\begin{equation}
y_{i,j} = \max_{(m,n) \in R_{i,j}} x_{m,n}
\end{equation}

Trong đó, $R_{i,j}$ là vùng pooling (thường là 2×2) tại vị trí $(i,j)$. Max Pooling giữ lại các đặc trưng nổi bật nhất (edge, texture) và loại bỏ thông tin ít quan trọng.

\textbf{Average Pooling:}

Average Pooling tính trung bình các giá trị trong vùng pooling:
\begin{equation}
y_{i,j} = \frac{1}{|R_{i,j}|} \sum_{(m,n) \in R_{i,j}} x_{m,n}
\end{equation}

Average Pooling giữ lại thông tin tổng quát hơn, phù hợp khi tất cả các giá trị trong vùng đều có ý nghĩa.

\textbf{Global Average Pooling (GAP):}

Global Average Pooling là trường hợp đặc biệt của Average Pooling, trong đó vùng pooling bao trùm toàn bộ feature map \cite{lin2013}:
\begin{equation}
y_c = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} x_{c,i,j}
\end{equation}

Trong đó, $H$ và $W$ là chiều cao và chiều rộng của feature map, $c$ là chỉ số channel. GAP biến đổi mỗi feature map có kích thước $H \times W$ thành một giá trị scalar, tạo ra vector có chiều bằng số channels.

\textbf{Ưu điểm của Global Average Pooling:}

GAP mang lại nhiều lợi ích quan trọng. Thứ nhất, GAP giảm mạnh số lượng tham số bằng cách loại bỏ hoàn toàn các fully-connected layers truyền thống. Thứ hai, GAP có tính chất regularization tự nhiên, giảm nguy cơ overfitting do không có tham số cần học. Thứ ba, GAP tạo ra spatial invariance hoàn toàn, cho phép mạng xử lý input có kích thước khác nhau. Trong nghiên cứu này, GAP được sử dụng ở cuối mạng CNN để tổng hợp thông tin từ tất cả các spatial locations trước khi đưa vào lớp classification.

\subsection{Batch Normalization}

\textbf{Batch Normalization Algorithm:}

Với một mini-batch $B = \{x_1, x_2, ..., x_m\}$:
\begin{align}
\mu_B &= \frac{1}{m} \sum_i x_i \\
\sigma^2_B &= \frac{1}{m} \sum_i (x_i - \mu_B)^2 \\
\hat{x}_i &= \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \epsilon}} \\
y_i &= \gamma \hat{x}_i + \beta
\end{align}

Batch Normalization giúp tăng tốc độ huấn luyện, giảm độ nhạy với khởi tạo trọng số, và đóng vai trò như một phương pháp regularization.

\subsection{Dropout}

\textbf{Motivation:}

Overfitting xảy ra khi model học quá chi tiết training data, không generalize tốt cho unseen data.

\textbf{Dropout Algorithm (Training):}
\begin{equation}
\text{For each neuron } i: \quad r_i \sim \text{Bernoulli}(p), \quad \tilde{y}_i = r_i \times y_i
\end{equation}

Với xác suất $p$ giữ lại neuron, nếu $r_i = 0$, neuron bị tắt.

\textbf{Dropout2d (Spatial Dropout):}

Thay vì dropout từng neuron, dropout toàn bộ feature maps, phù hợp cho CNN vì features trong cùng channel có correlation không gian cao.

\subsection{Loss Functions}

\textbf{Cross-Entropy Loss (Multi-class Classification):}
\begin{equation}
L = -\sum_i y_i \log(\hat{y}_i)
\end{equation}

Trong đó, $y_i$ là nhãn thực tế (true label) được mã hóa one-hot và $\hat{y}_i$ là xác suất dự đoán xuất ra từ hàm softmax.

\subsection{Optimization Algorithms}

\textbf{Adam (Adaptive Moment Estimation):}
\begin{align}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) \nabla L(\theta_t) \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) (\nabla L(\theta_t))^2 \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
\theta_{t+1} &= \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}

Hyperparameters thường sử dụng: $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$, $\eta = 0.001$.

\textbf{AdamW (Adam with Weight Decay):}
\begin{equation}
\theta_{t+1} = \theta_t - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t \right)
\end{equation}

Trong đó $\lambda$ là weight decay coefficient (L2 regularization).