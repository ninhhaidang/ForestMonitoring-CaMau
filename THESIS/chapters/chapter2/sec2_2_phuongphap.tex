\section{Phương pháp nghiên cứu}

Phần này trình bày phương pháp nghiên cứu và quy trình tổng quan được minh họa trong Hình~\ref{fig:methodology-flowchart}. Quy trình bao gồm năm giai đoạn chính: (1) Thu thập và tiền xử lý dữ liệu, (2) Trích xuất đặc trưng, (3) Chuẩn bị mẫu huấn luyện, (4) Huấn luyện mô hình, và (5) Áp dụng mô hình.

% Sơ đồ quy trình phương pháp nghiên cứu
\begin{figure}[H]
\centering
\includegraphics[width=0.9425\textwidth]{img/chapter2/flowchart.png}
\caption{Sơ đồ quy trình phương pháp nghiên cứu phát hiện biến động rừng}
\label{fig:methodology-flowchart}
\end{figure}

\subsection{Trích xuất đặc trưng}

Việc kết hợp dữ liệu ra-đa khẩu độ tổng hợp và quang học đã được chứng minh là hiệu quả trong nhiều nghiên cứu phân loại lớp phủ đất \citeen{ienco2019, hu2020}. Cách tiếp cận này tận dụng ưu điểm bổ sung của hai nguồn dữ liệu: ra-đa khẩu độ tổng hợp cung cấp thông tin về cấu trúc và độ ẩm bề mặt, trong khi quang học đa phổ cung cấp thông tin về đặc tính quang phổ của thực vật.

Tổng cộng 27 đặc trưng được xây dựng từ hai nguồn dữ liệu. Sentinel-2 đóng góp 21 đặc trưng quang học, bao gồm 4 kênh phổ (B4 --- đỏ, B8 --- cận hồng ngoại, B11 và B12 --- hồng ngoại sóng ngắn) và 3 chỉ số thực vật (NDVI, NBR, NDMI). Mỗi kênh phổ/chỉ số được tính cho cả hai thời kỳ (kỳ trước và kỳ sau) cùng với giá trị biến động delta ($\Delta$ = kỳ sau $-$ kỳ trước), tạo thành $7 \times 3 = 21$ đặc trưng. Sentinel-1 đóng góp 6 đặc trưng ra-đa từ hai kênh phân cực VV và VH, cũng được tính cho cả hai thời kỳ và giá trị delta ($2 \times 3 = 6$ đặc trưng).

% \subsubsection{Trích xuất patch 3×3}

Với mỗi điểm mẫu, một patch kích thước 3×3 điểm ảnh được trích xuất từ feature stack. Kích thước 3×3 được lựa chọn dựa trên ba tiêu chí: (1) cho phép mô hình học được thông tin ngữ cảnh không gian xung quanh điểm ảnh trung tâm; (2) phù hợp với độ phân giải 10m của Sentinel, mỗi patch tương đương vùng 30m × 30m — kích thước hợp lý cho các lô rừng ngập mặn; và (3) giảm thiểu nhiễu từ các điểm ảnh lân cận không đồng nhất. Kết quả thử nghiệm so sánh các kích thước patch khác nhau (xem Bảng~\ref{tab:patch_size}) xác nhận rằng patch 3×3 cho độ chính xác cao nhất (98,86\%) với số lượng tham số ít nhất.

Kết quả là mỗi tensor --- cấu trúc dữ liệu mảng đa chiều dùng để lưu trữ và xử lý dữ liệu trong học sâu --- có kích thước (3, 3, 27) — tương ứng với kích thước điểm ảnh và số kênh đặc trưng.

\subsection{Chuẩn bị mẫu huấn luyện}

% \subsubsection{Chuẩn hóa dữ liệu}

Việc chuẩn hóa dữ liệu là bước quan trọng để đảm bảo các đặc trưng có cùng phạm vi giá trị, giúp quá trình huấn luyện mô hình hội tụ nhanh và ổn định hơn. Nghiên cứu này áp dụng phương pháp chuẩn hóa Z-score:

\begin{equation}
x_{normalized} = \frac{x - \mu}{\sigma}
\end{equation}

trong đó $x$ là giá trị gốc, $\mu$ là giá trị trung bình và $\sigma$ là độ lệch chuẩn.

Để tránh rò rỉ dữ liệu, các tham số chuẩn hóa ($\mu$ và $\sigma$) được tính toán chỉ trên tập huấn luyện, sau đó áp dụng cho cả tập kiểm định và tập kiểm tra. Các tham số này cũng được lưu lại để sử dụng khi dự đoán trên dữ liệu mới.

% \subsubsection{Phân chia dữ liệu}

Chiến lược chia dữ liệu được thiết kế theo khuyến nghị của Roberts và cộng sự \citeen{roberts2017} về kiểm định chéo cho dữ liệu không gian. Ban đầu tách 20\% dữ liệu làm tập kiểm tra cố định (526 mẫu) — tập này không được sử dụng trong quá trình huấn luyện hay tinh chỉnh siêu tham số. Sau đó áp dụng kiểm định chéo 5 phần trên 80\% dữ liệu còn lại (2.104 mẫu) để đánh giá độ ổn định của mô hình và tìm kiếm siêu tham số tối ưu. Tiếp theo sau khi xác định siêu tham số tối ưu, huấn luyện mô hình cuối cùng trên toàn bộ 80\% dữ liệu để tận dụng tối đa dữ liệu huấn luyện. Cuối cùng đánh giá mô hình cuối cùng trên 20\% tập kiểm tra để báo cáo kết quả.

Cụ thể, việc tách 80/20 được thực hiện theo phương pháp phân chia theo lớp (stratified sampling) để giữ nguyên tỷ lệ tương ứng của bốn lớp (Rừng ổn định, Mất rừng, Phi rừng, Phục hồi rừng) trên cả hai tập. Quy trình thực hiện bằng cách tính tỷ lệ phần trăm của mỗi lớp trên toàn bộ tập dữ liệu sau đó tách 20\% mẫu làm tập kiểm tra cố định sao cho phân bố lớp trong tập kiểm tra tương tự phân bố trong tập gốc cuối cùng là trên 80\% dữ liệu còn lại, áp dụng kiểm định chéo 5 phần để thực hiện kiểm định chéo và tìm siêu tham số, đảm bảo mỗi phần có phân bố lớp tương tự.

\subsection{Kiến trúc mô hình CNN}

Nghiên cứu này sử dụng một kiến trúc mạng nơ-ron tích chập được thiết kế phù hợp với quy mô bộ dữ liệu 2.630 mẫu. Kiến trúc tổng quan của mô hình được minh họa trong Hình~\ref{fig:cnn_architecture}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{chapter2/CNN-architecture.png}
\caption{Kiến trúc mô hình CNN được sử dụng trong nghiên cứu}
\label{fig:cnn_architecture}
\end{figure}

Kiến trúc mô hình được xây dựng dựa trên các kỹ thuật đã được chứng minh hiệu quả trong lĩnh vực học sâu, đồng thời tùy chỉnh các siêu tham số để phù hợp với đặc thù bài toán và quy mô dữ liệu. Về các thành phần chuẩn, nghiên cứu sử dụng bộ lọc tích chập kích thước $3 \times 3$ --- kích thước được Simonyan và Zisserman \citeen{simonyan2015} chứng minh là hiệu quả nhất trong kiến trúc VGGNet, cho phép tăng chiều sâu mạng với ít tham số hơn so với bộ lọc lớn. Chuẩn hóa theo lô  \citeen{ioffe2015} được áp dụng sau mỗi lớp tích chập để ổn định quá trình huấn luyện. Hàm kích hoạt ReLU được sử dụng do tính đơn giản và khả năng giảm thiểu vấn đề triệt tiêu gradient \citeen{goodfellow2016}. GAP được đề xuất bởi Lin và cộng sự \citeen{lin2014} thay thế cho các lớp kết nối đầy đủ truyền thống, giúp giảm đáng kể số lượng tham số. Kỹ thuật Dropout \citeen{srivastava2014} được sử dụng để ngăn ngừa hiện tượng quá khớp.

Về các siêu tham số được tùy chỉnh, nghiên cứu lựa chọn số lượng bộ lọc giảm dần từ 64 xuống 32 qua các lớp tích chập, thay vì tăng dần như các kiến trúc CNN truyền thống (VGGNet, ResNet). Lựa chọn này nhằm giữ mô hình nhẹ và tránh quá khớp với bộ dữ liệu 2.630 mẫu. Tỷ lệ Dropout được thiết lập ở mức 70\% --- cao hơn mức thông thường (20--50\%) --- để tăng cường khả năng điều chuẩn cho mô hình khi huấn luyện với dữ liệu hạn chế. Kiến trúc chỉ sử dụng hai lớp tích chập, đủ để trích xuất đặc trưng từ patch $3 \times 3$ mà không gây ra hiện tượng quá khớp.

Dữ liệu đầu vào là các patch kích thước $3 \times 3$ điểm ảnh với 27 kênh đặc trưng, được biểu diễn dưới dạng tensor có kích thước $(N, 27, 3, 3)$ với $N$ là số lượng mẫu trong mỗi batch. Quy ước này tuân theo định dạng chuẩn của PyTorch (batch, channels, height, width), trong đó 27 là số kênh đặc trưng, và $3 \times 3$ là kích thước không gian của patch. Luồng xử lý của mô hình được mô tả như sau.

Đầu tiên, dữ liệu đi qua khối tích chập thứ nhất sử dụng 64 bộ lọc kích thước $3 \times 3$. Mỗi bộ lọc kết hợp thông tin từ tất cả 27 kênh đặc trưng để tạo ra một bản đồ đặc trưng mới, giúp mô hình học được mối quan hệ giữa các kênh phổ, chỉ số thực vật và dữ liệu ra-đa. Sau lớp tích chập là lớp chuẩn hóa theo lô \citeen{ioffe2015} giúp ổn định quá trình huấn luyện, hàm kích hoạt ReLU đưa tính phi tuyến vào mô hình, và lớp Dropout2D với tỷ lệ 70\% để ngăn ngừa quá khớp --- Dropout2D tắt ngẫu nhiên toàn bộ bản đồ đặc trưng thay vì từng nơ-ron riêng lẻ, phù hợp với CNN do tương quan không gian cao giữa các điểm ảnh lân cận. Đầu ra có kích thước $(N, 64, 3, 3)$.

Tiếp theo, khối tích chập thứ hai sử dụng 32 bộ lọc để nén thông tin từ 64 kênh xuống 32 kênh, với cấu trúc tương tự khối thứ nhất. Việc giảm số kênh buộc mô hình phải học cách biểu diễn thông tin cô đọng hơn. Đầu ra có kích thước $(N, 32, 3, 3)$.

Sau đó, GAP tính giá trị trung bình của mỗi kênh đặc trưng trên toàn bộ vùng không gian $3 \times 3$, chuyển đổi bản đồ đặc trưng thành vector 32 chiều. Kỹ thuật này giúp giảm số lượng tham số và tăng khả năng tổng quát hóa của mô hình.

Cuối cùng, hai lớp kết nối đầy đủ thực hiện phân loại. Lớp thứ nhất mở rộng từ 32 lên 64 chiều kèm theo chuẩn hóa theo lô, ReLU và Dropout. Lớp thứ hai (lớp đầu ra) ánh xạ từ 64 chiều xuống 4 chiều, tương ứng với 4 lớp phân loại: Rừng ổn định, Mất rừng, Phi rừng và Phục hồi rừng.

Mô hình sử dụng phương pháp khởi tạo trọng số Kaiming/He \citeen{he2015} cho các lớp tích chập và Xavier cho các lớp kết nối đầy đủ, đảm bảo quá trình huấn luyện ổn định ngay từ đầu. Tổng cộng mô hình có 36.676 tham số có thể huấn luyện, phù hợp với bộ dữ liệu nhỏ khi kết hợp với các kỹ thuật điều chuẩn. Bảng~\ref{tab:model_params} trình bày chi tiết số lượng tham số của từng thành phần.

\begin{table}[H]
\centering
\caption{Chi tiết số tham số huấn luyện của mô hình CNN}
\label{tab:model_params}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Thành phần} & \textbf{Số tham số} & \textbf{Công thức tính} \\
\hline
Khối tích chập 1 (Conv + BN) & 15.680 & $27 \times 3 \times 3 \times 64 + 128$ \\
\hline
Khối tích chập 2 (Conv + BN) & 18.496 & $64 \times 3 \times 3 \times 32 + 64$ \\
\hline
Lớp kết nối đầy đủ 1 (FC + BN) & 2.240 & $32 \times 64 + 64 + 128$ \\
\hline
Lớp đầu ra & 260 & $64 \times 4 + 4$ \\
\hline
\textbf{Tổng cộng} & \textbf{36.676} & \\
\hline
\end{tabular}
\end{table}

Trong công thức tính tham số, số hạng đầu tiên biểu diễn tham số của lớp tích chập hoặc kết nối đầy đủ (kênh đầu vào $\times$ kích thước bộ lọc $\times$ kênh đầu ra, cộng với bias), trong khi số hạng thứ hai biểu diễn tham số của chuẩn hóa theo lô (mỗi kênh có 2 tham số: hệ số co giãn $\gamma$ và hệ số dịch chuyển $\beta$). Ví dụ, khối tích chập 1 có $27 \times 3 \times 3 \times 64 = 15{.}552$ tham số cho lớp tích chập và $64 \times 2 = 128$ tham số cho chuẩn hóa theo lô.

\subsection{Huấn luyện mô hình}

Với bộ dữ liệu gồm 2.630 mẫu và mô hình có 36.676 tham số, tỷ lệ mẫu trên tham số là 72:1 --- tỷ lệ này thấp hơn ngưỡng khuyến nghị thông thường (100:1 đến 1000:1) cho mô hình học sâu \citeen{goodfellow2016}, do đó tăng nguy cơ quá khớp. Để ngăn chặn hiện tượng quá khớp --- khi mô hình ghi nhớ dữ liệu huấn luyện thay vì học các quy luật tổng quát --- nghiên cứu áp dụng ba kỹ thuật điều chuẩn: chuẩn hóa theo lô \citeen{ioffe2015} sau mỗi lớp mạng, Dropout với tỷ lệ 70\% \citeen{srivastava2014}, và phân rã trọng số với hệ số $10^{-3}$. Bảng~\ref{tab:hyperparams} trình bày các siêu tham số huấn luyện.

\begin{table}[H]
\centering
\caption{Cấu hình siêu tham số huấn luyện}
\label{tab:hyperparams}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Tham số} & \textbf{Giá trị} & \textbf{Mô tả} \\
\hline
epochs & 200 & Số vòng lặp tối đa \\
\hline
batch\_size & 64 & Số mẫu mỗi lô \\
\hline
learning\_rate & 0,001 & Tốc độ học khởi tạo \\
\hline
weight\_decay & $10^{-3}$ & Hệ số phân rã trọng số \\
\hline
dropout\_rate & 0.7 & Tỷ lệ Dropout \\
\hline
early\_stopping & 15 epochs & Patience dừng sớm \\
\hline
\end{tabular}
\end{table}

Nghiên cứu sử dụng thuật toán tối ưu AdamW \citeen{loshchilov2019} kết hợp với ReduceLROnPlateau để tự động giảm tốc độ học khi giá trị loss kiểm định không cải thiện. Ngoài ra, cơ chế dừng sớm (early stopping) được áp dụng nhằm ngăn ngừa hiện tượng quá khớp. Dừng sớm là kỹ thuật theo dõi hiệu suất của mô hình trên tập kiểm định trong quá trình huấn luyện; nếu giá trị loss kiểm định không cải thiện sau một số epoch liên tiếp (gọi là patience), quá trình huấn luyện sẽ tự động dừng lại. Trong nghiên cứu này, patience được thiết lập là 15 epochs, đồng thời mô hình có giá trị loss kiểm định thấp nhất sẽ được lưu lại làm phiên bản cuối cùng.

Quy trình huấn luyện gồm bốn giai đoạn. Giai đoạn thứ nhất, khởi tạo trọng số theo phương pháp Kaiming/He \citeen{he2015}. Giai đoạn thứ hai, thực hiện kiểm định chéo 5 phần trên 80\% dữ liệu (2.104 mẫu) để đánh giá độ ổn định của mô hình; trong đó dữ liệu được chia thành 5 phần bằng nhau, mô hình được huấn luyện 5 lần với mỗi lần sử dụng 4 phần làm tập huấn luyện và 1 phần làm tập kiểm định. Giai đoạn thứ ba, huấn luyện mô hình cuối cùng trên toàn bộ 80\% dữ liệu. Giai đoạn thứ tư, đánh giá mô hình trên 20\% tập kiểm tra cố định (526 mẫu) --- tập dữ liệu không được sử dụng trong quá trình huấn luyện.

\subsection{Áp dụng mô hình}

Sau khi huấn luyện, mô hình được áp dụng để phân loại toàn bộ vùng nghiên cứu với khoảng 16,2 triệu điểm ảnh hợp lệ, tương đương diện tích 162.468 ha. Quy trình dự đoán được thực hiện như sau.

Đầu tiên, dữ liệu đặc trưng gồm 27 kênh được tải vào bộ nhớ. Với mỗi điểm ảnh cần phân loại, một vùng lân cận kích thước 3×3 được trích xuất, chứa thông tin của điểm ảnh trung tâm và 8 điểm ảnh xung quanh. Đối với các điểm ảnh ở biên, kỹ thuật phản chiếu (mirror padding) được sử dụng để tạo đủ ngữ cảnh 3×3 --- kỹ thuật này sao chép các điểm ảnh gần biên theo cách đối xứng để mở rộng vùng dữ liệu, ví dụ chuỗi [a, b, c] ở biên trái được mở rộng thành [b, a, b, c]. Mỗi patch sau đó được chuẩn hóa Z-score sử dụng các tham số (mean, std) đã tính từ tập huấn luyện.

Dữ liệu đã chuẩn hóa được đưa qua mô hình CNN để tính xác suất thuộc về mỗi lớp. Mô hình xuất ra vector 4 chiều cho mỗi điểm ảnh, tương ứng với xác suất thuộc về 4 lớp: Rừng ổn định, Mất rừng, Phi rừng, và Phục hồi rừng. Lớp có xác suất cao nhất được chọn làm kết quả phân loại thông qua hàm argmax --- hàm trả về chỉ số của phần tử có giá trị lớn nhất trong vector xác suất.

Do khối lượng dữ liệu lớn, việc dự đoán được thực hiện theo lô, mỗi lô gồm 10.000 điểm ảnh. Kỹ thuật độ chính xác hỗn hợp (mixed precision) với định dạng FP16 được sử dụng để giảm mức sử dụng bộ nhớ GPU và tăng tốc độ tính toán --- FP16 (floating-point 16-bit) sử dụng 16 bit thay vì 32 bit (FP32) để biểu diễn số thực, giảm một nửa dung lượng bộ nhớ và tăng tốc tính toán trên GPU hiện đại mà vẫn duy trì độ chính xác đủ cho suy luận. Kết quả phân loại được xuất ra dưới dạng file GeoTIFF với hệ quy chiếu EPSG:32648 (WGS 84/UTM Zone 48N) và độ phân giải 10m, cho phép tích hợp trực tiếp với các phần mềm GIS.
