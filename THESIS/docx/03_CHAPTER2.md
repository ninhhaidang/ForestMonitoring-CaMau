# CHÆ¯Æ NG 2: CÆ  Sá» LÃ THUYáº¾T

## 2.1. CÃ´ng nghá»‡ viá»…n thÃ¡m vÃ  áº£nh vá»‡ tinh

### 2.1.1. NguyÃªn lÃ½ viá»…n thÃ¡m

Viá»…n thÃ¡m (Remote Sensing) lÃ  khoa há»c vÃ  ká»¹ thuáº­t thu tháº­p thÃ´ng tin vá» má»™t Ä‘á»‘i tÆ°á»£ng hoáº·c khu vá»±c tá»« xa, thÆ°á»ng thÃ´ng qua viá»‡c ghi nháº­n bá»©c xáº¡ Ä‘iá»‡n tá»« pháº£n xáº¡ hoáº·c phÃ¡t ra tá»« bá» máº·t TrÃ¡i Äáº¥t. NguyÃªn lÃ½ cÆ¡ báº£n cá»§a viá»…n thÃ¡m dá»±a trÃªn tÆ°Æ¡ng tÃ¡c giá»¯a bá»©c xáº¡ Ä‘iá»‡n tá»« vÃ  cÃ¡c Ä‘á»‘i tÆ°á»£ng trÃªn bá» máº·t:

**QuÃ¡ trÃ¬nh viá»…n thÃ¡m bá»‹ Ä‘á»™ng (Passive Remote Sensing):**

1. **Nguá»“n nÄƒng lÆ°á»£ng:** Máº·t Trá»i phÃ¡t ra bá»©c xáº¡ Ä‘iá»‡n tá»«
2. **Truyá»n qua khÃ­ quyá»ƒn:** Má»™t pháº§n bá»©c xáº¡ bá»‹ háº¥p thá»¥ vÃ  tÃ¡n xáº¡ bá»Ÿi khÃ­ quyá»ƒn
3. **TÆ°Æ¡ng tÃ¡c vá»›i bá» máº·t:** Bá»©c xáº¡ pháº£n xáº¡, háº¥p thá»¥, vÃ  truyá»n qua tÃ¹y theo Ä‘áº·c tÃ­nh váº­t liá»‡u
4. **Ghi nháº­n bá»Ÿi cáº£m biáº¿n:** Vá»‡ tinh thu nháº­n bá»©c xáº¡ pháº£n xáº¡
5. **Truyá»n dá»¯ liá»‡u:** TÃ­n hiá»‡u Ä‘Æ°á»£c truyá»n vá» tráº¡m máº·t Ä‘áº¥t

**PhÆ°Æ¡ng trÃ¬nh cÃ¢n báº±ng nÄƒng lÆ°á»£ng:**

```
E_incident = E_reflected + E_absorbed + E_transmitted
```

Trong Ä‘Ã³:
- E_incident: NÄƒng lÆ°á»£ng tá»›i (tá»« Máº·t Trá»i)
- E_reflected: NÄƒng lÆ°á»£ng pháº£n xáº¡ (Ä‘Æ°á»£c cáº£m biáº¿n ghi nháº­n)
- E_absorbed: NÄƒng lÆ°á»£ng háº¥p thá»¥ (chuyá»ƒn thÃ nh nhiá»‡t)
- E_transmitted: NÄƒng lÆ°á»£ng truyá»n qua

**Há»‡ sá»‘ pháº£n xáº¡ phá»• (Spectral Reflectance):**

```
Ï(Î») = E_reflected(Î») / E_incident(Î»)
```

Trong Ä‘Ã³:
- Ï(Î»): Há»‡ sá»‘ pháº£n xáº¡ táº¡i bÆ°á»›c sÃ³ng Î»
- GiÃ¡ trá»‹ tá»« 0 (háº¥p thá»¥ hoÃ n toÃ n) Ä‘áº¿n 1 (pháº£n xáº¡ hoÃ n toÃ n)

### 2.1.2. Radar kháº©u Ä‘á»™ tá»•ng há»£p (SAR)

**NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng:**

KhÃ¡c vá»›i viá»…n thÃ¡m bá»‹ Ä‘á»™ng, SAR lÃ  há»‡ thá»‘ng chá»§ Ä‘á»™ng (active remote sensing):

1. **PhÃ¡t xung radar:** Anten phÃ¡t xung sÃ³ng Ä‘iá»‡n tá»« vá» phÃ­a TrÃ¡i Äáº¥t
2. **TÆ°Æ¡ng tÃ¡c vá»›i bá» máº·t:** SÃ³ng radar pháº£n xáº¡ ngÆ°á»£c (backscatter) vá»›i cÆ°á»ng Ä‘á»™ phá»¥ thuá»™c vÃ o:
   - Äá»™ nhÃ¡m bá» máº·t (surface roughness)
   - Äá»™ áº©m (moisture content)
   - Háº±ng sá»‘ Ä‘iá»‡n mÃ´i (dielectric constant)
   - GÃ³c tá»›i (incidence angle)
3. **Thu tÃ­n hiá»‡u pháº£n xáº¡:** Anten thu nháº­n tÃ­n hiá»‡u backscatter
4. **Xá»­ lÃ½ tÃ­n hiá»‡u:** Tá»•ng há»£p kháº©u Ä‘á»™ Ä‘á»ƒ tÄƒng Ä‘á»™ phÃ¢n giáº£i

**PhÆ°Æ¡ng trÃ¬nh Radar:**

```
P_r = (P_t Ã— GÂ² Ã— Î»Â² Ã— Ïƒâ°) / ((4Ï€)Â³ Ã— Râ´)
```

Trong Ä‘Ã³:
- P_r: CÃ´ng suáº¥t nháº­n
- P_t: CÃ´ng suáº¥t phÃ¡t
- G: Äá»™ lá»£i anten
- Î»: BÆ°á»›c sÃ³ng
- Ïƒâ°: Há»‡ sá»‘ backscatter (radar cross-section per unit area)
- R: Khoáº£ng cÃ¡ch tá»« radar Ä‘áº¿n bá» máº·t

**Há»‡ sá»‘ Backscatter (Ïƒâ°):**

```
Ïƒâ° (dB) = 10 Ã— logâ‚â‚€(Ïƒâ°_linear)
```

GiÃ¡ trá»‹ Ïƒâ° phá»¥ thuá»™c vÃ o:
- **Äá»™ nhÃ¡m bá» máº·t:** Bá» máº·t nháºµn (nÆ°á»›c) â†’ Ïƒâ° tháº¥p, bá» máº·t nhÃ¡m (rá»«ng) â†’ Ïƒâ° cao
- **Äá»™ áº©m:** Äá»™ áº©m cao â†’ Ïƒâ° cao (nÆ°á»›c cÃ³ háº±ng sá»‘ Ä‘iá»‡n mÃ´i lá»›n)
- **Cáº¥u trÃºc thá»±c váº­t:** Rá»«ng cÃ³ cáº¥u trÃºc phá»©c táº¡p â†’ backscatter máº¡nh

**Polarization:**

SAR cÃ³ thá»ƒ phÃ¡t vÃ  thu theo cÃ¡c polarization khÃ¡c nhau:
- **VV:** PhÃ¡t V (Vertical), Thu V â†’ Nháº¡y vá»›i Ä‘á»™ áº©m bá» máº·t
- **VH:** PhÃ¡t V, Thu H (Horizontal) â†’ Nháº¡y vá»›i cáº¥u trÃºc thá»±c váº­t (volume scattering)
- **HH:** PhÃ¡t H, Thu H â†’ Nháº¡y vá»›i Ä‘á»™ nhÃ¡m bá» máº·t
- **HV:** PhÃ¡t H, Thu V â†’ TÆ°Æ¡ng tá»± VH

**Sentinel-1 SAR:**
- Dáº£i sÃ³ng: C-band (Î» = 5.5 cm, frequency = 5.4 GHz)
- Polarization: VV vÃ  VH (IW mode)
- Äá»™ phÃ¢n giáº£i khÃ´ng gian: 10m
- Æ¯u Ä‘iá»ƒm: XuyÃªn qua mÃ¢y, hoáº¡t Ä‘á»™ng ngÃ y/Ä‘Ãªm

### 2.1.3. áº¢nh quang há»c Ä‘a phá»• (Optical Multispectral)

**Dáº£i phá»• Ä‘iá»‡n tá»«:**

áº¢nh quang há»c ghi nháº­n bá»©c xáº¡ pháº£n xáº¡ tá»« bá» máº·t TrÃ¡i Äáº¥t á»Ÿ cÃ¡c dáº£i phá»• khÃ¡c nhau:

1. **Visible (VIS):** 400-700 nm
   - Blue (B): 450-520 nm
   - Green (G): 520-600 nm
   - Red (R): 630-690 nm

2. **Near-Infrared (NIR):** 700-1400 nm
   - Pháº£n xáº¡ cao á»Ÿ thá»±c váº­t xanh (chlorophyll)
   - Quan trá»ng cho tÃ­nh toÃ¡n NDVI

3. **Short-Wave Infrared (SWIR):** 1400-3000 nm
   - SWIR1: 1550-1750 nm
   - SWIR2: 2080-2350 nm
   - Nháº¡y vá»›i Ä‘á»™ áº©m thá»±c váº­t vÃ  Ä‘áº¥t

**Chá»¯ kÃ½ phá»• (Spectral Signature):**

Má»—i loáº¡i Ä‘á»‘i tÆ°á»£ng cÃ³ chá»¯ kÃ½ phá»• Ä‘áº·c trÆ°ng - máº«u pháº£n xáº¡ qua cÃ¡c dáº£i phá»•:

```
S = [Ï(Î»â‚), Ï(Î»â‚‚), ..., Ï(Î»â‚™)]
```

VÃ­ dá»¥:
- **Thá»±c váº­t xanh:** Pháº£n xáº¡ tháº¥p á»Ÿ Red (háº¥p thá»¥ bá»Ÿi chlorophyll), pháº£n xáº¡ cao á»Ÿ NIR
- **Äáº¥t trá»‘ng:** Pháº£n xáº¡ trung bÃ¬nh vÃ  tÄƒng dáº§n theo bÆ°á»›c sÃ³ng
- **NÆ°á»›c:** Pháº£n xáº¡ tháº¥p á»Ÿ táº¥t cáº£ cÃ¡c dáº£i (Ä‘áº·c biá»‡t NIR vÃ  SWIR)

**Sentinel-2 Multispectral Imager:**

| Band | TÃªn | BÆ°á»›c sÃ³ng (nm) | Äá»™ phÃ¢n giáº£i (m) | á»¨ng dá»¥ng |
|------|-----|---------------|------------------|----------|
| B2 | Blue | 490 | 10 | PhÃ¢n biá»‡t Ä‘áº¥t/nÆ°á»›c |
| B3 | Green | 560 | 10 | ÄÃ¡nh giÃ¡ thá»±c váº­t |
| B4 | Red | 665 | 10 | Chlorophyll absorption |
| B8 | NIR | 842 | 10 | Biomass, NDVI |
| B11 | SWIR1 | 1610 | 20 | Äá»™ áº©m, NDMI |
| B12 | SWIR2 | 2190 | 20 | PhÃ¢n biá»‡t Ä‘áº¥t/rá»«ng, NBR |

### 2.1.4. Chá»‰ sá»‘ thá»±c váº­t

**NDVI (Normalized Difference Vegetation Index):**

```
NDVI = (NIR - Red) / (NIR + Red)
```

*LÆ°u Ã½: Trong thá»±c táº¿, má»™t epsilon nhá» (1e-8) Ä‘Æ°á»£c thÃªm vÃ o máº«u sá»‘ Ä‘á»ƒ trÃ¡nh chia cho 0.*

**NguyÃªn lÃ½:**
- Thá»±c váº­t xanh: Háº¥p thá»¥ máº¡nh Red (chlorophyll), pháº£n xáº¡ cao NIR (cáº¥u trÃºc táº¿ bÃ o) â†’ NDVI cao
- Äáº¥t trá»‘ng/nÆ°á»›c: Pháº£n xáº¡ tháº¥p cáº£ Red vÃ  NIR â†’ NDVI tháº¥p

**Pháº¡m vi giÃ¡ trá»‹:**
- NDVI > 0.6: Thá»±c váº­t xanh tá»‘t (rá»«ng ráº­m)
- 0.2 < NDVI < 0.6: Thá»±c váº­t thÆ°a, cá»
- NDVI < 0.2: Äáº¥t trá»‘ng, nÆ°á»›c, Ä‘Ã´ thá»‹

**Äáº¡o hÃ m toÃ¡n há»c:**
```
âˆ‚NDVI/âˆ‚NIR = (2 Ã— Red) / (NIR + Red)Â²
âˆ‚NDVI/âˆ‚Red = -(2 Ã— NIR) / (NIR + Red)Â²
```

**NBR (Normalized Burn Ratio):**

```
NBR = (NIR - SWIR2) / (NIR + SWIR2)
```

*LÆ°u Ã½: Trong triá»ƒn khai, epsilon (1e-8) Ä‘Æ°á»£c thÃªm vÃ o máº«u sá»‘.*

**NguyÃªn lÃ½:**
- NIR: Pháº£n xáº¡ cao á»Ÿ thá»±c váº­t xanh
- SWIR2: Nháº¡y vá»›i Ä‘á»™ áº©m vÃ  vÃ¹ng chÃ¡y
- VÃ¹ng chÃ¡y: NIR giáº£m, SWIR2 tÄƒng â†’ NBR giáº£m máº¡nh

**Delta NBR (dNBR):**
```
dNBR = NBR_before - NBR_after
```

- dNBR > 0.66: ChÃ¡y nghiÃªm trá»ng
- 0.44 < dNBR < 0.66: ChÃ¡y vá»«a
- 0.27 < dNBR < 0.44: ChÃ¡y nháº¹
- dNBR < 0.27: KhÃ´ng chÃ¡y hoáº·c tÃ¡i sinh

**NDMI (Normalized Difference Moisture Index):**

```
NDMI = (NIR - SWIR1) / (NIR + SWIR1)
```

*LÆ°u Ã½: Trong triá»ƒn khai, epsilon (1e-8) Ä‘Æ°á»£c thÃªm vÃ o máº«u sá»‘.*

**NguyÃªn lÃ½:**
- SWIR1 (~1600 nm): Háº¥p thá»¥ máº¡nh bá»Ÿi nÆ°á»›c
- Äá»™ áº©m thá»±c váº­t cao â†’ SWIR1 pháº£n xáº¡ tháº¥p â†’ NDMI cao
- Stress háº¡n â†’ NDMI giáº£m

**Pháº¡m vi giÃ¡ trá»‹:**
- NDMI > 0.4: Äá»™ áº©m cao
- 0.0 < NDMI < 0.4: Äá»™ áº©m trung bÃ¬nh
- NDMI < 0: Stress háº¡n, nguy cÆ¡ chÃ¡y cao

### 2.1.5. PhÃ¡t hiá»‡n biáº¿n Ä‘á»™ng rá»«ng

**Change Detection Approach:**

```
Î”Feature = Feature_after - Feature_before
```

**Temporal Features:**
- **Before features:** Tráº¡ng thÃ¡i rá»«ng táº¡i thá»i Ä‘iá»ƒm tâ‚
- **After features:** Tráº¡ng thÃ¡i rá»«ng táº¡i thá»i Ä‘iá»ƒm tâ‚‚
- **Delta features:** Biáº¿n Ä‘á»•i giá»¯a hai thá»i Ä‘iá»ƒm (tâ‚‚ - tâ‚)

**VÃ­ dá»¥ vá»›i NDVI:**

```
Î”NDVI = NDVI_after - NDVI_before
```

**PhÃ¢n loáº¡i biáº¿n Ä‘á»™ng:**
- Î”NDVI << 0 (giáº£m máº¡nh): Máº¥t rá»«ng (deforestation)
- Î”NDVI â‰ˆ 0: Rá»«ng á»•n Ä‘á»‹nh
- Î”NDVI >> 0 (tÄƒng máº¡nh): TÃ¡i trá»“ng rá»«ng

**Káº¿t há»£p Ä‘a chá»‰ sá»‘:**

```
Change_score = wâ‚Ã—|Î”NDVI| + wâ‚‚Ã—|Î”NBR| + wâ‚ƒÃ—|Î”NDMI|
```

Trong Ä‘Ã³ wâ‚, wâ‚‚, wâ‚ƒ lÃ  trá»ng sá»‘ Ä‘Æ°á»£c há»c tá»« dá»¯ liá»‡u.

---

## 2.2. Máº¡ng Neural TÃ­ch cháº­p (Convolutional Neural Networks)

### 2.2.1. Giá»›i thiá»‡u vá» Neural Networks

**Perceptron - ÄÆ¡n vá»‹ cÆ¡ báº£n:**

Má»™t neuron nhÃ¢n táº¡o thá»±c hiá»‡n phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh vÃ  hÃ m kÃ­ch hoáº¡t:

```
y = f(w^T Ã— x + b)
```

Trong Ä‘Ã³:
- x âˆˆ â„â¿: Input vector (n features)
- w âˆˆ â„â¿: Weight vector
- b âˆˆ â„: Bias
- f(.): Activation function
- y: Output

**Multi-Layer Perceptron (MLP):**

Má»™t máº¡ng neural gá»“m nhiá»u layers:

```
Layer 1: hâ‚ = fâ‚(Wâ‚Ã—x + bâ‚)
Layer 2: hâ‚‚ = fâ‚‚(Wâ‚‚Ã—hâ‚ + bâ‚‚)
...
Output: y = fâ‚™(Wâ‚™Ã—hâ‚™â‚‹â‚ + bâ‚™)
```

**Universal Approximation Theorem:**

Má»™t MLP vá»›i Ã­t nháº¥t má»™t hidden layer vÃ  Ä‘á»§ neurons cÃ³ thá»ƒ xáº¥p xá»‰ báº¥t ká»³ hÃ m liÃªn tá»¥c nÃ o vá»›i Ä‘á»™ chÃ­nh xÃ¡c tÃ¹y Ã½ trÃªn má»™t compact subset.

### 2.2.2. Convolutional Layer

**PhÃ©p tÃ­ch cháº­p 2D (2D Convolution):**

ÄÃ¢y lÃ  thÃ nh pháº§n cá»‘t lÃµi cá»§a CNN, thá»±c hiá»‡n phÃ©p tÃ­ch cháº­p giá»¯a input vÃ  kernel:

```
(I * K)(i, j) = Î£Î£ I(i+m, j+n) Ã— K(m, n)
               m n
```

Trong Ä‘Ã³:
- I: Input feature map (height Ã— width Ã— channels)
- K: Kernel/Filter (k_h Ã— k_w Ã— channels)
- (i, j): Vá»‹ trÃ­ output
- (m, n): Vá»‹ trÃ­ trong kernel

**VÃ­ dá»¥ cá»¥ thá»ƒ vá»›i kernel 3Ã—3:**

Input I (5Ã—5):
```
[1  2  3  4  5]
[6  7  8  9  10]
[11 12 13 14 15]
[16 17 18 19 20]
[21 22 23 24 25]
```

Kernel K (3Ã—3):
```
[1  0 -1]
[1  0 -1]
[1  0 -1]
```

Output táº¡i (1,1):
```
O(1,1) = 1Ã—1 + 2Ã—0 + 3Ã—(-1) +
         6Ã—1 + 7Ã—0 + 8Ã—(-1) +
         11Ã—1 + 12Ã—0 + 13Ã—(-1)
       = 1 + 0 - 3 + 6 + 0 - 8 + 11 + 0 - 13
       = -6
```

**CÃ´ng thá»©c tá»•ng quÃ¡t cho Multi-Channel Convolution:**

```
O(i, j, k) = Î£   Î£  Î£  I(i+m, j+n, c) Ã— K(m, n, c, k) + b_k
             c   m  n
```

Trong Ä‘Ã³:
- I âˆˆ â„^(H Ã— W Ã— C_in): Input vá»›i C_in channels
- K âˆˆ â„^(k_h Ã— k_w Ã— C_in Ã— C_out): Kernel
- O âˆˆ â„^(H' Ã— W' Ã— C_out): Output vá»›i C_out channels
- b_k: Bias cho output channel k

**Output size calculation:**

```
H_out = âŒŠ(H_in + 2Ã—padding - kernel_size) / strideâŒ‹ + 1
W_out = âŒŠ(W_in + 2Ã—padding - kernel_size) / strideâŒ‹ + 1
```

**Parameters trong Convolutional Layer:**

```
#params = (k_h Ã— k_w Ã— C_in Ã— C_out) + C_out
```

Trong Ä‘Ã³:
- k_h Ã— k_w Ã— C_in Ã— C_out: Weights
- C_out: Biases

**VÃ­ dá»¥:** Conv2D(in_channels=27, out_channels=64, kernel_size=3)
```
#params = (3 Ã— 3 Ã— 27 Ã— 64) + 64
        = 15,552 + 64
        = 15,616
```

**Æ¯u Ä‘iá»ƒm cá»§a Convolution:**

1. **Parameter sharing:** CÃ¹ng má»™t kernel Ä‘Æ°á»£c Ã¡p dá»¥ng cho toÃ n bá»™ input
   - MLP: Má»—i connection cÃ³ weight riÃªng â†’ O(HÃ—W) parameters
   - CNN: Kernel Ä‘Æ°á»£c chia sáº» â†’ O(kÂ²) parameters (k << H, W)

2. **Translation invariance:** Nháº­n diá»‡n Ä‘áº·c trÆ°ng á»Ÿ báº¥t ká»³ vá»‹ trÃ­ nÃ o
   - Náº¿u Ä‘á»‘i tÆ°á»£ng dá»‹ch chuyá»ƒn, CNN váº«n nháº­n diá»‡n Ä‘Æ°á»£c

3. **Local connectivity:** Má»—i neuron chá»‰ káº¿t ná»‘i vá»›i vÃ¹ng local cá»§a input
   - PhÃ¹ há»£p vá»›i cáº¥u trÃºc khÃ´ng gian cá»§a áº£nh

### 2.2.3. Activation Functions

**ReLU (Rectified Linear Unit):**

```
f(x) = max(0, x) = {x  if x > 0
                    {0  if x â‰¤ 0
```

**Äáº¡o hÃ m:**
```
f'(x) = {1  if x > 0
        {0  if x â‰¤ 0
```

**Æ¯u Ä‘iá»ƒm:**
- TÃ­nh toÃ¡n nhanh (khÃ´ng cÃ³ exp, log)
- Giáº£i quyáº¿t vanishing gradient problem
- Sparse activation (nhiá»u neurons = 0)
- Gradient Ä‘Æ¡n giáº£n (0 hoáº·c 1)

**NhÆ°á»£c Ä‘iá»ƒm:**
- Dying ReLU: Neurons cÃ³ thá»ƒ "cháº¿t" (output luÃ´n 0) náº¿u gradient liÃªn tá»¥c Ã¢m

**Leaky ReLU:**

```
f(x) = {x      if x > 0
       {Î±Ã—x    if x â‰¤ 0
```

Vá»›i Î± = 0.01 (small positive slope cho x < 0)

**Softmax (cho Output Layer):**

```
softmax(x)áµ¢ = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)
```

**TÃ­nh cháº¥t:**
- Output lÃ  xÃ¡c suáº¥t: 0 â‰¤ softmax(x)áµ¢ â‰¤ 1
- Tá»•ng = 1: Î£áµ¢ softmax(x)áµ¢ = 1
- DÃ¹ng cho multi-class classification

**VÃ­ dá»¥:**
```
Input logits: [2.0, 1.0, 0.1]

exp([2.0, 1.0, 0.1]) = [7.39, 2.72, 1.11]
Sum = 11.22

Softmax = [7.39/11.22, 2.72/11.22, 1.11/11.22]
        = [0.659, 0.242, 0.099]
```

### 2.2.4. Pooling Layers

**Max Pooling:**

```
MaxPool(i, j) = max  I(i+m, j+n)
                m,nâˆˆW
```

Trong Ä‘Ã³ W lÃ  pooling window (thÆ°á»ng 2Ã—2)

**VÃ­ dá»¥ Max Pooling 2Ã—2:**

Input (4Ã—4):
```
[1  3  2  4]
[5  6  7  8]
[9  10 11 12]
[13 14 15 16]
```

Output (2Ã—2):
```
[max(1,3,5,6)=6    max(2,4,7,8)=8   ]
[max(9,10,13,14)=14 max(11,12,15,16)=16]
```

Result:
```
[6  8 ]
[14 16]
```

**Average Pooling:**

```
AvgPool(i, j) = (1/|W|) Ã— Î£ I(i+m, j+n)
                         m,nâˆˆW
```

**Global Average Pooling:**

```
GAP(k) = (1/(HÃ—W)) Ã— Î£Î£ I(i, j, k)
                      i j
```

- Giáº£m spatial dimensions vá» 1Ã—1
- Output: (1, 1, C) tá»« (H, W, C)
- Æ¯u Ä‘iá»ƒm: KhÃ´ng cÃ³ parameters, giáº£m overfitting

**TÃ¡c dá»¥ng cá»§a Pooling:**

1. **Downsampling:** Giáº£m spatial dimensions â†’ giáº£m computation
2. **Translation invariance:** TÄƒng kháº£ nÄƒng báº¥t biáº¿n vá»›i phÃ©p dá»‹ch nhá»
3. **Receptive field:** TÄƒng receptive field cá»§a cÃ¡c layers sau

### 2.2.5. Batch Normalization

**Motivation:**

Internal Covariate Shift: PhÃ¢n bá»‘ input cá»§a má»—i layer thay Ä‘á»•i trong quÃ¡ trÃ¬nh training, lÃ m cháº­m quÃ¡ trÃ¬nh há»™i tá»¥.

**Batch Normalization Algorithm:**

Vá»›i má»™t mini-batch B = {xâ‚, xâ‚‚, ..., x_m}:

**Step 1: TÃ­nh mean vÃ  variance cá»§a batch**
```
Î¼_B = (1/m) Ã— Î£áµ¢ xáµ¢

ÏƒÂ²_B = (1/m) Ã— Î£áµ¢ (xáµ¢ - Î¼_B)Â²
```

**Step 2: Normalize**
```
xÌ‚áµ¢ = (xáµ¢ - Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)
```

Trong Ä‘Ã³ Îµ = 1e-8 (trÃ¡nh chia cho 0)

**Step 3: Scale and shift (learnable parameters)**
```
yáµ¢ = Î³ Ã— xÌ‚áµ¢ + Î²
```

Trong Ä‘Ã³:
- Î³: Scale parameter (learnable)
- Î²: Shift parameter (learnable)

**Táº¡i sao cáº§n Î³ vÃ  Î²?**

Normalization cÃ³ thá»ƒ háº¡n cháº¿ kháº£ nÄƒng biá»ƒu diá»…n cá»§a network. Î³ vÃ  Î² cho phÃ©p network há»c láº¡i phÃ¢n bá»‘ tá»‘i Æ°u.

**Inference (Test time):**

Sá»­ dá»¥ng running mean vÃ  variance tá»« training:

```
Î¼_running = momentum Ã— Î¼_running + (1-momentum) Ã— Î¼_B
ÏƒÂ²_running = momentum Ã— ÏƒÂ²_running + (1-momentum) Ã— ÏƒÂ²_B

y_test = Î³ Ã— (x - Î¼_running) / âˆš(ÏƒÂ²_running + Îµ) + Î²
```

**Æ¯u Ä‘iá»ƒm:**
- TÄƒng tá»‘c training (cÃ³ thá»ƒ dÃ¹ng learning rate cao hÆ¡n)
- Giáº£m sensitivity vá»›i weight initialization
- CÃ³ tÃ¡c dá»¥ng regularization (tÆ°Æ¡ng tá»± dropout)
- á»”n Ä‘á»‹nh gradient flow

**Gradient Computation:**

```
âˆ‚L/âˆ‚xÌ‚áµ¢ = âˆ‚L/âˆ‚yáµ¢ Ã— Î³

âˆ‚L/âˆ‚ÏƒÂ²_B = Î£áµ¢ (âˆ‚L/âˆ‚xÌ‚áµ¢ Ã— (xáµ¢ - Î¼_B)) Ã— (-1/2) Ã— (ÏƒÂ²_B + Îµ)^(-3/2)

âˆ‚L/âˆ‚Î¼_B = Î£áµ¢ (âˆ‚L/âˆ‚xÌ‚áµ¢ Ã— (-1/âˆš(ÏƒÂ²_B + Îµ)))

âˆ‚L/âˆ‚xáµ¢ = âˆ‚L/âˆ‚xÌ‚áµ¢ Ã— (1/âˆš(ÏƒÂ²_B + Îµ)) + (âˆ‚L/âˆ‚ÏƒÂ²_B Ã— 2(xáµ¢-Î¼_B)/m) + (âˆ‚L/âˆ‚Î¼_B/m)
```

### 2.2.6. Dropout

**Motivation:**

Overfitting xáº£y ra khi model há»c quÃ¡ chi tiáº¿t training data, khÃ´ng generalize tá»‘t cho unseen data.

**Dropout Algorithm (Training):**

```
For each neuron i:
    r_i ~ Bernoulli(p)  # Vá»›i xÃ¡c suáº¥t p giá»¯ láº¡i neuron
    á»¹_i = r_i Ã— y_i     # Náº¿u r_i=0, neuron bá»‹ táº¯t
```

Trong Ä‘Ã³:
- p: Dropout probability (thÆ°á»ng p=0.5 cho FC, p=0.7-0.9 cho Conv)
- r_i: Binary mask (0 hoáº·c 1)

**Dropout (Inference):**

Táº¡i inference, táº¥t cáº£ neurons Ä‘á»u active, nhÆ°ng output Ä‘Æ°á»£c scale:

```
y_test = p Ã— y_train
```

Hoáº·c Ã¡p dá»¥ng inverted dropout (phá»• biáº¿n hÆ¡n):

**Training:**
```
á»¹_i = (r_i Ã— y_i) / p
```

**Inference:**
```
y_test = y_train  # KhÃ´ng cáº§n scale
```

**VÃ­ dá»¥:**

Giáº£ sá»­ cÃ³ 4 neurons vá»›i outputs [0.5, 0.8, 0.3, 0.9], p=0.5

**Training iteration 1:**
```
Random mask: [1, 0, 1, 0]
Output: [0.5/0.5, 0, 0.3/0.5, 0] = [1.0, 0, 0.6, 0]
```

**Training iteration 2:**
```
Random mask: [0, 1, 1, 1]
Output: [0, 0.8/0.5, 0.3/0.5, 0.9/0.5] = [0, 1.6, 0.6, 1.8]
```

**Inference:**
```
Output: [0.5, 0.8, 0.3, 0.9]  # Táº¥t cáº£ neurons active
```

**TÃ¡c dá»¥ng:**
- **Ensemble effect:** Má»—i iteration training vá»›i má»™t sub-network khÃ¡c nhau
- **Co-adaptation prevention:** Neurons khÃ´ng phá»¥ thuá»™c láº«n nhau
- **Regularization:** Giáº£m overfitting

**Dropout2d (Spatial Dropout):**

Thay vÃ¬ dropout tá»«ng neuron, dropout toÃ n bá»™ feature maps:

```
For each channel k:
    r_k ~ Bernoulli(p)
    á»¹[:,:,k] = r_k Ã— y[:,:,k]
```

PhÃ¹ há»£p cho CNN vÃ¬ features trong cÃ¹ng channel cÃ³ correlation khÃ´ng gian cao.

### 2.2.7. Loss Functions

**Cross-Entropy Loss (Multi-class Classification):**

```
L = -Î£áµ¢ yáµ¢ Ã— log(Å·áµ¢)
```

Trong Ä‘Ã³:
- yáµ¢: True label (one-hot encoded)
- Å·áµ¢: Predicted probability (from softmax)

**VÃ­ dá»¥ 4-class classification:**

True label: Class 1 â†’ y = [0, 1, 0, 0]
Predicted: Å· = [0.1, 0.7, 0.15, 0.05]

```
L = -(0Ã—log(0.1) + 1Ã—log(0.7) + 0Ã—log(0.15) + 0Ã—log(0.05))
  = -log(0.7)
  = 0.357
```

**PyTorch Implementation:**

```python
loss = nn.CrossEntropyLoss()
```

Note: PyTorch's CrossEntropyLoss Ä‘Ã£ bao gá»“m softmax, input lÃ  logits (trÆ°á»›c softmax).

**Weighted Cross-Entropy (Class Imbalance):**

```
L = -Î£áµ¢ wáµ¢ Ã— yáµ¢ Ã— log(Å·áµ¢)
```

Trong Ä‘Ã³ wáµ¢ lÃ  weight cho class i:

```
wáµ¢ = n_samples / (n_classes Ã— n_samples_class_i)
```

**VÃ­ dá»¥:**
- Class 0: 656 samples â†’ wâ‚€ = 2630/(4Ã—656) = 1.003
- Class 1: 650 samples â†’ wâ‚ = 2630/(4Ã—650) = 1.010
- Class 2: 664 samples â†’ wâ‚‚ = 2630/(4Ã—664) = 0.992
- Class 3: 660 samples â†’ wâ‚ƒ = 2630/(4Ã—660) = 0.995

**Gradient cá»§a Cross-Entropy Loss:**

Káº¿t há»£p vá»›i softmax:

```
âˆ‚L/âˆ‚záµ¢ = Å·áµ¢ - yáµ¢
```

Trong Ä‘Ã³ záµ¢ lÃ  logits (input cá»§a softmax).

CÃ´ng thá»©c nÃ y ráº¥t Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£ cho backpropagation!

### 2.2.8. Optimization Algorithms

**Stochastic Gradient Descent (SGD):**

```
Î¸_{t+1} = Î¸_t - Î· Ã— âˆ‡L(Î¸_t)
```

Trong Ä‘Ã³:
- Î¸: Parameters (weights, biases)
- Î·: Learning rate
- âˆ‡L(Î¸): Gradient cá»§a loss

**Momentum:**

```
v_t = Î² Ã— v_{t-1} + (1-Î²) Ã— âˆ‡L(Î¸_t)
Î¸_{t+1} = Î¸_t - Î· Ã— v_t
```

- Î²: Momentum coefficient (thÆ°á»ng 0.9)
- GiÃºp tÄƒng tá»‘c trong hÆ°á»›ng consistent, giáº£m oscillation

**Adam (Adaptive Moment Estimation):**

```
m_t = Î²â‚ Ã— m_{t-1} + (1-Î²â‚) Ã— âˆ‡L(Î¸_t)        # First moment (mean)
v_t = Î²â‚‚ Ã— v_{t-1} + (1-Î²â‚‚) Ã— (âˆ‡L(Î¸_t))Â²    # Second moment (variance)

mÌ‚_t = m_t / (1 - Î²â‚^t)                       # Bias correction
vÌ‚_t = v_t / (1 - Î²â‚‚^t)

Î¸_{t+1} = Î¸_t - Î· Ã— mÌ‚_t / (âˆšvÌ‚_t + Îµ)
```

Hyperparameters:
- Î²â‚ = 0.9: Exponential decay for first moment
- Î²â‚‚ = 0.999: Exponential decay for second moment
- Îµ = 1e-8: Numerical stability
- Î· = 0.001: Learning rate

**AdamW (Adam with Weight Decay):**

```
Î¸_{t+1} = Î¸_t - Î· Ã— (mÌ‚_t / (âˆšvÌ‚_t + Îµ) + Î» Ã— Î¸_t)
```

Trong Ä‘Ã³ Î» lÃ  weight decay coefficient (L2 regularization).

**Æ¯u Ä‘iá»ƒm cá»§a Adam:**
- Adaptive learning rates cho má»—i parameter
- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i sparse gradients
- Ãt sensitive vá»›i learning rate choice
- Fast convergence

**Learning Rate Scheduling:**

**ReduceLROnPlateau:**

```
if val_loss khÃ´ng giáº£m sau patience epochs:
    Î·_new = factor Ã— Î·_old
```

VÃ­ dá»¥: factor=0.5, patience=5
- Sau 5 epochs val_loss khÃ´ng cáº£i thiá»‡n â†’ Î· giáº£m 50%

### 2.2.9. Backpropagation Algorithm

**Forward Pass:**

```
Layer 1: zâ‚ = Wâ‚Ã—x + bâ‚,  aâ‚ = fâ‚(zâ‚)
Layer 2: zâ‚‚ = Wâ‚‚Ã—aâ‚ + bâ‚‚, aâ‚‚ = fâ‚‚(zâ‚‚)
...
Output: y = softmax(zâ‚™)
Loss: L = -Î£ yáµ¢Ã—log(Å·áµ¢)
```

**Backward Pass (Chain Rule):**

```
âˆ‚L/âˆ‚Wâ‚™ = (âˆ‚L/âˆ‚zâ‚™) Ã— (âˆ‚zâ‚™/âˆ‚Wâ‚™)
       = (âˆ‚L/âˆ‚zâ‚™) Ã— aâ‚™â‚‹â‚^T

âˆ‚L/âˆ‚bâ‚™ = âˆ‚L/âˆ‚zâ‚™

âˆ‚L/âˆ‚aâ‚™â‚‹â‚ = Wâ‚™^T Ã— (âˆ‚L/âˆ‚zâ‚™)

âˆ‚L/âˆ‚zâ‚™â‚‹â‚ = (âˆ‚L/âˆ‚aâ‚™â‚‹â‚) âŠ™ f'(zâ‚™â‚‹â‚)
```

Trong Ä‘Ã³ âŠ™ lÃ  element-wise multiplication.

**Backprop cho Convolutional Layer:**

Forward:
```
O = I * K + b
```

Backward:
```
âˆ‚L/âˆ‚K = I * (âˆ‚L/âˆ‚O)         # Gradient w.r.t. kernel
âˆ‚L/âˆ‚b = Î£áµ¢â±¼ (âˆ‚L/âˆ‚O)áµ¢â±¼       # Gradient w.r.t. bias
âˆ‚L/âˆ‚I = (âˆ‚L/âˆ‚O) * K_flipped # Gradient w.r.t. input
```

**Computational Graph:**

```
Input â†’ Conv â†’ BN â†’ ReLU â†’ MaxPool â†’ ... â†’ FC â†’ Softmax â†’ Loss
  â†‘                                                          â†“
  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Backpropagation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Vanishing/Exploding Gradient Problem:**

Trong deep networks:

```
âˆ‚L/âˆ‚Wâ‚ = (âˆ‚L/âˆ‚zâ‚™) Ã— (âˆ‚zâ‚™/âˆ‚zâ‚™â‚‹â‚) Ã— ... Ã— (âˆ‚zâ‚‚/âˆ‚zâ‚) Ã— (âˆ‚zâ‚/âˆ‚Wâ‚)
```

Náº¿u má»—i term < 1 â†’ gradient vanish (tiáº¿n vá» 0)
Náº¿u má»—i term > 1 â†’ gradient explode (tiáº¿n vá» âˆ)

**Giáº£i phÃ¡p:**
- ReLU activation (gradient = 1 for x > 0)
- Batch Normalization (á»•n Ä‘á»‹nh gradient)
- Residual connections (skip connections)
- Gradient clipping (giá»›i háº¡n gradient magnitude)

---

## 2.3. PhÆ°Æ¡ng phÃ¡p phÃ¢n loáº¡i áº£nh viá»…n thÃ¡m

### 2.3.1. Pixel-based vs Patch-based Classification

**Pixel-based Classification:**

Má»—i pixel Ä‘Æ°á»£c phÃ¢n loáº¡i Ä‘á»™c láº­p dá»±a trÃªn vector Ä‘áº·c trÆ°ng:

```
x_i = [fâ‚, fâ‚‚, ..., f_n]  # n features táº¡i pixel i
y_i = classifier(x_i)      # Predict class
```

**Æ¯u Ä‘iá»ƒm:**
- ÄÆ¡n giáº£n, dá»… implement
- Nhanh (parallel processing)
- PhÃ¹ há»£p vá»›i ML truyá»n thá»‘ng (RF, SVM)

**NhÆ°á»£c Ä‘iá»ƒm:**
- KhÃ´ng sá»­ dá»¥ng spatial context
- Salt-and-pepper noise trong káº¿t quáº£
- Bá» qua relationships giá»¯a neighboring pixels

**Patch-based Classification:**

TrÃ­ch xuáº¥t patches (windows) xung quanh má»—i pixel:

```
P_i = extract_patch(I, center=(row_i, col_i), size=kÃ—k)
y_i = classifier(P_i)
```

**VÃ­ dá»¥ vá»›i patch 3Ã—3:**

```
Image I:
[1  2  3  4  5]
[6  7  8  9  10]
[11 12 13 14 15]
[16 17 18 19 20]
[21 22 23 24 25]

Patch táº¡i (2, 2):
[7  8  9 ]
[12 13 14]
[17 18 19]
```

**Æ¯u Ä‘iá»ƒm:**
- Sá»­ dá»¥ng spatial context
- Káº¿t quáº£ smooth hÆ¡n
- PhÃ¹ há»£p vá»›i CNN (automatic feature learning)

**NhÆ°á»£c Ä‘iá»ƒm:**
- Cháº­m hÆ¡n (nhiá»u data hÆ¡n)
- Cáº§n xá»­ lÃ½ edge pixels
- Redundant computation (overlapping patches)

### 2.3.2. Spatial Autocorrelation

**Tobler's First Law of Geography:**

"Everything is related to everything else, but near things are more related than distant things."

**Moran's I (Spatial Autocorrelation Index):**

```
I = (N/W) Ã— (Î£áµ¢ Î£â±¼ wáµ¢â±¼(xáµ¢ - xÌ„)(xâ±¼ - xÌ„)) / (Î£áµ¢ (xáµ¢ - xÌ„)Â²)
```

Trong Ä‘Ã³:
- N: Number of samples
- wáµ¢â±¼: Spatial weight (1 if neighbors, 0 otherwise)
- xáµ¢, xâ±¼: Values at locations i, j
- xÌ„: Mean value

**Interpretation:**
- I > 0: Positive autocorrelation (similar values cluster)
- I â‰ˆ 0: Random distribution
- I < 0: Negative autocorrelation (dissimilar values cluster)

**Implication for Machine Learning:**

Training vÃ  test samples gáº§n nhau trong khÃ´ng gian cÃ³ high correlation â†’ **Data leakage** â†’ Overestimate accuracy.

**Giáº£i phÃ¡p: Stratified Data Splitting + Cross Validation**

### 2.3.3. Chiáº¿n lÆ°á»£c chia dá»¯ liá»‡u

**Stratified Random Splitting:**

```
1. Stratified shuffle all samples (giá»¯ tá»· lá»‡ lá»›p)
2. Train+Val: 80% stratified samples
3. Test: 20% stratified samples (fixed)
4. 5-Fold Stratified CV trÃªn Train+Val
```

**Æ¯u Ä‘iá»ƒm:**

```
- Äáº£m báº£o phÃ¢n bá»‘ lá»›p Ä‘á»“ng Ä‘á»u trong táº¥t cáº£ cÃ¡c folds
- 5-Fold CV Ä‘Ã¡nh giÃ¡ variance cá»§a mÃ´ hÃ¬nh
- Fixed test set Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a
```

**Cross Validation Strategy:**

```
1. Stratified K-Fold (K=5) Ä‘á»ƒ chia Train+Val
2. Má»—i fold: 80% train, 20% validation
3. BÃ¡o cÃ¡o mean Â± std accuracy qua 5 folds
```

**Káº¿t quáº£:**

```
  Train cluster: â—â—â—â—


  Test cluster:           â—‹â—‹â—‹â—‹

  Distance > 50m â†’ Independent
```

### 2.3.4. Evaluation Metrics

**Confusion Matrix:**

```
                Predicted
              0      1
Actual  0   [TN    FP]
        1   [FN    TP]
```

**Accuracy:**

```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**Precision (Positive Predictive Value):**

```
Precision = TP / (TP + FP)
```

Tá»· lá»‡ predictions Ä‘Ãºng trong sá»‘ cÃ¡c positive predictions.

**Recall (Sensitivity, True Positive Rate):**

```
Recall = TP / (TP + FN)
```

Tá»· lá»‡ actual positives Ä‘Æ°á»£c detect Ä‘Ãºng.

**F1-Score (Harmonic Mean):**

```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**Specificity (True Negative Rate):**

```
Specificity = TN / (TN + FP)
```

**ROC Curve (Receiver Operating Characteristic):**

Plot TPR vs FPR at various thresholds:

```
TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
```

**AUC (Area Under ROC Curve):**

```
AUC = âˆ«â‚€Â¹ TPR(FPR) d(FPR)
```

- AUC = 1.0: Perfect classifier
- AUC = 0.5: Random classifier
- AUC > 0.9: Excellent
- 0.8 < AUC < 0.9: Good
- 0.7 < AUC < 0.8: Fair

**Multi-class Extension:**

**One-vs-Rest (OvR) AUC:**

```
AUC_ovr = (1/C) Ã— Î£áµ¢ AUC_i
```

Trong Ä‘Ã³ AUC_i lÃ  AUC cho class i vs all other classes.

**Macro-averaged Metrics:**

```
Precision_macro = (1/C) Ã— Î£áµ¢ Precisionáµ¢
Recall_macro = (1/C) Ã— Î£áµ¢ Recalláµ¢
F1_macro = (1/C) Ã— Î£áµ¢ F1áµ¢
```

**Weighted-averaged Metrics:**

```
Precision_weighted = Î£áµ¢ (náµ¢/N) Ã— Precisionáµ¢
```

Trong Ä‘Ã³ náµ¢ lÃ  sá»‘ samples cá»§a class i.

---

**[Káº¿t thÃºc ChÆ°Æ¡ng 2]**

ğŸ“š **Xem danh sÃ¡ch Ä‘áº§y Ä‘á»§ tÃ i liá»‡u tham kháº£o:** [REFERENCES.md](REFERENCES.md)
